{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processor - Financial Data:\n",
    "- AAPL\n",
    "- Bitcoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# # Test-size Ratio\n",
    "# test_size_ratio = 1-(1/48)\n",
    "# min_width = 200\n",
    "# # Ablation Finess\n",
    "# N_plot_finess = 10\n",
    "# # min_parts_threshold = .001; max_parts_threshold = 0.9\n",
    "# N_min_parts = 1; N_max_plots = 10\n",
    "# Tied_Neurons_Q = True\n",
    "# # Partition with Inputs (determine parts with domain) or outputs (determine parts with image)\n",
    "# Partition_using_Inputs = True\n",
    "# # Cuttoff Level\n",
    "# gamma = .5\n",
    "# # Softmax Layer instead of sigmoid\n",
    "# softmax_layer = True\n",
    "\n",
    "# #TEMP FOR DEBUGGING\n",
    "# # Load Packages/Modules\n",
    "# exec(open('Init_Dump.py').read())\n",
    "# # Load Hyper-parameter Grid\n",
    "# exec(open('Grid_Enhanced_Network.py').read())\n",
    "# # Load Helper Function(s)\n",
    "# exec(open('Helper_Functions.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Initializations\n",
    "# Here we dump the list of all initializations needed to run any code snippet for the NEU.\n",
    "\n",
    "# ---\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# (Semi-)Classical Regressor(s)\n",
    "from scipy.interpolate import interp1d\n",
    "import statsmodels.api as sm\n",
    "# import rpy2.robjects as robjects # Work directly from R (since smoothing splines packages is better)\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# (Semi-)Classical Dimension Reducers\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Grid-Search and CV\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# Data Structuring\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "# Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import linalg as scila\n",
    "\n",
    "# Random Forest & Gradient Boosting (Arch. Construction)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Random-Seeds\n",
    "import random\n",
    "\n",
    "# Rough Signals\n",
    "# from fbm import FBM\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from keras.utils.layer_utils import count_params\n",
    "import keras as K\n",
    "import keras.backend as Kb\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import utils as np_utils\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Operating-System Related\n",
    "import os\n",
    "from pathlib import Path\n",
    "# import pickle\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "# Timeing\n",
    "import time\n",
    "import datetime as DT\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# z_Misc\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "# Make Paths\n",
    "########################\n",
    "\n",
    "Path('./outputs/models/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/Invertible_Networks/GLd_Net/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/Invertible_Networks/Ed_Net/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/Linear_Regression/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/NAIVE_NEU/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/NEU/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/tables/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/results/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/plotsANDfigures/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./inputs/data/').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "## Melding Parameters (These are put to meet rapid ICML rebuttle deadline when merging codes)\n",
    "Train_step_proportion = test_size_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#================================================#\n",
      " Training Datasize: 587 and test datasize: 12.  \n",
      "#================================================#\n"
     ]
    }
   ],
   "source": [
    "#------------------------#\n",
    "# Run External Notebooks #\n",
    "#------------------------#\n",
    "if Option_Function == \"SnP\" or \"AAPL\":\n",
    "    #--------------#\n",
    "    # Get S&P Data #\n",
    "    #--------------#\n",
    "    #=# SnP Constituents #=#\n",
    "    # Load Data\n",
    "    snp_data = pd.read_csv('inputs/data/snp500_data/snp500-adjusted-close.csv')\n",
    "    # Impute data\n",
    "    snp_data = snp_data.interpolate(method='linear')\n",
    "    snp_data = snp_data.fillna(value=None, method='backfill', axis=None, limit=None, downcast=None)\n",
    "    # Format Data\n",
    "    ## Index by Time\n",
    "    snp_data['date'] = pd.to_datetime(snp_data['date'],infer_datetime_format=True)\n",
    "    #-------------------------------------------------------------------------------#\n",
    "\n",
    "    #=# SnP Index #=#\n",
    "    ## Read Regression Target\n",
    "#     snp_index_target_data = pd.read_csv('inputs/data/snp500_data/GSPC.csv')\n",
    "    snp_index_target_data = pd.read_csv('inputs/data/snp500_data/SNP.csv',sep='\\t')\n",
    "    # Impute data\n",
    "    snp_index_target_data = snp_index_target_data.interpolate(method='linear')\n",
    "    snp_index_target_data = snp_index_target_data.fillna(value=None, method='backfill', axis=None, limit=None, downcast=None)\n",
    "    TailLength = int(np.minimum(snp_index_target_data.shape[0],snp_data.shape[0]))\n",
    "    ## Get (Reference) Dates\n",
    "    dates_temp = pd.to_datetime(snp_data['date'],infer_datetime_format=True).tail(TailLength)\n",
    "    ## Format Target\n",
    "    snp_index_target_data = pd.DataFrame({'SnP_Index': snp_index_target_data['sp500'],'DATE':dates_temp.reset_index(drop=True)})\n",
    "    snp_index_target_data['DATE'] = pd.to_datetime(snp_index_target_data['DATE'],infer_datetime_format=True)\n",
    "    snp_index_target_data.set_index('DATE', drop=True, inplace=True)\n",
    "    snp_index_target_data.index.names = [None]\n",
    "    #-------------------------------------------------------------------------------#\n",
    "    \n",
    "    ## Get Rid of Rubbish\n",
    "    snp_data.set_index('date', drop=True, inplace=True)\n",
    "    snp_data.index.names = [None]\n",
    "    ## Get Rid of NAs and Expired Trends\n",
    "    snp_data = (snp_data.tail(TailLength)).dropna(axis=1).fillna(0)\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    # Apple\n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    if Option_Function == \"AAPL\":\n",
    "        snp_index_target_data = snp_data[{'AAPL'}]\n",
    "        snp_data = snp_data[{'IBM','QCOM','MSFT','CSCO','ADI','MU','MCHP','NVR','NVDA','GOOGL','GOOG'}]\n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    # Get Return(s)\n",
    "    snp_data_returns = snp_data.diff().iloc[1:]\n",
    "    snp_index_target_data_returns = snp_index_target_data.diff().iloc[1:]\n",
    "    #--------------------------------------------------------#\n",
    "    \n",
    "    #-------------#\n",
    "    # Subset Data #\n",
    "    #-------------#\n",
    "    # Get indices\n",
    "    N_train_step = int(round(snp_index_target_data_returns.shape[0]*Train_step_proportion,0))\n",
    "    N_test_set = int(snp_index_target_data_returns.shape[0] - round(snp_index_target_data_returns.shape[0]*Train_step_proportion,0))\n",
    "    # # Get Datasets\n",
    "    X_train = snp_data_returns[:N_train_step]\n",
    "    X_test = snp_data_returns[-N_test_set:]\n",
    "    ## Coerce into format used in benchmark model(s)\n",
    "    data_x = X_train\n",
    "    data_x_test = X_test\n",
    "    # Get Targets \n",
    "    data_y = snp_index_target_data_returns[:N_train_step]\n",
    "    data_y_test = snp_index_target_data_returns[-N_test_set:]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    data_x = scaler.fit_transform(data_x)\n",
    "    data_x_test = scaler.transform(data_x_test)\n",
    "\n",
    "    # # Update User\n",
    "    print('#================================================#')\n",
    "    print(' Training Datasize: '+str(X_train.shape[0])+' and test datasize: ' + str(X_test.shape[0]) + '.  ')\n",
    "    print('#================================================#')\n",
    "\n",
    "    # # Set First Run to Off\n",
    "    First_run = False\n",
    "\n",
    "    #-----------#\n",
    "    # Plot Data #\n",
    "    #-----------#\n",
    "    fig = snp_data_returns.plot(figsize=(16, 16))\n",
    "    fig.get_legend().remove()\n",
    "    plt.title(\"S&P Data Returns\")\n",
    "\n",
    "    # SAVE Figure to .eps\n",
    "    plt.savefig('./outputs/plotsANDfigures/SNP_Data_returns.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "# Run External Notebooks #\n",
    "#------------------------#\n",
    "if Option_Function == \"crypto\":\n",
    "    #--------------#\n",
    "    # Prepare Data #\n",
    "    #--------------#\n",
    "    # Read Dataset\n",
    "    crypto_data = pd.read_csv('inputs/data/cryptocurrencies/Cryptos_All_in_one.csv')\n",
    "    # Format Date-Time\n",
    "    crypto_data['Date'] = pd.to_datetime(crypto_data['Date'],infer_datetime_format=True)\n",
    "    crypto_data.set_index('Date', drop=True, inplace=True)\n",
    "    crypto_data.index.names = [None]\n",
    "\n",
    "    # Remove Missing Data\n",
    "    crypto_data = crypto_data[crypto_data.isna().any(axis=1)==False]\n",
    "\n",
    "    # Get Returns\n",
    "    crypto_returns = crypto_data.diff().iloc[1:]\n",
    "\n",
    "    # Parse Regressors from Targets\n",
    "    ## Get Regression Targets\n",
    "    crypto_target_data = pd.DataFrame({'BITCOIN-closing':crypto_returns['BITCOIN-Close']})\n",
    "    ## Get Regressors\n",
    "    crypto_data_returns = crypto_returns.drop('BITCOIN-Close', axis=1)  \n",
    "\n",
    "    #-------------#\n",
    "    # Subset Data #\n",
    "    #-------------#\n",
    "    # Get indices\n",
    "    N_train_step = int(round(crypto_data_returns.shape[0]*Train_step_proportion,0))\n",
    "    N_test_set = int(crypto_data_returns.shape[0] - round(crypto_data_returns.shape[0]*Train_step_proportion,0))\n",
    "    # # Get Datasets\n",
    "    X_train = crypto_data_returns[:N_train_step]\n",
    "    X_test = crypto_data_returns[-N_test_set:]\n",
    "\n",
    "    ## Coerce into format used in benchmark model(s)\n",
    "    data_x = X_train\n",
    "    data_x_test = X_test\n",
    "    # Get Targets \n",
    "    data_y = crypto_target_data[:N_train_step]\n",
    "    data_y_test = crypto_target_data[-N_test_set:]\n",
    "\n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    data_x = scaler.fit_transform(data_x)\n",
    "    data_x_test = scaler.transform(data_x_test)\n",
    "\n",
    "    # # Update User\n",
    "    print('#================================================#')\n",
    "    print(' Training Datasize: '+str(X_train.shape[0])+' and test datasize: ' + str(X_test.shape[0]) + '.  ')\n",
    "    print('#================================================#')\n",
    "\n",
    "    # # Set First Run to Off\n",
    "    First_run = False\n",
    "\n",
    "    #-----------#\n",
    "    # Plot Data #\n",
    "    #-----------#\n",
    "    fig = crypto_data_returns.plot(figsize=(16, 16))\n",
    "    fig.get_legend().remove()\n",
    "    plt.title(\"Crypto_Market Returns\")\n",
    "\n",
    "    # SAVE Figure to .eps\n",
    "    plt.savefig('./outputs/plotsANDfigures/Crypto_Data_returns.pdf', format='pdf')\n",
    "    \n",
    "    \n",
    "    # Set option to SnP to port rest of pre-processing automatically that way\n",
    "    Option_Function = \"SnP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICML Coercsion\n",
    "y_train = np.array(data_y).reshape(-1)\n",
    "y_test = np.array(data_y_test).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
