{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation: Semi-Supervised Architope (Financial Data)\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"PC-NNs\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 0.1\n",
    "min_height = 50\n",
    "# Ablation Finess\n",
    "N_plot_finess = 15; min_parts_threshold = .001; max_parts_threshold = 0.9\n",
    "Tied_Neurons_Q = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------#\n",
    "# Only For Motivational Example Only #\n",
    "#------------------------------------#\n",
    "## Hyperparameters\n",
    "percentage_in_row = .25\n",
    "N = 10**4\n",
    "\n",
    "def f_1(x):\n",
    "    return x\n",
    "def f_2(x):\n",
    "    return x**2\n",
    "x_0 = 0\n",
    "x_end = 1\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "#================================================#\n",
      " Training Datasize: 60 and test datasize: 539.  \n",
      "#================================================#\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "if Option_Function != \"Motivational_Example\": \n",
    "    exec(open('Financial_Data_Preprocessor.py').read())\n",
    "else:\n",
    "    print(1)\n",
    "    exec(open('Motivational_Example.py').read())\n",
    "    print(\"Training Data size: \",X_train.shape[0])\n",
    "# Import time separately\n",
    "import time\n",
    "\n",
    "# TEMP\n",
    "# import pickle_compat\n",
    "# pickle_compat.patch()\n",
    "# param_grid_Vanilla_Nets['input_dim']=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit Partion Builder:\n",
    "Implements exactly Algorithm 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(X_in,\n",
    "                               y_in,\n",
    "                               N_parts_to_get=4):\n",
    "\n",
    "    # Compute Size of each part\n",
    "    size_part_reference = int(round(X_in.shape[0]/N_parts_to_get))\n",
    "\n",
    "    # Apply random bijection #\n",
    "    #------------------------#\n",
    "    ## Get random bijection indices\n",
    "    random_bijection_indices = np.random.choice(range(X_in.shape[0]),size=X_in.shape[0], replace=False)\n",
    "    ## Apply random bijections\n",
    "    X_in_shuffled = X_in[random_bijection_indices,:]\n",
    "    y_in_shuffled = y_in[random_bijection_indices,:]\n",
    "\n",
    "    # Initialize Lists #\n",
    "    #------------------#\n",
    "    X_parts = []\n",
    "    y_parts = []\n",
    "\n",
    "    for i_th_part_to_get in range(N_parts_to_get):\n",
    "        # Build random balls #\n",
    "        #--------------------#\n",
    "        ## Sample random radius\n",
    "        size_part = int(np.maximum(1,np.round(size_part_reference*np.random.uniform(low=.5,high=1.5,size=1)[0])))\n",
    "        ## Sample random point\n",
    "        X_center_loop_index = np.random.choice(range(X_in_shuffled.shape[0]),size=1, replace=False)\n",
    "        X_center_loop = X_in_shuffled[X_center_loop_index,:]\n",
    "        ## Compute Typical Distances from Center\n",
    "        distances_loop = X_center_loop-X_in_shuffled\n",
    "        distances_loop = np.linalg.norm(distances_loop, axis=1)\n",
    "\n",
    "        # Remove Random Ball from Dataset\n",
    "        if size_part <= len(distances_loop):\n",
    "            ## Identify indices\n",
    "            indices_smallest_to_random_ball = np.argsort(distances_loop)[:size_part]\n",
    "        else:\n",
    "            print('Final Loop')\n",
    "            indices_smallest_to_random_ball = np.array(range(X_in_shuffled.shape[0]))\n",
    "        ## Extract Parts\n",
    "        X_current_part_loop = X_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        y_current_part_loop = y_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        ## Append to List of Parts\n",
    "        X_parts.append(X_current_part_loop)\n",
    "        y_parts.append(y_current_part_loop)\n",
    "\n",
    "        # Remove Selected Entries From Array #\n",
    "        #------------------------------------#\n",
    "        X_in_shuffled = np.delete(X_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "        y_in_shuffled = np.delete(y_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "\n",
    "        # Failsafe if procedure has terminated\n",
    "        if X_in_shuffled.shape[0] == 0:\n",
    "            print('breaking early')\n",
    "            break\n",
    "    # Count Number of Parts Generated        \n",
    "    N_parts_generated = len(X_parts)\n",
    "    # Output Parts\n",
    "    return X_parts, y_parts, N_parts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loop\n",
      "breaking early\n",
      "The_parts_listhe number of parts are: 2.\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 0/2Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Train on 27 samples\n",
      "Epoch 1/50\n",
      "27/27 [==============================] - 0s 12ms/sample - loss: 12.2518 - mse: 262.7237 - mae: 12.2518 - mape: 112.3723\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 222us/sample - loss: 11.9819 - mse: 252.8321 - mae: 11.9819 - mape: 107.7773\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 319us/sample - loss: 11.7122 - mse: 243.1610 - mae: 11.7122 - mape: 103.1862\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 245us/sample - loss: 11.4426 - mse: 233.7110 - mae: 11.4426 - mape: 98.6007\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 283us/sample - loss: 11.1730 - mse: 224.4829 - mae: 11.1730 - mape: 94.0190\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 326us/sample - loss: 10.9036 - mse: 215.4767 - mae: 10.9036 - mape: 89.4397\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 256us/sample - loss: 10.6343 - mse: 206.6920 - mae: 10.6343 - mape: 84.8618\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 335us/sample - loss: 10.3651 - mse: 198.1285 - mae: 10.3651 - mape: 80.2841\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 222us/sample - loss: 10.0960 - mse: 189.7856 - mae: 10.0960 - mape: 75.7062\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 184us/sample - loss: 9.8356 - mse: 181.6622 - mae: 9.8356 - mape: 73.5822\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 276us/sample - loss: 9.5884 - mse: 173.7881 - mae: 9.5884 - mape: 72.6514\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 275us/sample - loss: 9.3413 - mse: 166.1513 - mae: 9.3413 - mape: 71.4711\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 250us/sample - loss: 9.0944 - mse: 158.7438 - mae: 9.0944 - mape: 70.0963\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 596us/sample - loss: 8.8475 - mse: 151.5596 - mae: 8.8475 - mape: 68.5644\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 263us/sample - loss: 8.6103 - mse: 144.5933 - mae: 8.6103 - mape: 67.3930\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 367us/sample - loss: 8.3764 - mse: 137.8662 - mae: 8.3764 - mape: 66.2279\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 363us/sample - loss: 8.1479 - mse: 131.3741 - mae: 8.1479 - mape: 65.1863\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 244us/sample - loss: 7.9182 - mse: 125.1093 - mae: 7.9182 - mape: 63.9663\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 332us/sample - loss: 7.6876 - mse: 119.0645 - mae: 7.6876 - mape: 62.5961\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 290us/sample - loss: 7.4655 - mse: 113.2335 - mae: 7.4655 - mape: 61.2092\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 314us/sample - loss: 7.2917 - mse: 107.6412 - mae: 7.2917 - mape: 60.3244\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 210us/sample - loss: 7.1182 - mse: 102.2944 - mae: 7.1182 - mape: 59.3097\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 282us/sample - loss: 6.9433 - mse: 97.1775 - mae: 6.9433 - mape: 58.1663\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 242us/sample - loss: 6.7675 - mse: 92.2753 - mae: 6.7675 - mape: 56.9113\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 269us/sample - loss: 6.5973 - mse: 87.5738 - mae: 6.5973 - mape: 55.6669\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 306us/sample - loss: 6.4370 - mse: 83.0404 - mae: 6.4370 - mape: 55.3343\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 324us/sample - loss: 6.2736 - mse: 78.6458 - mae: 6.2736 - mape: 54.8120\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 331us/sample - loss: 6.1073 - mse: 74.3930 - mae: 6.1073 - mape: 54.0590\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 295us/sample - loss: 5.9385 - mse: 70.2808 - mae: 5.9385 - mape: 53.1110\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 488us/sample - loss: 5.7676 - mse: 66.3074 - mae: 5.7676 - mape: 51.9946\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 313us/sample - loss: 5.5948 - mse: 62.4711 - mae: 5.5948 - mape: 50.7316\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 380us/sample - loss: 5.4204 - mse: 58.7703 - mae: 5.4204 - mape: 49.3402\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 431us/sample - loss: 5.2446 - mse: 55.2035 - mae: 5.2446 - mape: 47.8470\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 252us/sample - loss: 5.0751 - mse: 51.7792 - mae: 5.0751 - mape: 47.0294\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 391us/sample - loss: 4.9033 - mse: 48.4886 - mae: 4.9033 - mape: 46.0548\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 225us/sample - loss: 4.7296 - mse: 45.3307 - mae: 4.7296 - mape: 44.9405\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 313us/sample - loss: 4.5699 - mse: 42.3044 - mae: 4.5699 - mape: 43.7872\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 193us/sample - loss: 4.4131 - mse: 39.4218 - mae: 4.4131 - mape: 42.5098\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 191us/sample - loss: 4.2545 - mse: 36.6760 - mae: 4.2545 - mape: 41.1403\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 395us/sample - loss: 4.1037 - mse: 34.0647 - mae: 4.1037 - mape: 39.8982\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 366us/sample - loss: 3.9518 - mse: 31.5884 - mae: 3.9518 - mape: 38.5381\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 271us/sample - loss: 3.7972 - mse: 29.2416 - mae: 3.7972 - mape: 37.0369\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 305us/sample - loss: 3.6513 - mse: 27.0196 - mae: 3.6513 - mape: 35.5072\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 198us/sample - loss: 3.5144 - mse: 24.9330 - mae: 3.5144 - mape: 33.9515\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 474us/sample - loss: 3.3750 - mse: 22.9735 - mae: 3.3750 - mape: 32.2741\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 293us/sample - loss: 3.2335 - mse: 21.1340 - mae: 3.2335 - mape: 30.4879\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 481us/sample - loss: 3.0959 - mse: 19.4088 - mae: 3.0959 - mape: 30.3108\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 441us/sample - loss: 2.9549 - mse: 17.7807 - mae: 2.9549 - mape: 29.9720\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 263us/sample - loss: 2.8099 - mse: 16.2503 - mae: 2.8099 - mape: 29.1756\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 265us/sample - loss: 2.6634 - mse: 14.8179 - mae: 2.6634 - mape: 28.2393\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 1/2Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Train on 33 samples\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 0s 11ms/sample - loss: 12.7146 - mse: 271.8063 - mae: 12.7146 - mape: 89.8288\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 505us/sample - loss: 12.3239 - mse: 257.6402 - mae: 12.3239 - mape: 86.4914\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 964us/sample - loss: 12.0209 - mse: 246.9149 - mae: 12.0209 - mape: 83.8582\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 449us/sample - loss: 11.7077 - mse: 236.2815 - mae: 11.7077 - mape: 81.2209\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 509us/sample - loss: 11.3956 - mse: 225.9865 - mae: 11.3956 - mape: 78.6406\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 511us/sample - loss: 11.0889 - mse: 215.8417 - mae: 11.0889 - mape: 76.3571\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 624us/sample - loss: 10.7850 - mse: 205.7107 - mae: 10.7850 - mape: 74.5395\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 1ms/sample - loss: 10.4780 - mse: 195.8230 - mae: 10.4780 - mape: 72.7182\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 633us/sample - loss: 10.1695 - mse: 186.1667 - mae: 10.1695 - mape: 70.9264\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 605us/sample - loss: 9.8515 - mse: 176.4898 - mae: 9.8515 - mape: 69.0768\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 852us/sample - loss: 9.5386 - mse: 167.4573 - mae: 9.5386 - mape: 67.2555\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 920us/sample - loss: 9.2210 - mse: 158.4535 - mae: 9.2210 - mape: 65.2166\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 491us/sample - loss: 8.9210 - mse: 150.4856 - mae: 8.9210 - mape: 63.1988\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 407us/sample - loss: 8.6099 - mse: 142.2709 - mae: 8.6099 - mape: 61.1963\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 513us/sample - loss: 8.2953 - mse: 134.1196 - mae: 8.2953 - mape: 59.4481\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 1ms/sample - loss: 7.9950 - mse: 126.4775 - mae: 7.9950 - mape: 57.8366\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 714us/sample - loss: 7.6980 - mse: 119.3666 - mae: 7.6980 - mape: 56.4899\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 406us/sample - loss: 7.4417 - mse: 112.8704 - mae: 7.4417 - mape: 55.2615\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 513us/sample - loss: 7.2071 - mse: 107.2228 - mae: 7.2071 - mape: 54.1667\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 462us/sample - loss: 6.9699 - mse: 101.3051 - mae: 6.9699 - mape: 52.9943\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 529us/sample - loss: 6.7250 - mse: 95.8007 - mae: 6.7250 - mape: 51.8406\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 361us/sample - loss: 6.5281 - mse: 90.9987 - mae: 6.5281 - mape: 51.0242\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 326us/sample - loss: 6.3672 - mse: 86.8294 - mae: 6.3672 - mape: 50.3327\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 402us/sample - loss: 6.2145 - mse: 83.0202 - mae: 6.2145 - mape: 49.6225\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 327us/sample - loss: 6.0987 - mse: 79.7728 - mae: 6.0987 - mape: 48.8243\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 324us/sample - loss: 6.0001 - mse: 76.7179 - mae: 6.0001 - mape: 48.4483\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 377us/sample - loss: 5.8916 - mse: 73.6550 - mae: 5.8916 - mape: 47.9167\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 422us/sample - loss: 5.7802 - mse: 70.8256 - mae: 5.7802 - mape: 47.0924\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 465us/sample - loss: 5.6688 - mse: 68.0649 - mae: 5.6688 - mape: 46.2389\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 345us/sample - loss: 5.5544 - mse: 65.2479 - mae: 5.5544 - mape: 45.4678\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 347us/sample - loss: 5.4468 - mse: 62.8558 - mae: 5.4468 - mape: 44.5640\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 351us/sample - loss: 5.3424 - mse: 60.7015 - mae: 5.3424 - mape: 43.5526\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 369us/sample - loss: 5.2302 - mse: 58.4609 - mae: 5.2302 - mape: 42.4890\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 349us/sample - loss: 5.1170 - mse: 56.0806 - mae: 5.1170 - mape: 41.6082\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 389us/sample - loss: 4.9955 - mse: 53.5023 - mae: 4.9955 - mape: 40.7850\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 666us/sample - loss: 4.8709 - mse: 50.9485 - mae: 4.8709 - mape: 39.8998\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 369us/sample - loss: 4.7378 - mse: 48.1950 - mae: 4.7378 - mape: 38.9116\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 473us/sample - loss: 4.6124 - mse: 45.7025 - mae: 4.6124 - mape: 37.8693\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 445us/sample - loss: 4.4611 - mse: 42.4918 - mae: 4.4611 - mape: 36.7336\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 451us/sample - loss: 4.3320 - mse: 40.0492 - mae: 4.3320 - mape: 35.5691\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 409us/sample - loss: 4.1859 - mse: 37.2283 - mae: 4.1859 - mape: 34.4024\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 411us/sample - loss: 4.0475 - mse: 34.7253 - mae: 4.0475 - mape: 33.2304\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 404us/sample - loss: 3.9078 - mse: 32.4379 - mae: 3.9078 - mape: 32.0661\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 425us/sample - loss: 3.7848 - mse: 30.5204 - mae: 3.7848 - mape: 31.0284\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 456us/sample - loss: 3.6747 - mse: 28.7837 - mae: 3.6747 - mape: 30.2752\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 589us/sample - loss: 3.5654 - mse: 26.9570 - mae: 3.5654 - mape: 29.7270\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 601us/sample - loss: 3.4457 - mse: 25.2389 - mae: 3.4457 - mape: 29.1085\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 529us/sample - loss: 3.3171 - mse: 23.3815 - mae: 3.3171 - mape: 28.2971\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 423us/sample - loss: 3.1974 - mse: 21.8674 - mae: 3.1974 - mape: 27.2887\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 729us/sample - loss: 3.0849 - mse: 20.5957 - mae: 3.0849 - mape: 26.2451\n"
     ]
    }
   ],
   "source": [
    "N_parts = 2\n",
    "\n",
    "# Initialization(s) #\n",
    "#-------------------#\n",
    "N_neurons = 0\n",
    "L_timer = 0\n",
    "P_timer = 0\n",
    "Mean_Width_Subnetworks = 0\n",
    "\n",
    "# Partitioner Begin #\n",
    "#-------------------#\n",
    "import time\n",
    "partitioning_time_begin = time.time()\n",
    "X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(X_train.to_numpy(),\n",
    "                                                                                     y_train.reshape(-1,1),\n",
    "                                                                                     N_parts)\n",
    "partitioning_time = time.time() - partitioning_time_begin\n",
    "print('The_parts_listhe number of parts are: ' + str(N_parts_Generated_by_Algo_2)+'.')\n",
    "############# Partitioner End ########\n",
    "\n",
    "# Train Sub-Network(s) #\n",
    "#----------------------#\n",
    "# Time-Elapse (Start) for Training on Each Part #\n",
    "PCNN_timer = time.time(); PCNN_timer = -math.inf; N_params_Architope = 0; N_params_tally = 0\n",
    "# Remove Eager Execution Error(s)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# Automatically Initialize Correct Input/Output Dimension(s)\n",
    "param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]; param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "# Decide if/or not to tie neuron numbers of sub-patterns together\n",
    "if Tied_Neurons_Q == True:\n",
    "    param_grid_Vanilla_Nets['height'] = [int(np.maximum(round(param_grid_Vanilla_Nets['height'][0]/N_parts),min_height))]\n",
    "    param_grid_Deep_Classifier['height'] = [int(np.maximum(round(param_grid_Deep_Classifier['height'][0]/N_parts),min_height))]\n",
    "\n",
    "for current_part in range(N_parts_Generated_by_Algo_2):\n",
    "    # Update User #\n",
    "    #-------------#\n",
    "    print('-----------------------------------------------------------')\n",
    "    print('Currently Training Part: '+str(current_part)+'/'+str(N_parts_Generated_by_Algo_2 )+'Total Parts.')\n",
    "    print('-----------------------------------------------------------')\n",
    "    \n",
    "    # Timer for Part\n",
    "    part_training_timer = time.time()\n",
    "    # Get Data for Sub-Pattern\n",
    "    X_loop = pd.DataFrame(X_parts_list[current_part])\n",
    "    y_loop = (y_parts_list[current_part]).reshape(-1,)\n",
    "    # Train ffNN\n",
    "    y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN(n_folds = 4, \n",
    "                                                                                      n_jobs = n_jobs,\n",
    "                                                                                      n_iter = n_iter, \n",
    "                                                                                      param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                      X_train= X_loop, \n",
    "                                                                                      y_train=y_loop,\n",
    "                                                                                      X_test_partial=X_train,\n",
    "                                                                                      X_test=X_test,\n",
    "                                                                                      NOCV=True)\n",
    "    # Reshape y\n",
    "    ## Training\n",
    "    y_train.shape = (y_train.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "    y_hat_part_loop.shape = (y_hat_part_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "    ## Testing\n",
    "    y_test.shape = (y_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "    y_hat_part_loop_test.shape = (y_hat_part_loop_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "\n",
    "    # Append predictions to data-frames\n",
    "    ## If first prediction we initialize data-frames\n",
    "    if current_part==0:\n",
    "        # Register quality\n",
    "        training_quality = np.array(np.abs(y_hat_part_loop-y_train)).reshape(training_quality.shape[0],1)\n",
    "\n",
    "        # Save Predictions\n",
    "        predictions_train = y_hat_part_loop.reshape(y_hat_part_loop.shape[0],1)\n",
    "        predictions_test = y_hat_part_loop_test.reshape(y_hat_part_loop_test.shape[0],1)\n",
    "\n",
    "\n",
    "    ## If not first prediction we append to already initialized dataframes\n",
    "    else:\n",
    "    # Register Best Scores\n",
    "        #----------------------#\n",
    "        # Write Predictions \n",
    "        # Save Predictions\n",
    "        y_hat_train_loop = y_hat_train_full_loop.reshape(predictions_train.shape[0],1)\n",
    "        predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "        y_hat_test_loop = y_hat_test_full_loop.reshape(predictions_test.shape[0],1)\n",
    "        predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "\n",
    "        # Evaluate Errors #\n",
    "        #-----------------#\n",
    "        # Training\n",
    "        prediction_errors = np.abs(y_hat_train_loop-y_train)\n",
    "        training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "\n",
    "    #==============================#\n",
    "    # Update Performance Metric(s) #\n",
    "    #==============================#\n",
    "    part_training_timer = time.time() - part_training_timer\n",
    "    # L-Time\n",
    "    L_timer += partitioning_time\n",
    "    # P-Time\n",
    "    P_timer = max(P_timer,part_training_timer)\n",
    "    # N. Params\n",
    "    N_neurons += N_neurons_PCNN_loop\n",
    "    # Mean Width for Sub-Network(s)\n",
    "    Mean_Width_Subnetworks += param_grid_Vanilla_Nets['height'][0]\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('----------------------------------------------------')\n",
    "print('Feature Generation (Learning Phase): Score Generated')\n",
    "print('----------------------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Function\n",
    "*Made for 1-D output...for multidimensional y just remove reshaping in code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ablate_PCNNs(N_parts,data_y,X_train,X_test,y_test):\n",
    "    #---------------------#\n",
    "    # Building Partitions #\n",
    "    #---------------------#\n",
    "\n",
    "    ############# Partitioner Begin #######\n",
    "    import time\n",
    "    partitioning_time_begin = time.time()\n",
    "    \n",
    "    X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(X_train.to_numpy(),\n",
    "                                                                                         y_train.reshape(-1,1),\n",
    "                                                                                         N_parts)\n",
    "\n",
    "\n",
    "    partitioning_time = time.time() - partitioning_time_begin\n",
    "    print('The_parts_listhe number of parts are: ' + str(N_parts_Generated_by_Algo_2)+'.')\n",
    "    ############# Partitioner End ########\n",
    "\n",
    "\n",
    "\n",
    "    #-----------------------------------------------#\n",
    "    # #### Building Training Predictions on each part\n",
    "    #-----------------------------------------------#\n",
    "    # - Train locally (on each \"naive part\")\n",
    "    # - Generate predictions for (full) training and testings sets respectively, to be used in training the classifer and for prediction, respectively.  \n",
    "    # - Generate predictions on all of testing-set (will be selected between later using classifier)\n",
    "\n",
    "    # Time-Elapse (Start) for Training on Each Part #\n",
    "    Architope_partition_training_begin = time.time()\n",
    "    # Initialize running max for Parallel time\n",
    "    Architope_partitioning_max_time_running = -math.inf # Initialize slowest-time at - infinity to force updating!\n",
    "    # Initialize N_parameter counter for Architope\n",
    "    N_params_Architope = 0; N_params_tally = 0\n",
    "\n",
    "\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    # Train each part!\n",
    "    for current_part in range(Iteration_Length):\n",
    "        #==============#\n",
    "        # Timer(begin) #\n",
    "        #==============#\n",
    "        current_part_training_time_for_parallel_begin = time.time()\n",
    "\n",
    "\n",
    "        # Initializations #\n",
    "        #-----------------#\n",
    "        # Reload Grid\n",
    "        exec(open('Grid_Enhanced_Network.py').read())\n",
    "        # Modify heights according to optimal (data-driven) rule (with threshold)\n",
    "        current_height = np.ceil(np.array(param_grid_Vanilla_Nets['height'])*N_ratios[current_part])\n",
    "        current_height_threshold = np.repeat(min_height,(current_height.shape[0]))\n",
    "        if Fix_Neurons_Q == True:\n",
    "            current_height = np.maximum(current_height,current_height_threshold)/N_parts_Generated_by_Algo_2\n",
    "        current_height = current_height.astype(int).tolist()\n",
    "        param_grid_Vanilla_Nets['height'] = current_height\n",
    "        # Automatically Fix Input Dimension\n",
    "        param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]\n",
    "        param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "        \n",
    "        # Update Parameter Counter for PC-NNs (tally parameter count for sub-patterns)\n",
    "        N_params_tally += (current_height[0])*(param_grid_Vanilla_Nets['depth'][0])\n",
    "\n",
    "        # Update User #\n",
    "        #-------------#\n",
    "        print('Status: Current part: ' + str(current_part) + ' out of : '+str(len(X_parts_list)) +' parts.')\n",
    "        print('Heights to iterate over: '+str(current_height))\n",
    "\n",
    "        # Generate Prediction(s) on current Part #\n",
    "        #----------------------------------------#\n",
    "        # Failsafe (number of data-points)\n",
    "        CV_folds_failsafe = min(CV_folds,max(1,(X_train.shape[0]-1)))\n",
    "        # Train Network\n",
    "        y_hat_train_full_loop, y_hat_test_full_loop, N_params_Architope_loop = build_ffNN(n_folds = CV_folds_failsafe, \n",
    "                                                                                          n_jobs = n_jobs,\n",
    "                                                                                          n_iter = n_iter, \n",
    "                                                                                          param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                          X_train= pd.DataFrame(X_parts_list[current_part]), \n",
    "                                                                                          y_train=(y_parts_list[current_part]).reshape(-1,),\n",
    "                                                                                          X_test_partial=X_train,\n",
    "                                                                                          X_test=X_test,\n",
    "                                                                                          NOCV=True)\n",
    "        #put shape formats in order\n",
    "        y_train.shape = (y_train.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_train_full_loop.shape = (y_hat_train_full_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_test.shape = (y_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_test_full_loop.shape = (y_hat_test_full_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        # Append predictions to data-frames\n",
    "        ## If first prediction we initialize data-frames\n",
    "        if current_part==0:\n",
    "            # Register quality\n",
    "            training_quality = np.array(np.abs(y_hat_train_full_loop-y_train))\n",
    "            training_quality = training_quality.reshape(training_quality.shape[0],1)\n",
    "\n",
    "            # Save Predictions\n",
    "            predictions_train = y_hat_train_full_loop\n",
    "            predictions_train = predictions_train.reshape(predictions_train.shape[0],1)\n",
    "            predictions_test = y_hat_test_full_loop\n",
    "            predictions_test = predictions_test.reshape(predictions_test.shape[0],1)\n",
    "\n",
    "\n",
    "        ## If not first prediction we append to already initialized dataframes\n",
    "        else:\n",
    "        # Register Best Scores\n",
    "            #----------------------#\n",
    "            # Write Predictions \n",
    "            # Save Predictions\n",
    "            y_hat_train_loop = y_hat_train_full_loop.reshape(predictions_train.shape[0],1)\n",
    "            predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "            y_hat_test_loop = y_hat_test_full_loop.reshape(predictions_test.shape[0],1)\n",
    "            predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "\n",
    "            # Evaluate Errors #\n",
    "            #-----------------#\n",
    "            # Training\n",
    "            prediction_errors = np.abs(y_hat_train_loop-y_train)\n",
    "            training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "\n",
    "        #============#\n",
    "        # Timer(end) #\n",
    "        #============#\n",
    "        current_part_training_time_for_parallel = time.time() - current_part_training_time_for_parallel_begin\n",
    "        Architope_partitioning_max_time_running = max(Architope_partitioning_max_time_running,current_part_training_time_for_parallel)\n",
    "\n",
    "        #============---===============#\n",
    "\n",
    "        # N_parameter Counter (Update) #\n",
    "        #------------===---------------#\n",
    "        N_params_Architope = N_params_Architope + N_params_Architope_loop\n",
    "\n",
    "    # Update User\n",
    "    #-------------#\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print('----------------------------------------------------')\n",
    "    print('Feature Generation (Learning Phase): Score Generated')\n",
    "    print('----------------------------------------------------')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "\n",
    "    # Time-Elapsed Training on Each Part\n",
    "    Architope_partition_training = time.time() - Architope_partition_training_begin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------#\n",
    "    # Train Deep Zero-Sets #\n",
    "    #----------------------#\n",
    "    #### Deep Classifier\n",
    "    # Prepare Labels/Classes\n",
    "    # Time-Elapsed Training Deep Classifier\n",
    "    Architope_deep_classifier_training_begin = time.time()\n",
    "    # Initialize Classes Labels\n",
    "    partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "    partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "    # Build Classes\n",
    "    for part_column_i in range(1,(training_quality.shape[1])):\n",
    "        partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                               (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                              ],axis=1)\n",
    "    # Convert to integers\n",
    "    partition_labels_training = partition_labels_training+0\n",
    "    #### Re-Load Grid and Redefine Relevant Input/Output dimensions in dictionary.\n",
    "    # Re-Load Hyper-parameter Grid\n",
    "    exec(open('Grid_Enhanced_Network.py').read())\n",
    "    # Re-Load Helper Function(s)\n",
    "    exec(open('Helper_Functions.py').read())\n",
    "    param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "    param_grid_Deep_Classifier['output_dim'] = [partition_labels_training.shape[1]]\n",
    "    ## Re-adjust heights\n",
    "    if Fix_Neurons_Q == True:\n",
    "        param_grid_Deep_Classifier['height'] = [int(max(round(param_grid_Deep_Classifier['height'][0]/N_parts_Generated_by_Algo_2,0),1))]\n",
    "    \n",
    "    height_hist += param_grid_Deep_Classifier['height'][0]\n",
    "\n",
    "    \n",
    "    # Update Parameter Counter for PC-NNs (tally parameter count for sub-patterns)\n",
    "    N_params_tally += (param_grid_Deep_Classifier['height'][0])*(param_grid_Deep_Classifier['depth'][0])\n",
    "    \n",
    "    #### Train Deep Classifier\n",
    "    # Train simple deep classifier\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter =n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train.values, \n",
    "                                                                                                        y_train = partition_labels_training.values,\n",
    "                                                                                                        X_test = X_test.values)\n",
    "    # COMMENT: .values() is used to convert the Pandas Dataframes here, and not in the vanilla ffNNs, since the former is coded in Keras and the latter in tensorflow.  \n",
    "    # Time-Elapsed Training Deep Classifier\n",
    "    Architope_deep_classifier_training = time.time() - Architope_deep_classifier_training_begin\n",
    "\n",
    "    ##### Get Binary Classes (Discontinuous Unit)\n",
    "    #Maps deep classifier's outputs $\\tilde{C}(x)\\triangleq \\hat{s}(x)$ to deep zero-sets $I_{(.5,1]}\\circ \\sigma_{\\mbox{sigmoid}}(\\tilde{C}(x))$.\n",
    "\n",
    "    # Training Set\n",
    "    predicted_classes_train = ((predicted_classes_train>.5)*1).astype(int)\n",
    "    #### OLD: Architope_prediction_y_train = np.take_along_axis(predictions_train, predicted_classes_train[:,None], axis=1)\n",
    "    # Testing Set\n",
    "    predicted_classes_test = ((predicted_classes_test > .5)*1).astype(int)\n",
    "    #### OLD: Architope_prediction_y_test = np.take_along_axis(predictions_test, predicted_classes_test[:,None], axis=1)\n",
    "    #### Get PC-NN Prediction(s)\n",
    "    # Comuptes $\\sum_{n=1}^N \\, \\hat{f}(x)\\cdot I_{K_n}$\n",
    "    # Train\n",
    "    Architope_prediction_y_train = (predictions_train*predicted_classes_train).sum(axis=1)\n",
    "    # Test\n",
    "    Architope_prediction_y_test = (predictions_test*predicted_classes_test).sum(axis=1)\n",
    "\n",
    "\n",
    "    # Compute Peformance\n",
    "    performance_Architope = reporter(y_train_hat_in=Architope_prediction_y_train,\n",
    "                                    y_test_hat_in=Architope_prediction_y_test,\n",
    "                                    y_train_in=y_train,\n",
    "                                    y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_Architope.to_latex((results_tables_path+\"Architopes_full_performance.tex\"))\n",
    "\n",
    "    # Update User\n",
    "    print(performance_Architope)\n",
    "\n",
    "    ### Model Complexity/Efficiency Metrics\n",
    "    # Compute Parameters for composite models #\n",
    "    #-----------------------------------------#\n",
    "    N_params_Architope_full = N_params_Architope + N_params_deep_classifier\n",
    "\n",
    "    # Build AIC-like Metric #\n",
    "    #-----------------------#\n",
    "    AIC_like = 2*(N_params_Architope_full - np.log((performance_Architope['test']['MAE'])))\n",
    "    AIC_like = np.round(AIC_like,3)\n",
    "    Efficiency = np.log(N_params_Architope_full) *(performance_Architope['test']['MAE'])\n",
    "    Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "    # Build Table #\n",
    "    #-------------#\n",
    "    Architope_Model_Complexity_full = pd.DataFrame({'L-time': [Architope_partition_training],\n",
    "                                                  'P-time':[Architope_partitioning_max_time_running],\n",
    "                                                  'N_params_expt': [N_params_Architope_full],\n",
    "                                                  'AIC-like': [AIC_like],\n",
    "                                                  'Eff': [Efficiency]})\n",
    "\n",
    "\n",
    "    # Write Required Training Time(s)\n",
    "    Architope_Model_Complexity_full.to_latex((results_tables_path+\"Architope_full_model_complexities.tex\"))\n",
    "\n",
    "    #--------------======---------------#\n",
    "    # Display Required Training Time(s) #\n",
    "    #--------------======---------------#\n",
    "    print(Architope_Model_Complexity_full)\n",
    "    \n",
    "    # Compute Mean Subnetwork Widths\n",
    "    height_mean = height_hist/N_parts_Generated_by_Algo_2\n",
    "    \n",
    "    # Return Performance Metrics\n",
    "    return performance_Architope, Architope_Model_Complexity_full, N_parts_Generated_by_Algo_2, N_params_tally, height_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit Partitioner:\n",
    "Here we use $\\Delta_{in} = Q_{q}\\left(\\Delta(\\mathbb{X})\\right)$ where $\\Delta(\\mathbb{X})$ is the vector of (Euclidean) distances between the given data-points, $q \\in (0,1)$ is a hyper-parameter, and $Q$ is the empirical quantile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import distance_matrix\n",
    "# def Random_Lipschitz_Partioner_IMPLICIT(Min_data_size_percentage,\n",
    "#                                 q_in, \n",
    "#                                 X_train_in,\n",
    "#                                 y_train_in, \n",
    "#                                 CV_folds_failsafe, \n",
    "#                                 min_size,\n",
    "#                                 N_parts_to_generate=5):\n",
    "       \n",
    "#     #-----------------------#\n",
    "#     # Reset Seed Internally #\n",
    "#     #-----------------------#\n",
    "#     random.seed(2020)\n",
    "#     np.random.seed(2020)\n",
    "\n",
    "#     #-------------------------------------------#\n",
    "#     #-------------------------------------------#\n",
    "#     # 1) Sample radius from unifom distribution #\n",
    "#     #-------------------------------------------#\n",
    "#     #-------------------------------------------#\n",
    "#     alpha = np.random.uniform(low=.25,high=.5,size=1)[0]\n",
    "\n",
    "#     #-------------------------------------#\n",
    "#     #-------------------------------------#\n",
    "#     # 2) Apply Random Bijection (Shuffle) #\n",
    "#     #-------------------------------------#\n",
    "#     #-------------------------------------#\n",
    "#     X_train_in_shuffled = X_train_in.sample(frac=1)\n",
    "#     y_train_in_shuffled = y_train_in.sample(frac=1)\n",
    "\n",
    "#     #--------------------#\n",
    "#     #--------------------#\n",
    "#     # X) Initializations #\n",
    "#     #--------------------#\n",
    "#     #--------------------#\n",
    "#     # Compute-data-driven radius\n",
    "#     Delta_X = distance_matrix(X_train_in_shuffled,X_train_in_shuffled)[::,0]\n",
    "#     Delta_in = np.quantile(Delta_X,q_in)\n",
    "\n",
    "#     # Initialize Random Radius\n",
    "#     rand_radius = Delta_in*alpha\n",
    "\n",
    "#     # Initialize Data_sizes & ratios\n",
    "#     N_tot = X_train_in.shape[0] #<- Total number of data-points in input data-set!\n",
    "#     N_radios = np.array([])\n",
    "#     N_pool_train_loop = N_tot\n",
    "#     # Initialize List of Dataframes\n",
    "#     X_internal_train_list = list()\n",
    "#     y_internal_train_list = list()\n",
    "\n",
    "#     # Initialize Partioned Data-pool\n",
    "#     X_internal_train_pool = X_train_in_shuffled\n",
    "#     y_internal_train_pool = y_train_in_shuffled\n",
    "\n",
    "#     # Initialize counter \n",
    "#     part_current_loop = 0\n",
    "\n",
    "#     #----------------------------#\n",
    "#     #----------------------------#\n",
    "#     # 3) Iteratively Build Parts #\n",
    "#     #----------------------------#\n",
    "#     #----------------------------#\n",
    "\n",
    "# #     while ((N_pool_train_loop/N_tot > Min_data_size_percentage) or (X_internal_train_pool.empty == False)):\n",
    "#     for ith_part in range(N_parts_to_generate):\n",
    "#         print('building'+str(ith_part)+'part')\n",
    "#         # Extract Current Center\n",
    "#         center_loop = X_internal_train_pool.iloc[0]\n",
    "#         # Compute Distances\n",
    "#         ## Training\n",
    "#         distances_pool_loop_train = X_internal_train_pool.sub(center_loop)\n",
    "#         distances_pool_loop_train = np.array(np.sqrt(np.square(distances_pool_loop_train).sum(axis=1)))\n",
    "#         # Evaluate which Distances are less than the given random radius\n",
    "#         Part_train_loop = X_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "#         Part_train_loop_y = y_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "\n",
    "#         # Resample (unifomly and randomly) if too few datapoints in this batch\n",
    "#         if X_internal_train_pool.shape[0] <= min_size:\n",
    "#             get_index = np.isin(range(X_internal_train_pool.shape[0]), np.random.choice(range(X_internal_train_pool.shape[0]), size=min_size, replace=False))\n",
    "#             Part_train_loop = X_internal_train_pool[get_index]\n",
    "#             Part_train_loop_y = y_internal_train_pool[get_index]\n",
    "\n",
    "#         # Append Current part to list\n",
    "#         X_internal_train_list.append(Part_train_loop)\n",
    "#         y_internal_train_list.append(Part_train_loop_y)\n",
    "\n",
    "#         # Remove current part from pool \n",
    "#         X_internal_train_pool = X_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "#         y_internal_train_pool = y_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "\n",
    "#         # Update Current size of pool of training data\n",
    "#         N_pool_train_loop = X_internal_train_pool.shape[0]\n",
    "#         N_radios = np.append(N_radios,(N_pool_train_loop/N_tot))\n",
    "\n",
    "#         # Update Counter\n",
    "#         part_current_loop = part_current_loop +1\n",
    "        \n",
    "#         # Update User\n",
    "#         print((N_pool_train_loop/N_tot))\n",
    "\n",
    "\n",
    "#     # Post processing #\n",
    "#     #-----------------#\n",
    "#     # Remove Empty Partitions\n",
    "#     N_radios = N_radios[N_radios>0]\n",
    "    \n",
    "\n",
    "\n",
    "#     # Set good lists to regular lists\n",
    "#     X_internal_train_list = X_internal_train_list_good\n",
    "#     y_internal_train_list = y_internal_train_list_good\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Return Value #\n",
    "#     #--------------#\n",
    "#     return [X_internal_train_list, y_internal_train_list, N_radios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Random_Lipschitz_Partioner(Min_data_size_percentage,q_in, X_train_in,y_train_in, CV_folds_failsafe, min_size):\n",
    "       \n",
    "#     #-----------------------#\n",
    "#     # Reset Seed Internally #\n",
    "#     #-----------------------#\n",
    "#     random.seed(2020)\n",
    "#     np.random.seed(2020)\n",
    "\n",
    "#     #-------------------------------------------#\n",
    "#     #-------------------------------------------#\n",
    "#     # 1) Sample radius from unifom distribution #\n",
    "#     #-------------------------------------------#\n",
    "#     #-------------------------------------------#\n",
    "#     alpha = np.random.uniform(low=.25,high=.5,size=1)[0]\n",
    "\n",
    "#     #-------------------------------------#\n",
    "#     #-------------------------------------#\n",
    "#     # 2) Apply Random Bijection (Shuffle) #\n",
    "#     #-------------------------------------#\n",
    "#     #-------------------------------------#\n",
    "#     X_train_in_shuffled = X_train_in#.sample(frac=1)\n",
    "#     y_train_in_shuffled = y_train_in#.sample(frac=1)\n",
    "\n",
    "#     #--------------------#\n",
    "#     #--------------------#\n",
    "#     # X) Initializations #\n",
    "#     #--------------------#\n",
    "#     #--------------------#\n",
    "#     # Compute-data-driven radius\n",
    "#     Delta_X = distance_matrix(X_train_in_shuffled,X_train_in_shuffled)[::,0]\n",
    "#     Delta_in = np.quantile(Delta_X,q_in)\n",
    "\n",
    "#     # Initialize Random Radius\n",
    "#     rand_radius = Delta_in*alpha\n",
    "\n",
    "#     # Initialize Data_sizes & ratios\n",
    "#     N_tot = X_train_in.shape[0] #<- Total number of data-points in input data-set!\n",
    "#     N_radios = np.array([])\n",
    "#     N_pool_train_loop = N_tot\n",
    "#     # Initialize List of Dataframes\n",
    "#     X_internal_train_list = list()\n",
    "#     y_internal_train_list = list()\n",
    "\n",
    "#     # Initialize Partioned Data-pool\n",
    "#     X_internal_train_pool = X_train_in_shuffled\n",
    "#     y_internal_train_pool = y_train_in_shuffled\n",
    "\n",
    "#     # Initialize counter \n",
    "#     part_current_loop = 0\n",
    "\n",
    "#     #----------------------------#\n",
    "#     #----------------------------#\n",
    "#     # 3) Iteratively Build Parts #\n",
    "#     #----------------------------#\n",
    "#     #----------------------------#\n",
    "\n",
    "#     while ((N_pool_train_loop/N_tot > Min_data_size_percentage) or (X_internal_train_pool.empty == False)):\n",
    "#         # Extract Current Center\n",
    "#         center_loop = X_internal_train_pool.iloc[0]\n",
    "#         # Compute Distances\n",
    "#         ## Training\n",
    "#         distances_pool_loop_train = X_internal_train_pool.sub(center_loop)\n",
    "#         distances_pool_loop_train = np.array(np.sqrt(np.square(distances_pool_loop_train).sum(axis=1)))\n",
    "#         # Evaluate which Distances are less than the given random radius\n",
    "#         Part_train_loop = X_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "#         Part_train_loop_y = y_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "\n",
    "#         # Remove all data-points which are \"too small\"\n",
    "#         if X_internal_train_pool.shape[0] > max(CV_folds,4):\n",
    "#             # Append Current part to list\n",
    "#             X_internal_train_list.append(Part_train_loop)\n",
    "#             y_internal_train_list.append(Part_train_loop_y)\n",
    "\n",
    "#         # Remove current part from pool \n",
    "#         X_internal_train_pool = X_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "#         y_internal_train_pool = y_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "\n",
    "#         # Update Current size of pool of training data\n",
    "#         N_pool_train_loop = X_internal_train_pool.shape[0]\n",
    "#         N_radios = np.append(N_radios,(N_pool_train_loop/N_tot))\n",
    "\n",
    "#         # Update Counter\n",
    "#         part_current_loop = part_current_loop +1\n",
    "        \n",
    "#         # Update User\n",
    "#         print((N_pool_train_loop/N_tot))\n",
    "\n",
    "\n",
    "#     # Post processing #\n",
    "#     #-----------------#\n",
    "#     # Remove Empty Partitions\n",
    "#     N_radios = N_radios[N_radios>0]\n",
    "    \n",
    "    \n",
    "#     #-----------------------------------------------------------------#\n",
    "#     # Combine parts which are too small to perform CV without an error\n",
    "#     #-----------------------------------------------------------------#\n",
    "#     # Initialize lists (partitions) with \"enough\" datums per part\n",
    "#     X_internal_train_list_good = list()\n",
    "#     y_internal_train_list_good = list()\n",
    "#     X_small_parts = list()\n",
    "#     y_small_parts = list()\n",
    "#     # Initialize first list item test\n",
    "#     is_first = True\n",
    "#     # Initialize counter\n",
    "#     goods_counter = 0\n",
    "#     for search_i in range(len(X_internal_train_list)):\n",
    "#         number_of_instances_in_part = len(X_internal_train_list[search_i]) \n",
    "#         if number_of_instances_in_part < max(CV_folds_failsafe,min_size):\n",
    "#             # Check if first \n",
    "#             if is_first:\n",
    "#                 # Initialize set of small X_parts\n",
    "#                 X_small_parts = X_internal_train_list[search_i]\n",
    "#                 # Initialize set of small y_parts\n",
    "#                 y_small_parts = y_internal_train_list[search_i]\n",
    "\n",
    "#                 # Set is_first to false\n",
    "#                 is_first = False\n",
    "#             else:\n",
    "#                 X_small_parts = X_small_parts.append(X_internal_train_list[search_i])\n",
    "#                 y_small_parts = np.append(y_small_parts,y_internal_train_list[search_i])\n",
    "# #                 y_small_parts = y_small_parts.append(y_internal_train_list[search_i])\n",
    "#         else:\n",
    "#             # Append to current list\n",
    "#             X_internal_train_list_good.append(X_internal_train_list[search_i])\n",
    "#             y_internal_train_list_good.append(y_internal_train_list[search_i])\n",
    "#             # Update goods counter \n",
    "#             goods_counter = goods_counter +1\n",
    "\n",
    "#     # Append final one to good list\n",
    "#     X_internal_train_list_good.append(X_small_parts)\n",
    "#     y_internal_train_list_good.append(y_small_parts)\n",
    "\n",
    "#     # reset is_first to false (inscase we want to re-run this particular block)\n",
    "#     is_first = True\n",
    "\n",
    "#     # Set good lists to regular lists\n",
    "#     X_internal_train_list = X_internal_train_list_good\n",
    "#     y_internal_train_list = y_internal_train_list_good\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Return Value #\n",
    "#     #--------------#\n",
    "#     return [X_internal_train_list, y_internal_train_list, N_radios]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Function (Implicit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Ablate_PCNNs_IMPLICIT(q_inplicit_N_parts,data_y,X_train,X_test,y_test):\n",
    "#     #---------------------#\n",
    "#     # Building Partitions #\n",
    "#     #---------------------#\n",
    "\n",
    "#     ############# Partitioner Begin #######\n",
    "#     import time\n",
    "#     partitioning_time_begin = time.time()\n",
    "#     if Option_Function == 'SnP':\n",
    "#         q_in_auto = q_inplicit_N_parts\n",
    "#         Min_data_size_percentage_auto = .0001\n",
    "#         min_size_part = 5\n",
    "#     else:\n",
    "#         if Option_Function == 'crypto':\n",
    "#             q_in_auto = q_inplicit_N_parts\n",
    "#             Min_data_size_percentage_auto = .0001\n",
    "#             min_size_part = 5\n",
    "#         if Option_Function == 'Motivational_Example':\n",
    "#             q_in_auto = q_inplicit_N_parts\n",
    "#             Min_data_size_percentage_auto = .0001\n",
    "#             min_size_part = 5\n",
    "#             # Partition Based on Y\n",
    "#             holder_temp = data_y\n",
    "#             data_y = X_train\n",
    "#             X_train = holder_temp\n",
    "#         else:\n",
    "#             q_in_auto = q_inplicit_N_parts\n",
    "#             Min_data_size_percentage_auto = .0001\n",
    "#             min_size_part = 5\n",
    "\n",
    "#     # Initialize Number of Parts currently generated\n",
    "#     N_parts_generated = 0\n",
    "#     height_hist = 0\n",
    "\n",
    "#     # Generate Partition (with option to regernerate if only 1 part is randomly produced)\n",
    "#     while N_parts_generated < 2:\n",
    "#         # Generate Parts\n",
    "#         X_parts_list, y_parts_list, N_ratios = Random_Lipschitz_Partioner(Min_data_size_percentage=Min_data_size_percentage_auto, \n",
    "#                                                                           q_in=q_in_auto, \n",
    "#                                                                           X_train_in=X_train, \n",
    "#                                                                           y_train_in=data_y, \n",
    "#                                                                           CV_folds_failsafe=CV_folds,\n",
    "#                                                                           min_size = min_size_part)\n",
    "\n",
    "#         # Update Number of Parts\n",
    "#         N_parts_generated = len(X_parts_list)\n",
    "#         # Shuffle hyperparameters\n",
    "#         Min_data_size_percentage_auto = (Min_data_size_percentage_auto + random.uniform(0,.3)) % 1\n",
    "#         q_in_auto = (q_in_auto + random.uniform(0,.3)) % 1\n",
    "\n",
    "#         # Update User\n",
    "#         print('The_parts_listhe number of parts are: ' + str(len(X_parts_list))+'.')\n",
    "\n",
    "#     # Trash removal (removes empty parts)\n",
    "#     X_parts_list = list(filter(([]).__ne__, X_parts_list))\n",
    "#     y_parts_list = list(filter(([]).__ne__, y_parts_list))\n",
    "\n",
    "\n",
    "#     # ICML Rebuttle Deadline = Coersion!\n",
    "#     if Option_Function == 'Motivational_Example':\n",
    "#         # Flipback After Partitioning Based on Y (since code was made for partitioning in X!)\n",
    "#         holder_temp = data_y\n",
    "#         data_y = X_train\n",
    "#         X_train = holder_temp\n",
    "#         holder_temp = y_parts_list\n",
    "#         y_parts_list = X_parts_list\n",
    "#         X_parts_list = holder_temp\n",
    "\n",
    "\n",
    "#     partitioning_time = time.time() - partitioning_time_begin\n",
    "#     print('The_parts_listhe number of parts are: ' + str(len(X_parts_list))+'.')\n",
    "#     # Record the number of parts:\n",
    "#     N_parts_Generated_by_Algo_2 = len(X_parts_list)\n",
    "#     ############# Partitioner End ########\n",
    "\n",
    "\n",
    "\n",
    "#     #-----------------------------------------------#\n",
    "#     # #### Building Training Predictions on each part\n",
    "#     #-----------------------------------------------#\n",
    "#     # - Train locally (on each \"naive part\")\n",
    "#     # - Generate predictions for (full) training and testings sets respectively, to be used in training the classifer and for prediction, respectively.  \n",
    "#     # - Generate predictions on all of testing-set (will be selected between later using classifier)\n",
    "\n",
    "#     # Time-Elapse (Start) for Training on Each Part #\n",
    "#     Architope_partition_training_begin = time.time()\n",
    "#     # Initialize running max for Parallel time\n",
    "#     Architope_partitioning_max_time_running = -math.inf # Initialize slowest-time at - infinity to force updating!\n",
    "#     # Initialize N_parameter counter for Architope\n",
    "#     N_params_Architope = 0\n",
    "\n",
    "\n",
    "#     tf.compat.v1.disable_eager_execution()\n",
    "#     # Silly Coercsion for ICML rebuttle deadline timeline\n",
    "#     if Option_Function == 'Motivational_Example':\n",
    "#         Iteration_Length = len(X_parts_list) -1\n",
    "#     else:\n",
    "#         Iteration_Length = len(X_parts_list)\n",
    "\n",
    "\n",
    "#     # Initialize Parameter Counter\n",
    "#     N_params_tally = 0\n",
    "#     # Train each part!\n",
    "#     for current_part in range(Iteration_Length):\n",
    "#         #==============#\n",
    "#         # Timer(begin) #\n",
    "#         #==============#\n",
    "#         current_part_training_time_for_parallel_begin = time.time()\n",
    "\n",
    "\n",
    "#         # Initializations #\n",
    "#         #-----------------#\n",
    "#         # Reload Grid\n",
    "#         exec(open('Grid_Enhanced_Network.py').read())\n",
    "#         # Modify heights according to optimal (data-driven) rule (with threshold)\n",
    "#         current_height = np.ceil(np.array(param_grid_Vanilla_Nets['height'])*N_ratios[current_part])\n",
    "#         current_height_threshold = np.repeat(min_height,(current_height.shape[0]))\n",
    "#         if Fix_Neurons_Q == True:\n",
    "#             current_height = np.maximum(current_height,current_height_threshold)/N_parts_Generated_by_Algo_2\n",
    "#         current_height = current_height.astype(int).tolist()\n",
    "#         param_grid_Vanilla_Nets['height'] = current_height\n",
    "#         # Automatically Fix Input Dimension\n",
    "#         param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]\n",
    "#         param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "        \n",
    "#         # Update Parameter Counter for PC-NNs (tally parameter count for sub-patterns)\n",
    "#         N_params_tally += (current_height[0])*(param_grid_Vanilla_Nets['depth'][0])\n",
    "\n",
    "#         # Update User #\n",
    "#         #-------------#\n",
    "#         print('Status: Current part: ' + str(current_part) + ' out of : '+str(len(X_parts_list)) +' parts.')\n",
    "#         print('Heights to iterate over: '+str(current_height))\n",
    "\n",
    "#         # Generate Prediction(s) on current Part #\n",
    "#         #----------------------------------------#\n",
    "#         # Failsafe (number of data-points)\n",
    "#         CV_folds_failsafe = min(CV_folds,max(1,(X_train.shape[0]-1)))\n",
    "#         # Train Network\n",
    "#         y_hat_train_full_loop, y_hat_test_full_loop, N_params_Architope_loop = build_ffNN(n_folds = CV_folds_failsafe, \n",
    "#                                                                                           n_jobs = n_jobs,\n",
    "#                                                                                           n_iter = n_iter, \n",
    "#                                                                                           param_grid_in = param_grid_Vanilla_Nets, \n",
    "#                                                                                           X_train= X_parts_list[current_part], \n",
    "#                                                                                           y_train=y_parts_list[current_part],\n",
    "#                                                                                           X_test_partial=X_train,\n",
    "#                                                                                           X_test=X_test,\n",
    "#                                                                                           NOCV=True)\n",
    "#         #put shape formats in order\n",
    "#         y_train.shape = (y_train.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "#         y_hat_train_full_loop.shape = (y_hat_train_full_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "#         y_test.shape = (y_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "#         y_hat_test_full_loop.shape = (y_hat_test_full_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "#         # Append predictions to data-frames\n",
    "#         ## If first prediction we initialize data-frames\n",
    "#         if current_part==0:\n",
    "#             # Register quality\n",
    "#             training_quality = np.array(np.abs(y_hat_train_full_loop-y_train))\n",
    "#             training_quality = training_quality.reshape(training_quality.shape[0],1)\n",
    "\n",
    "#             # Save Predictions\n",
    "#             predictions_train = y_hat_train_full_loop\n",
    "#             predictions_train = predictions_train.reshape(predictions_train.shape[0],1)\n",
    "#             predictions_test = y_hat_test_full_loop\n",
    "#             predictions_test = predictions_test.reshape(predictions_test.shape[0],1)\n",
    "\n",
    "\n",
    "#         ## If not first prediction we append to already initialized dataframes\n",
    "#         else:\n",
    "#         # Register Best Scores\n",
    "#             #----------------------#\n",
    "#             # Write Predictions \n",
    "#             # Save Predictions\n",
    "#             y_hat_train_loop = y_hat_train_full_loop.reshape(predictions_train.shape[0],1)\n",
    "#             predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "#             y_hat_test_loop = y_hat_test_full_loop.reshape(predictions_test.shape[0],1)\n",
    "#             predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "\n",
    "#             # Evaluate Errors #\n",
    "#             #-----------------#\n",
    "#             # Training\n",
    "#             prediction_errors = np.abs(y_hat_train_loop-y_train)\n",
    "#             training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "\n",
    "#         #============#\n",
    "#         # Timer(end) #\n",
    "#         #============#\n",
    "#         current_part_training_time_for_parallel = time.time() - current_part_training_time_for_parallel_begin\n",
    "#         Architope_partitioning_max_time_running = max(Architope_partitioning_max_time_running,current_part_training_time_for_parallel)\n",
    "\n",
    "#         #============---===============#\n",
    "\n",
    "#         # N_parameter Counter (Update) #\n",
    "#         #------------===---------------#\n",
    "#         N_params_Architope = N_params_Architope + N_params_Architope_loop\n",
    "\n",
    "#     # Update User\n",
    "#     #-------------#\n",
    "#     print(' ')\n",
    "#     print(' ')\n",
    "#     print(' ')\n",
    "#     print('----------------------------------------------------')\n",
    "#     print('Feature Generation (Learning Phase): Score Generated')\n",
    "#     print('----------------------------------------------------')\n",
    "#     print(' ')\n",
    "#     print(' ')\n",
    "#     print(' ')\n",
    "\n",
    "#     # Time-Elapsed Training on Each Part\n",
    "#     Architope_partition_training = time.time() - Architope_partition_training_begin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #----------------------#\n",
    "#     # Train Deep Zero-Sets #\n",
    "#     #----------------------#\n",
    "#     #### Deep Classifier\n",
    "#     # Prepare Labels/Classes\n",
    "#     # Time-Elapsed Training Deep Classifier\n",
    "#     Architope_deep_classifier_training_begin = time.time()\n",
    "#     # Initialize Classes Labels\n",
    "#     partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "#     partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "#     # Build Classes\n",
    "#     for part_column_i in range(1,(training_quality.shape[1])):\n",
    "#         partition_labels_training = pd.concat([partition_labels_training,\n",
    "#                                                (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "#                                               ],axis=1)\n",
    "#     # Convert to integers\n",
    "#     partition_labels_training = partition_labels_training+0\n",
    "#     #### Re-Load Grid and Redefine Relevant Input/Output dimensions in dictionary.\n",
    "#     # Re-Load Hyper-parameter Grid\n",
    "#     exec(open('Grid_Enhanced_Network.py').read())\n",
    "#     # Re-Load Helper Function(s)\n",
    "#     exec(open('Helper_Functions.py').read())\n",
    "#     param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "#     param_grid_Deep_Classifier['output_dim'] = [partition_labels_training.shape[1]]\n",
    "#     ## Re-adjust heights\n",
    "#     if Fix_Neurons_Q == True:\n",
    "#         param_grid_Deep_Classifier['height'] = [int(max(round(param_grid_Deep_Classifier['height'][0]/N_parts_Generated_by_Algo_2,0),1))]\n",
    "    \n",
    "#     height_hist += param_grid_Deep_Classifier['height'][0]\n",
    "\n",
    "    \n",
    "#     # Update Parameter Counter for PC-NNs (tally parameter count for sub-patterns)\n",
    "#     N_params_tally += (param_grid_Deep_Classifier['height'][0])*(param_grid_Deep_Classifier['depth'][0])\n",
    "    \n",
    "#     #### Train Deep Classifier\n",
    "#     # Train simple deep classifier\n",
    "#     tf.compat.v1.disable_eager_execution()\n",
    "#     predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "#                                                                                                         n_jobs = n_jobs, \n",
    "#                                                                                                         n_iter =n_iter, \n",
    "#                                                                                                         param_grid_in=param_grid_Deep_Classifier, \n",
    "#                                                                                                         X_train = X_train.values, \n",
    "#                                                                                                         y_train = partition_labels_training.values,\n",
    "#                                                                                                         X_test = X_test.values)\n",
    "#     # COMMENT: .values() is used to convert the Pandas Dataframes here, and not in the vanilla ffNNs, since the former is coded in Keras and the latter in tensorflow.  \n",
    "#     # Time-Elapsed Training Deep Classifier\n",
    "#     Architope_deep_classifier_training = time.time() - Architope_deep_classifier_training_begin\n",
    "\n",
    "#     ##### Get Binary Classes (Discontinuous Unit)\n",
    "#     #Maps deep classifier's outputs $\\tilde{C}(x)\\triangleq \\hat{s}(x)$ to deep zero-sets $I_{(.5,1]}\\circ \\sigma_{\\mbox{sigmoid}}(\\tilde{C}(x))$.\n",
    "\n",
    "#     # Training Set\n",
    "#     predicted_classes_train = ((predicted_classes_train>.5)*1).astype(int)\n",
    "#     #### OLD: Architope_prediction_y_train = np.take_along_axis(predictions_train, predicted_classes_train[:,None], axis=1)\n",
    "#     # Testing Set\n",
    "#     predicted_classes_test = ((predicted_classes_test > .5)*1).astype(int)\n",
    "#     #### OLD: Architope_prediction_y_test = np.take_along_axis(predictions_test, predicted_classes_test[:,None], axis=1)\n",
    "#     #### Get PC-NN Prediction(s)\n",
    "#     # Comuptes $\\sum_{n=1}^N \\, \\hat{f}(x)\\cdot I_{K_n}$\n",
    "#     # Train\n",
    "#     Architope_prediction_y_train = (predictions_train*predicted_classes_train).sum(axis=1)\n",
    "#     # Test\n",
    "#     Architope_prediction_y_test = (predictions_test*predicted_classes_test).sum(axis=1)\n",
    "\n",
    "\n",
    "#     # Compute Peformance\n",
    "#     performance_Architope = reporter(y_train_hat_in=Architope_prediction_y_train,\n",
    "#                                     y_test_hat_in=Architope_prediction_y_test,\n",
    "#                                     y_train_in=y_train,\n",
    "#                                     y_test_in=y_test)\n",
    "#     # Write Performance\n",
    "#     performance_Architope.to_latex((results_tables_path+\"Architopes_full_performance.tex\"))\n",
    "\n",
    "#     # Update User\n",
    "#     print(performance_Architope)\n",
    "\n",
    "#     ### Model Complexity/Efficiency Metrics\n",
    "#     # Compute Parameters for composite models #\n",
    "#     #-----------------------------------------#\n",
    "#     N_params_Architope_full = N_params_Architope + N_params_deep_classifier\n",
    "\n",
    "#     # Build AIC-like Metric #\n",
    "#     #-----------------------#\n",
    "#     AIC_like = 2*(N_params_Architope_full - np.log((performance_Architope['test']['MAE'])))\n",
    "#     AIC_like = np.round(AIC_like,3)\n",
    "#     Efficiency = np.log(N_params_Architope_full) *(performance_Architope['test']['MAE'])\n",
    "#     Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "#     # Build Table #\n",
    "#     #-------------#\n",
    "#     Architope_Model_Complexity_full = pd.DataFrame({'L-time': [Architope_partition_training],\n",
    "#                                                   'P-time':[Architope_partitioning_max_time_running],\n",
    "#                                                   'N_params_expt': [N_params_Architope_full],\n",
    "#                                                   'AIC-like': [AIC_like],\n",
    "#                                                   'Eff': [Efficiency]})\n",
    "\n",
    "\n",
    "#     # Write Required Training Time(s)\n",
    "#     Architope_Model_Complexity_full.to_latex((results_tables_path+\"Architope_full_model_complexities.tex\"))\n",
    "\n",
    "#     #--------------======---------------#\n",
    "#     # Display Required Training Time(s) #\n",
    "#     #--------------======---------------#\n",
    "#     print(Architope_Model_Complexity_full)\n",
    "    \n",
    "#     # Compute Mean Subnetwork Widths\n",
    "#     height_mean = height_hist/N_parts_Generated_by_Algo_2\n",
    "    \n",
    "#     # Return Performance Metrics\n",
    "#     return performance_Architope, Architope_Model_Complexity_full, N_parts_Generated_by_Algo_2, N_params_tally, height_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Perform Ablation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Ablation Completion Percentage: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-59eeaee873a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mq_implicit_N_parts_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_implicit_N_parts_possibilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minplicit_N_parts_loop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Run Algos. 1+2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mperformance_Architope_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArchitope_Model_Complexity_full_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_parts_Generated_by_Algo_2_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_params_architope_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_mean_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAblate_PCNNs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_implicit_N_parts_loop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mperformance_Architope_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformance_Architope_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b87700e77c0f>\u001b[0m in \u001b[0;36mAblate_PCNNs\u001b[0;34m(N_parts, data_y, X_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     10\u001b[0m     X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(X_train.to_numpy(),\n\u001b[1;32m     11\u001b[0m                                                                                          \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                                                                          N_parts)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ab5157a1f17a>\u001b[0m in \u001b[0;36mRandom_Lipschitz_Partioner\u001b[0;34m(X_in, y_in, N_parts_to_get)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0my_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_th_part_to_get\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_parts_to_get\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Build random balls #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#--------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Initialize \n",
    "q_implicit_N_parts_possibilities = np.linspace(min_parts_threshold,max_parts_threshold,N_plot_finess)\n",
    "\n",
    "# Get Performance Metric\n",
    "for inplicit_N_parts_loop in range(N_plot_finess):\n",
    "    ### UPDATE USER ###\n",
    "    print('--------------------------------------')\n",
    "    print('--------------------------------------')\n",
    "    print('--------------------------------------')\n",
    "    print('Ablation Completion Percentage:',(inplicit_N_parts_loop/N_plot_finess))\n",
    "    print('--------------------------------------')\n",
    "    print('--------------------------------------')\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    # Implicitly Set: Current Number of Parts\n",
    "    q_implicit_N_parts_loop = q_implicit_N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    # Run Algos. 1+2\n",
    "    performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, height_mean_loop = Ablate_PCNNs(q_implicit_N_parts_loop,data_y,X_train,X_test,y_test)\n",
    "    # Reshape\n",
    "    performance_Architope_loop = performance_Architope_loop.to_numpy().reshape([3,2,1])\n",
    "    Architope_Model_Complexity_full_loop = Architope_Model_Complexity_full_loop.to_numpy().reshape([1,5,1])\n",
    "\n",
    "    # Record\n",
    "    if inplicit_N_parts_loop == 0:\n",
    "        performance_Architope_history = performance_Architope_loop\n",
    "        Architope_Model_Complexity_history = Architope_Model_Complexity_full_loop\n",
    "        N_parts_Generated_by_Algo_2_history = N_parts_Generated_by_Algo_2_loop\n",
    "        N_params_architope_hist = N_params_architope_loop\n",
    "        height_mean_hist = height_mean_loop\n",
    "    else:\n",
    "        performance_Architope_history = np.concatenate((performance_Architope_history,performance_Architope_loop),axis=2)\n",
    "        Architope_Model_Complexity_history = np.concatenate((Architope_Model_Complexity_history,Architope_Model_Complexity_full_loop),axis=2)\n",
    "        N_parts_Generated_by_Algo_2_history = np.append(N_parts_Generated_by_Algo_2_history,N_parts_Generated_by_Algo_2_loop)\n",
    "        N_params_architope_hist = np.append(N_params_architope_hist,N_params_architope_loop)\n",
    "        height_mean_hist = np.append(height_mean_hist,height_mean_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_parts_Generated_by_Algo_2_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-191b36223a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Randomization may produce duplicates; we remove these with the following snippet:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_unique_entries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_parts_Generated_by_Algo_2_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mN_parts_Generated_by_Algo_2_history_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_parts_Generated_by_Algo_2_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_unique_entries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## Prediction Qualities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mperformance_Architope_history_report_MAE_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mperformance_Architope_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_unique_entries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N_parts_Generated_by_Algo_2_history' is not defined"
     ]
    }
   ],
   "source": [
    "# Randomization may produce duplicates; we remove these with the following snippet:\n",
    "get_unique_entries = np.unique(N_parts_Generated_by_Algo_2_history, return_index=True)[1]\n",
    "N_parts_Generated_by_Algo_2_history_report = N_parts_Generated_by_Algo_2_history[get_unique_entries]\n",
    "## Prediction Qualities\n",
    "performance_Architope_history_report_MAE_train = (performance_Architope_history[1,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MAE_test = (performance_Architope_history[1,1,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_train = (performance_Architope_history[0,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_test = (performance_Architope_history[0,1,:])[get_unique_entries]\n",
    "## Model Complexities\n",
    "L_Times = (Architope_Model_Complexity_history[:,0].reshape(-1,))[get_unique_entries]\n",
    "P_Times = (Architope_Model_Complexity_history[:,1].reshape(-1,))[get_unique_entries]\n",
    "N_Params = (N_params_architope_hist.reshape(-1,))[get_unique_entries]\n",
    "height_mean_hist = (height_mean_hist.reshape(-1,))[get_unique_entries]\n",
    "AIC_Like = (Architope_Model_Complexity_history[:,3].reshape(-1,))[get_unique_entries]\n",
    "Eff = (Architope_Model_Complexity_history[:,4].reshape(-1,))[get_unique_entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Plots\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MAE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MAE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MAE_test)\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MAE_test___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Fix_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MSE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MSE_test)\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MSE___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Fix_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"L-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"L-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_Times)\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_L_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Fix_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"P-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"P-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_Times)\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_P_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Fix_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"N. Params\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"N. Params\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_Params)\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_N_Params___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Fix_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Widths for Sub-Pattern Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Mean SubNetwork Widths\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Mean SubNetwork Widths\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         height_mean_hist)\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Mean_Widths___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Fix_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC-Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"AIC-Like\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"AIC-Like\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         AIC_Like)\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_AIC_Like___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Fix_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Eff\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Eff\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         Eff)\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Eff___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Fix_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
