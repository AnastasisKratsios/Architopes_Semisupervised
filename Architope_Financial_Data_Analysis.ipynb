{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Architope (Financial Data)\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"Architopes\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 0.90\n",
    "min_height = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "#================================================#\n",
      " Training Datasize: 539 and test datasize: 60.  \n",
      "#================================================#\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "exec(open('Financial_Data_Preprocessor.py').read())\n",
    "# Import time separately\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use $\\Delta_{in} = Q_{q}\\left(\\Delta(\\mathbb{X})\\right)$ where $\\Delta(\\mathbb{X})$ is the vector of (Euclidean) distances between the given data-points, $q \\in (0,1)$ is a hyper-parameter, and $Q$ is the empirical quantile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(Min_data_size_percentage,q_in, X_train_in,y_train_in, CV_folds_failsafe, min_size):\n",
    "       \n",
    "    #-----------------------#\n",
    "    # Reset Seed Internally #\n",
    "    #-----------------------#\n",
    "    random.seed(2020)\n",
    "    np.random.seed(2020)\n",
    "\n",
    "    #-------------------------------------------#\n",
    "    #-------------------------------------------#\n",
    "    # 1) Sample radius from unifom distribution #\n",
    "    #-------------------------------------------#\n",
    "    #-------------------------------------------#\n",
    "    alpha = np.random.uniform(low=.25,high=.5,size=1)[0]\n",
    "\n",
    "    #-------------------------------------#\n",
    "    #-------------------------------------#\n",
    "    # 2) Apply Random Bijection (Shuffle) #\n",
    "    #-------------------------------------#\n",
    "    #-------------------------------------#\n",
    "    X_train_in_shuffled = X_train_in#.sample(frac=1)\n",
    "    y_train_in_shuffled = y_train_in#.sample(frac=1)\n",
    "\n",
    "    #--------------------#\n",
    "    #--------------------#\n",
    "    # X) Initializations #\n",
    "    #--------------------#\n",
    "    #--------------------#\n",
    "    # Compute-data-driven radius\n",
    "    Delta_X = distance_matrix(X_train_in_shuffled,X_train_in_shuffled)[::,0]\n",
    "    Delta_in = np.quantile(Delta_X,q_in)\n",
    "\n",
    "    # Initialize Random Radius\n",
    "    rand_radius = Delta_in*alpha\n",
    "\n",
    "    # Initialize Data_sizes & ratios\n",
    "    N_tot = X_train_in.shape[0] #<- Total number of data-points in input data-set!\n",
    "    N_radios = np.array([])\n",
    "    N_pool_train_loop = N_tot\n",
    "    # Initialize List of Dataframes\n",
    "    X_internal_train_list = list()\n",
    "    y_internal_train_list = list()\n",
    "\n",
    "    # Initialize Partioned Data-pool\n",
    "    X_internal_train_pool = X_train_in_shuffled\n",
    "    y_internal_train_pool = y_train_in_shuffled\n",
    "\n",
    "    # Initialize counter \n",
    "    part_current_loop = 0\n",
    "\n",
    "    #----------------------------#\n",
    "    #----------------------------#\n",
    "    # 3) Iteratively Build Parts #\n",
    "    #----------------------------#\n",
    "    #----------------------------#\n",
    "\n",
    "    while ((N_pool_train_loop/N_tot > Min_data_size_percentage) or (X_internal_train_pool.empty == False)):\n",
    "        # Extract Current Center\n",
    "        center_loop = X_internal_train_pool.iloc[0]\n",
    "        # Compute Distances\n",
    "        ## Training\n",
    "        distances_pool_loop_train = X_internal_train_pool.sub(center_loop)\n",
    "        distances_pool_loop_train = np.array(np.sqrt(np.square(distances_pool_loop_train).sum(axis=1)))\n",
    "        # Evaluate which Distances are less than the given random radius\n",
    "        Part_train_loop = X_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "        Part_train_loop_y = y_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "\n",
    "        # Remove all data-points which are \"too small\"\n",
    "        if X_internal_train_pool.shape[0] > max(CV_folds,4):\n",
    "            # Append Current part to list\n",
    "            X_internal_train_list.append(Part_train_loop)\n",
    "            y_internal_train_list.append(Part_train_loop_y)\n",
    "\n",
    "        # Remove current part from pool \n",
    "        X_internal_train_pool = X_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "        y_internal_train_pool = y_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "\n",
    "        # Update Current size of pool of training data\n",
    "        N_pool_train_loop = X_internal_train_pool.shape[0]\n",
    "        N_radios = np.append(N_radios,(N_pool_train_loop/N_tot))\n",
    "\n",
    "        # Update Counter\n",
    "        part_current_loop = part_current_loop +1\n",
    "        \n",
    "        # Update User\n",
    "        print((N_pool_train_loop/N_tot))\n",
    "\n",
    "\n",
    "    # Post processing #\n",
    "    #-----------------#\n",
    "    # Remove Empty Partitions\n",
    "    N_radios = N_radios[N_radios>0]\n",
    "    \n",
    "    \n",
    "    #-----------------------------------------------------------------#\n",
    "    # Combine parts which are too small to perform CV without an error\n",
    "    #-----------------------------------------------------------------#\n",
    "    # Initialize lists (partitions) with \"enough\" datums per part\n",
    "    X_internal_train_list_good = list()\n",
    "    y_internal_train_list_good = list()\n",
    "    X_small_parts = list()\n",
    "    y_small_parts = list()\n",
    "    # Initialize first list item test\n",
    "    is_first = True\n",
    "    # Initialize counter\n",
    "    goods_counter = 0\n",
    "    for search_i in range(len(X_internal_train_list)):\n",
    "        number_of_instances_in_part = len(X_internal_train_list[search_i]) \n",
    "        if number_of_instances_in_part < max(CV_folds_failsafe,min_size):\n",
    "            # Check if first \n",
    "            if is_first:\n",
    "                # Initialize set of small X_parts\n",
    "                X_small_parts = X_internal_train_list[search_i]\n",
    "                # Initialize set of small y_parts\n",
    "                y_small_parts = y_internal_train_list[search_i]\n",
    "\n",
    "                # Set is_first to false\n",
    "                is_first = False\n",
    "            else:\n",
    "                X_small_parts = X_small_parts.append(X_internal_train_list[search_i])\n",
    "                y_small_parts = np.append(y_small_parts,y_internal_train_list[search_i])\n",
    "#                 y_small_parts = y_small_parts.append(y_internal_train_list[search_i])\n",
    "        else:\n",
    "            # Append to current list\n",
    "            X_internal_train_list_good.append(X_internal_train_list[search_i])\n",
    "            y_internal_train_list_good.append(y_internal_train_list[search_i])\n",
    "            # Update goods counter \n",
    "            goods_counter = goods_counter +1\n",
    "\n",
    "    # Append final one to good list\n",
    "    X_internal_train_list_good.append(X_small_parts)\n",
    "    y_internal_train_list_good.append(y_small_parts)\n",
    "\n",
    "    # reset is_first to false (inscase we want to re-run this particular block)\n",
    "    is_first = True\n",
    "\n",
    "    # Set good lists to regular lists\n",
    "    X_internal_train_list = X_internal_train_list_good\n",
    "    y_internal_train_list = y_internal_train_list_good\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return Value #\n",
    "    #--------------#\n",
    "    return [X_internal_train_list, y_internal_train_list, N_radios]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Random Partitioner to the given Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "partitioning_time_begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Option_Function == 'SnP':\n",
    "    q_in_auto = .8\n",
    "    Min_data_size_percentage_auto = .1\n",
    "else:\n",
    "    if Option_Function == 'crypto':\n",
    "        q_in_auto = .99\n",
    "        Min_data_size_percentage_auto = .3\n",
    "    else:\n",
    "        q_in_auto = .5\n",
    "        Min_data_size_percentage_auto = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8200371057513914\n",
      "0.8181818181818182\n",
      "0.7606679035250464\n",
      "0.7588126159554731\n",
      "0.7569573283858998\n",
      "0.75139146567718\n",
      "0.7365491651205937\n",
      "0.725417439703154\n",
      "0.7235621521335807\n",
      "0.7198515769944341\n",
      "0.6846011131725418\n",
      "0.6827458256029685\n",
      "0.6808905380333952\n",
      "0.6790352504638218\n",
      "0.6697588126159555\n",
      "0.6623376623376623\n",
      "0.6586270871985158\n",
      "0.647495361781076\n",
      "0.6419294990723562\n",
      "0.6307977736549165\n",
      "0.6196660482374768\n",
      "0.6066790352504638\n",
      "0.5974025974025974\n",
      "0.5862708719851577\n",
      "0.5844155844155844\n",
      "0.5732838589981447\n",
      "0.5695732838589982\n",
      "0.5528756957328386\n",
      "0.5510204081632653\n",
      "0.549165120593692\n",
      "0.5454545454545454\n",
      "0.5435992578849722\n",
      "0.5417439703153989\n",
      "0.5398886827458256\n",
      "0.5380333951762524\n",
      "0.536178107606679\n",
      "0.5269016697588126\n",
      "0.5213358070500927\n",
      "0.5194805194805194\n",
      "0.5176252319109462\n",
      "0.5157699443413729\n",
      "0.5120593692022264\n",
      "0.5064935064935064\n",
      "0.5027829313543599\n",
      "0.49907235621521334\n",
      "0.49536178107606677\n",
      "0.4935064935064935\n",
      "0.4897959183673469\n",
      "0.48794063079777367\n",
      "0.48608534322820035\n",
      "0.48237476808905383\n",
      "0.4805194805194805\n",
      "0.47866419294990725\n",
      "0.47680890538033394\n",
      "0.46938775510204084\n",
      "0.4675324675324675\n",
      "0.46567717996289426\n",
      "0.46382189239332094\n",
      "0.4619666048237477\n",
      "0.4601113172541744\n",
      "0.4582560296846011\n",
      "0.45640074211502785\n",
      "0.45454545454545453\n",
      "0.4489795918367347\n",
      "0.44712430426716143\n",
      "0.4452690166975881\n",
      "0.44341372912801486\n",
      "0.4397031539888683\n",
      "0.437847866419295\n",
      "0.4359925788497217\n",
      "0.43413729128014844\n",
      "0.4322820037105751\n",
      "0.43042671614100186\n",
      "0.42857142857142855\n",
      "0.4267161410018553\n",
      "0.424860853432282\n",
      "0.4230055658627087\n",
      "0.41929499072356213\n",
      "0.4174397031539889\n",
      "0.4100185528756957\n",
      "0.40816326530612246\n",
      "0.40630797773654914\n",
      "0.4044526901669759\n",
      "0.4007421150278293\n",
      "0.39888682745825604\n",
      "0.3970315398886827\n",
      "0.39517625231910947\n",
      "0.39332096474953615\n",
      "0.38961038961038963\n",
      "0.38589981447124305\n",
      "0.38404452690166974\n",
      "0.3821892393320965\n",
      "0.3803339517625232\n",
      "0.3784786641929499\n",
      "0.37662337662337664\n",
      "0.37105751391465674\n",
      "0.3692022263450835\n",
      "0.3673469387755102\n",
      "0.3654916512059369\n",
      "0.36363636363636365\n",
      "0.3562152133580705\n",
      "0.35064935064935066\n",
      "0.34879406307977734\n",
      "0.3432282003710575\n",
      "0.34137291280148424\n",
      "0.3395176252319109\n",
      "0.33766233766233766\n",
      "0.3339517625231911\n",
      "0.32838589981447125\n",
      "0.32653061224489793\n",
      "0.3246753246753247\n",
      "0.3228200371057514\n",
      "0.3209647495361781\n",
      "0.31910946196660483\n",
      "0.3172541743970315\n",
      "0.31539888682745826\n",
      "0.313543599257885\n",
      "0.3116883116883117\n",
      "0.3098330241187384\n",
      "0.3079777365491651\n",
      "0.30612244897959184\n",
      "0.3042671614100185\n",
      "0.300556586270872\n",
      "0.2987012987012987\n",
      "0.29684601113172543\n",
      "0.2949907235621521\n",
      "0.2912801484230056\n",
      "0.287569573283859\n",
      "0.28385899814471244\n",
      "0.2820037105751391\n",
      "0.28014842300556586\n",
      "0.2782931354359926\n",
      "0.2764378478664193\n",
      "0.2727272727272727\n",
      "0.27087198515769945\n",
      "0.2690166975881262\n",
      "0.26716141001855287\n",
      "0.2653061224489796\n",
      "0.2634508348794063\n",
      "0.26159554730983303\n",
      "0.2597402597402597\n",
      "0.25788497217068646\n",
      "0.2560296846011132\n",
      "0.2541743970315399\n",
      "0.2523191094619666\n",
      "0.24860853432282004\n",
      "0.24675324675324675\n",
      "0.24489795918367346\n",
      "0.24304267161410018\n",
      "0.24118738404452691\n",
      "0.23933209647495363\n",
      "0.23747680890538034\n",
      "0.23562152133580705\n",
      "0.23376623376623376\n",
      "0.23191094619666047\n",
      "0.2300556586270872\n",
      "0.22634508348794063\n",
      "0.22263450834879406\n",
      "0.22077922077922077\n",
      "0.2189239332096475\n",
      "0.21706864564007422\n",
      "0.21521335807050093\n",
      "0.21335807050092764\n",
      "0.21150278293135436\n",
      "0.20964749536178107\n",
      "0.2077922077922078\n",
      "0.20593692022263452\n",
      "0.20408163265306123\n",
      "0.20222634508348794\n",
      "0.20037105751391465\n",
      "0.19851576994434136\n",
      "0.19666048237476808\n",
      "0.19480519480519481\n",
      "0.19294990723562153\n",
      "0.19109461966604824\n",
      "0.18923933209647495\n",
      "0.18738404452690166\n",
      "0.18552875695732837\n",
      "0.1836734693877551\n",
      "0.18181818181818182\n",
      "0.17996289424860853\n",
      "0.17810760667903525\n",
      "0.17625231910946196\n",
      "0.17439703153988867\n",
      "0.1725417439703154\n",
      "0.17068645640074212\n",
      "0.16883116883116883\n",
      "0.16697588126159554\n",
      "0.16512059369202226\n",
      "0.16326530612244897\n",
      "0.1614100185528757\n",
      "0.15955473098330242\n",
      "0.15769944341372913\n",
      "0.15584415584415584\n",
      "0.15398886827458255\n",
      "0.15213358070500926\n",
      "0.150278293135436\n",
      "0.14842300556586271\n",
      "0.14656771799628943\n",
      "0.14471243042671614\n",
      "0.14285714285714285\n",
      "0.14100185528756956\n",
      "0.1391465677179963\n",
      "0.137291280148423\n",
      "0.13543599257884972\n",
      "0.13358070500927643\n",
      "0.13172541743970315\n",
      "0.12987012987012986\n",
      "0.1280148423005566\n",
      "0.1261595547309833\n",
      "0.12430426716141002\n",
      "0.12244897959183673\n",
      "0.12059369202226346\n",
      "0.11873840445269017\n",
      "0.11688311688311688\n",
      "0.1150278293135436\n",
      "0.11317254174397032\n",
      "0.11131725417439703\n",
      "0.10946196660482375\n",
      "0.10760667903525047\n",
      "0.10575139146567718\n",
      "0.1038961038961039\n",
      "0.10204081632653061\n",
      "0.10018552875695733\n",
      "0.09833024118738404\n",
      "0.09647495361781076\n",
      "0.09461966604823747\n",
      "0.09276437847866419\n",
      "0.09090909090909091\n",
      "0.08905380333951762\n",
      "0.08719851576994433\n",
      "0.08534322820037106\n",
      "0.08348794063079777\n",
      "0.08163265306122448\n",
      "0.07977736549165121\n",
      "0.07792207792207792\n",
      "0.07606679035250463\n",
      "0.07421150278293136\n",
      "0.07235621521335807\n",
      "0.07050092764378478\n",
      "0.0686456400742115\n",
      "0.06679035250463822\n",
      "0.06493506493506493\n",
      "0.06307977736549165\n",
      "0.061224489795918366\n",
      "0.059369202226345084\n",
      "0.0575139146567718\n",
      "0.055658627087198514\n",
      "0.05380333951762523\n",
      "0.05194805194805195\n",
      "0.05009276437847866\n",
      "0.04823747680890538\n",
      "0.04638218923933209\n",
      "0.04452690166975881\n",
      "0.04267161410018553\n",
      "0.04081632653061224\n",
      "0.03896103896103896\n",
      "0.03710575139146568\n",
      "0.03525046382189239\n",
      "0.03339517625231911\n",
      "0.03153988868274583\n",
      "0.029684601113172542\n",
      "0.027829313543599257\n",
      "0.025974025974025976\n",
      "0.02411873840445269\n",
      "0.022263450834879406\n",
      "0.02040816326530612\n",
      "0.01855287569573284\n",
      "0.016697588126159554\n",
      "0.014842300556586271\n",
      "0.012987012987012988\n",
      "0.011131725417439703\n",
      "0.00927643784786642\n",
      "0.0074211502782931356\n",
      "0.0055658627087198514\n",
      "0.0037105751391465678\n",
      "0.0018552875695732839\n",
      "0.0\n",
      "The_parts_listhe number of parts are: 1.\n",
      "0.6920222634508348\n",
      "0.6901669758812616\n",
      "0.6883116883116883\n",
      "0.686456400742115\n",
      "0.6679035250463822\n",
      "0.5899814471243042\n",
      "0.5825602968460112\n",
      "0.5769944341372912\n",
      "0.5528756957328386\n",
      "0.549165120593692\n",
      "0.5454545454545454\n",
      "0.5435992578849722\n",
      "0.5120593692022264\n",
      "0.49907235621521334\n",
      "0.48794063079777367\n",
      "0.46567717996289426\n",
      "0.46382189239332094\n",
      "0.45083487940630795\n",
      "0.42115027829313545\n",
      "0.4025974025974026\n",
      "0.4007421150278293\n",
      "0.39332096474953615\n",
      "0.3914656771799629\n",
      "0.38961038961038963\n",
      "0.38589981447124305\n",
      "0.38404452690166974\n",
      "0.3803339517625232\n",
      "0.3784786641929499\n",
      "0.37662337662337664\n",
      "0.36363636363636365\n",
      "0.35064935064935066\n",
      "0.3469387755102041\n",
      "0.3450834879406308\n",
      "0.34137291280148424\n",
      "0.3358070500927644\n",
      "0.3339517625231911\n",
      "0.3320964749536178\n",
      "0.32653061224489793\n",
      "0.3246753246753247\n",
      "0.3228200371057514\n",
      "0.3209647495361781\n",
      "0.31910946196660483\n",
      "0.3172541743970315\n",
      "0.31539888682745826\n",
      "0.313543599257885\n",
      "0.3079777365491651\n",
      "0.30612244897959184\n",
      "0.3042671614100185\n",
      "0.30241187384044527\n",
      "0.300556586270872\n",
      "0.2987012987012987\n",
      "0.29684601113172543\n",
      "0.2949907235621521\n",
      "0.29313543599257885\n",
      "0.287569573283859\n",
      "0.28385899814471244\n",
      "0.2820037105751391\n",
      "0.2764378478664193\n",
      "0.274582560296846\n",
      "0.27087198515769945\n",
      "0.26716141001855287\n",
      "0.2653061224489796\n",
      "0.2634508348794063\n",
      "0.26159554730983303\n",
      "0.2597402597402597\n",
      "0.25788497217068646\n",
      "0.2560296846011132\n",
      "0.2541743970315399\n",
      "0.2523191094619666\n",
      "0.2504638218923933\n",
      "0.24489795918367346\n",
      "0.24304267161410018\n",
      "0.24118738404452691\n",
      "0.23933209647495363\n",
      "0.23747680890538034\n",
      "0.23562152133580705\n",
      "0.23376623376623376\n",
      "0.23191094619666047\n",
      "0.2300556586270872\n",
      "0.22820037105751392\n",
      "0.22448979591836735\n",
      "0.22263450834879406\n",
      "0.2189239332096475\n",
      "0.21521335807050093\n",
      "0.21335807050092764\n",
      "0.21150278293135436\n",
      "0.20964749536178107\n",
      "0.2077922077922078\n",
      "0.20593692022263452\n",
      "0.20408163265306123\n",
      "0.20222634508348794\n",
      "0.20037105751391465\n",
      "0.19851576994434136\n",
      "0.19480519480519481\n",
      "0.19294990723562153\n",
      "0.19109461966604824\n",
      "0.18923933209647495\n",
      "0.18738404452690166\n",
      "0.18552875695732837\n",
      "0.1836734693877551\n",
      "0.18181818181818182\n",
      "0.17996289424860853\n",
      "0.17810760667903525\n",
      "0.17625231910946196\n",
      "0.17439703153988867\n",
      "0.1725417439703154\n",
      "0.17068645640074212\n",
      "0.16326530612244897\n",
      "0.1614100185528757\n",
      "0.15955473098330242\n",
      "0.15769944341372913\n",
      "0.15398886827458255\n",
      "0.15213358070500926\n",
      "0.150278293135436\n",
      "0.14842300556586271\n",
      "0.14656771799628943\n",
      "0.14471243042671614\n",
      "0.14285714285714285\n",
      "0.14100185528756956\n",
      "0.1391465677179963\n",
      "0.137291280148423\n",
      "0.13543599257884972\n",
      "0.13358070500927643\n",
      "0.13172541743970315\n",
      "0.12987012987012986\n",
      "0.1280148423005566\n",
      "0.1261595547309833\n",
      "0.12430426716141002\n",
      "0.12244897959183673\n",
      "0.12059369202226346\n",
      "0.11873840445269017\n",
      "0.11688311688311688\n",
      "0.1150278293135436\n",
      "0.11317254174397032\n",
      "0.11131725417439703\n",
      "0.10946196660482375\n",
      "0.10760667903525047\n",
      "0.10575139146567718\n",
      "0.1038961038961039\n",
      "0.10204081632653061\n",
      "0.10018552875695733\n",
      "0.09833024118738404\n",
      "0.09647495361781076\n",
      "0.09461966604823747\n",
      "0.09276437847866419\n",
      "0.09090909090909091\n",
      "0.08905380333951762\n",
      "0.08719851576994433\n",
      "0.08534322820037106\n",
      "0.08348794063079777\n",
      "0.08163265306122448\n",
      "0.07977736549165121\n",
      "0.07792207792207792\n",
      "0.07606679035250463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07421150278293136\n",
      "0.07235621521335807\n",
      "0.07050092764378478\n",
      "0.0686456400742115\n",
      "0.06679035250463822\n",
      "0.06493506493506493\n",
      "0.06307977736549165\n",
      "0.061224489795918366\n",
      "0.059369202226345084\n",
      "0.0575139146567718\n",
      "0.055658627087198514\n",
      "0.05380333951762523\n",
      "0.05194805194805195\n",
      "0.05009276437847866\n",
      "0.04823747680890538\n",
      "0.04638218923933209\n",
      "0.04452690166975881\n",
      "0.04267161410018553\n",
      "0.04081632653061224\n",
      "0.03896103896103896\n",
      "0.03710575139146568\n",
      "0.03525046382189239\n",
      "0.03339517625231911\n",
      "0.03153988868274583\n",
      "0.029684601113172542\n",
      "0.027829313543599257\n",
      "0.025974025974025976\n",
      "0.02411873840445269\n",
      "0.022263450834879406\n",
      "0.02040816326530612\n",
      "0.01855287569573284\n",
      "0.016697588126159554\n",
      "0.014842300556586271\n",
      "0.012987012987012988\n",
      "0.011131725417439703\n",
      "0.00927643784786642\n",
      "0.0074211502782931356\n",
      "0.0055658627087198514\n",
      "0.0037105751391465678\n",
      "0.0018552875695732839\n",
      "0.0\n",
      "The_parts_listhe number of parts are: 2.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Number of Parts currently generated\n",
    "N_parts_generated = 0\n",
    "\n",
    "# Generate Partition (with option to regernerate if only 1 part is randomly produced)\n",
    "while N_parts_generated < 2:\n",
    "    # Generate Parts\n",
    "    X_parts_list, y_parts_list, N_ratios = Random_Lipschitz_Partioner(Min_data_size_percentage=Min_data_size_percentage_auto, \n",
    "                                                                      q_in=q_in_auto, \n",
    "                                                                      X_train_in=X_train, \n",
    "                                                                      y_train_in=data_y, \n",
    "                                                                      CV_folds_failsafe=CV_folds,\n",
    "                                                                      min_size = 100)\n",
    "    \n",
    "    # Update Number of Parts\n",
    "    N_parts_generated = len(X_parts_list)\n",
    "    # Shuffle hyperparameters\n",
    "    Min_data_size_percentage_auto = (Min_data_size_percentage_auto + random.uniform(0,.3)) % 1\n",
    "    q_in_auto = (q_in_auto + random.uniform(0,.3)) % 1\n",
    "    \n",
    "    # Update User\n",
    "    print('The_parts_listhe number of parts are: ' + str(len(X_parts_list))+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioning_time = time.time() - partitioning_time_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_parts_listhe number of parts are: 2.\n"
     ]
    }
   ],
   "source": [
    "print('The_parts_listhe number of parts are: ' + str(len(X_parts_list))+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Training Predictions on each part\n",
    "- Train locally (on each \"naive part\")\n",
    "- Generate predictions for (full) training and testings sets respectively, to be used in training the classifer and for prediction, respectively.  \n",
    "- Generate predictions on all of testing-set (will be selected between later using classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapse (Start) for Training on Each Part\n",
    "Architope_partition_training_begin = time.time()\n",
    "# Initialize running max for Parallel time\n",
    "Architope_partitioning_max_time_running = -math.inf # Initialize slowest-time at - infinity to force updating!\n",
    "# Initialize N_parameter counter for Architope\n",
    "N_params_Architope = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Current part: 0 out of : 2 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    7.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 9.1937 - mse: 139.7442 - mae: 9.1937 - mape: 99.5414\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3139 - mse: 88.3630 - mae: 7.3139 - mape: 105.1338\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 3.3153 - mse: 18.9093 - mae: 3.3153 - mape: 142.3476\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7150 - mse: 12.6905 - mae: 2.7150 - mape: 129.1352\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2800 - mse: 9.2129 - mae: 2.2800 - mape: 96.8482\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6574 - mse: 4.8894 - mae: 1.6574 - mape: 82.6039\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4920 - mse: 3.9936 - mae: 1.4920 - mape: 68.0452\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.2350 - mse: 2.9614 - mae: 1.2350 - mape: 51.7620\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.9133 - mse: 1.7499 - mae: 0.9133 - mape: 40.2753\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.8075 - mse: 1.2785 - mae: 0.8075 - mape: 35.4723\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6539 - mse: 0.8515 - mae: 0.6539 - mape: 24.2496\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7425 - mse: 1.0509 - mae: 0.7425 - mape: 29.5172\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5730 - mse: 0.6186 - mae: 0.5730 - mape: 19.1918\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5330 - mse: 0.5565 - mae: 0.5330 - mape: 22.9030\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5205 - mse: 0.5216 - mae: 0.5205 - mape: 17.1161\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4382 - mse: 0.3832 - mae: 0.4382 - mape: 15.0012\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4691 - mse: 0.4115 - mae: 0.4691 - mape: 21.8424\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3961 - mse: 0.3096 - mae: 0.3961 - mape: 16.0214\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3770 - mse: 0.2778 - mae: 0.3770 - mape: 17.5193\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3873 - mse: 0.2847 - mae: 0.3873 - mape: 15.0298\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3944 - mse: 0.2880 - mae: 0.3944 - mape: 14.1231\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4171 - mse: 0.3332 - mae: 0.4171 - mape: 11.3314\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4195 - mse: 0.3280 - mae: 0.4195 - mape: 13.2885\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4399 - mse: 0.3400 - mae: 0.4399 - mape: 16.8470\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4334 - mse: 0.3396 - mae: 0.4334 - mape: 13.5110\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3221 - mse: 0.1781 - mae: 0.3221 - mape: 12.3008\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3609 - mse: 0.2036 - mae: 0.3609 - mape: 13.7528\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3346 - mse: 0.1900 - mae: 0.3346 - mape: 10.7191\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3518 - mse: 0.1976 - mae: 0.3518 - mape: 11.1688\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3764 - mse: 0.2557 - mae: 0.3764 - mape: 15.6748\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3647 - mse: 0.2619 - mae: 0.3647 - mape: 17.5309\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4190 - mse: 0.3286 - mae: 0.4190 - mape: 13.8251\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3711 - mse: 0.2581 - mae: 0.3711 - mape: 16.0830\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3237 - mse: 0.1692 - mae: 0.3237 - mape: 15.0499\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3607 - mse: 0.2321 - mae: 0.3607 - mape: 20.2876\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2428 - mse: 0.1129 - mae: 0.2428 - mape: 8.2596\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3175 - mse: 0.1637 - mae: 0.3175 - mape: 14.0592\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3795 - mse: 0.2416 - mae: 0.3795 - mape: 13.2625\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3202 - mse: 0.1672 - mae: 0.3202 - mape: 11.2574\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2853 - mse: 0.1395 - mae: 0.2853 - mape: 11.3685\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2511 - mse: 0.1014 - mae: 0.2511 - mape: 10.4678\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2605 - mse: 0.1149 - mae: 0.2605 - mape: 11.3263\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3588 - mse: 0.2359 - mae: 0.3588 - mape: 15.4097\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3231 - mse: 0.1936 - mae: 0.3231 - mape: 13.7132\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3464 - mse: 0.2181 - mae: 0.3464 - mape: 11.0979\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2948 - mse: 0.1616 - mae: 0.2948 - mape: 10.4970\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3135 - mse: 0.1827 - mae: 0.3135 - mape: 11.4377\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2872 - mse: 0.1465 - mae: 0.2872 - mape: 9.7366\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4463 - mse: 0.3750 - mae: 0.4463 - mape: 13.3313\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4553 - mse: 0.3939 - mae: 0.4553 - mape: 12.9299\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4042 - mse: 0.2690 - mae: 0.4042 - mape: 12.4229\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3872 - mse: 0.2726 - mae: 0.3872 - mape: 9.4074\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3125 - mse: 0.1615 - mae: 0.3125 - mape: 12.4002\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2358 - mse: 0.0910 - mae: 0.2358 - mape: 9.2160\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3163 - mse: 0.1754 - mae: 0.3163 - mape: 10.2899\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4069 - mse: 0.3516 - mae: 0.4069 - mape: 10.9739\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3379 - mse: 0.1986 - mae: 0.3379 - mape: 12.1086\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3174 - mse: 0.1828 - mae: 0.3174 - mape: 11.8556\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3744 - mse: 0.2614 - mae: 0.3744 - mape: 12.9644\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3115 - mse: 0.1883 - mae: 0.3115 - mape: 8.8681\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2942 - mse: 0.1559 - mae: 0.2942 - mape: 8.7716\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2974 - mse: 0.1377 - mae: 0.2974 - mape: 8.3542\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3451 - mse: 0.2091 - mae: 0.3451 - mape: 13.8900\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3774 - mse: 0.3124 - mae: 0.3774 - mape: 11.1819\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4119 - mse: 0.3737 - mae: 0.4119 - mape: 12.7172\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3454 - mse: 0.2446 - mae: 0.3454 - mape: 9.5148\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2681 - mse: 0.1327 - mae: 0.2681 - mape: 8.5998\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3075 - mse: 0.1529 - mae: 0.3075 - mape: 13.9241\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2556 - mse: 0.1122 - mae: 0.2556 - mape: 10.9281\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2359 - mse: 0.0982 - mae: 0.2359 - mape: 6.9797\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3097 - mse: 0.1662 - mae: 0.3097 - mape: 12.2858\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3336 - mse: 0.2132 - mae: 0.3336 - mape: 12.6847\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2892 - mse: 0.1659 - mae: 0.2892 - mape: 11.5502\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2909 - mse: 0.1602 - mae: 0.2909 - mape: 12.8459\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2921 - mse: 0.1690 - mae: 0.2921 - mape: 10.2811\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2653 - mse: 0.1181 - mae: 0.2653 - mape: 11.3520\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3528 - mse: 0.2472 - mae: 0.3528 - mape: 8.5789\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3067 - mse: 0.1974 - mae: 0.3067 - mape: 8.8163\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3021 - mse: 0.1710 - mae: 0.3021 - mape: 9.0331\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2856 - mse: 0.1512 - mae: 0.2856 - mape: 3.34 - 0s 2ms/step - loss: 0.2653 - mse: 0.1316 - mae: 0.2653 - mape: 11.7014\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3167 - mse: 0.2037 - mae: 0.3167 - mape: 11.6596\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2778 - mse: 0.1332 - mae: 0.2778 - mape: 12.4496\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2203 - mse: 0.0870 - mae: 0.2203 - mape: 9.9169\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3432 - mse: 0.2177 - mae: 0.3432 - mape: 9.8655\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2817 - mse: 0.1249 - mae: 0.2817 - mape: 11.8001\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.1391 - mae: 0.2649 - mape: 8.4302\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.1085 - mae: 0.2619 - mape: 16.1902\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3018 - mse: 0.1568 - mae: 0.3018 - mape: 13.0706\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2634 - mse: 0.1226 - mae: 0.2634 - mape: 11.6742\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2420 - mse: 0.1021 - mae: 0.2420 - mape: 8.3055\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.1180 - mae: 0.2535 - mape: 10.9373\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2535 - mse: 0.1139 - mae: 0.2535 - mape: 10.3408\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2335 - mse: 0.1053 - mae: 0.2335 - mape: 9.5374\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2450 - mse: 0.0952 - mae: 0.2450 - mape: 9.3664\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2913 - mse: 0.1516 - mae: 0.2913 - mape: 9.9170\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2879 - mse: 0.1507 - mae: 0.2879 - mape: 11.0136\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2738 - mse: 0.1350 - mae: 0.2738 - mape: 8.5197\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2688 - mse: 0.1501 - mae: 0.2688 - mape: 8.5213\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2446 - mse: 0.0962 - mae: 0.2446 - mape: 7.7542\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2043 - mse: 0.0634 - mae: 0.2043 - mape: 8.8111\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2229 - mse: 0.0861 - mae: 0.2229 - mape: 7.5632\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2232 - mse: 0.0836 - mae: 0.2232 - mape: 8.2426\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3410 - mse: 0.2125 - mae: 0.3410 - mape: 8.7474\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3274 - mse: 0.1950 - mae: 0.3274 - mape: 8.8108\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2280 - mse: 0.0915 - mae: 0.2280 - mape: 8.3271\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2695 - mse: 0.1371 - mae: 0.2695 - mape: 9.6822\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2969 - mse: 0.1609 - mae: 0.2969 - mape: 11.1344\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2522 - mse: 0.1085 - mae: 0.2522 - mape: 8.9895\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2647 - mse: 0.1237 - mae: 0.2647 - mape: 8.1841\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3713 - mse: 0.2805 - mae: 0.3713 - mape: 9.3438\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3710 - mse: 0.2447 - mae: 0.3710 - mape: 11.5515\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2696 - mse: 0.1565 - mae: 0.2696 - mape: 9.0141\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2641 - mse: 0.1246 - mae: 0.2641 - mape: 9.5509\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2301 - mse: 0.0937 - mae: 0.2301 - mape: 7.1061\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2369 - mse: 0.0995 - mae: 0.2369 - mape: 8.4530\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1946 - mse: 0.0694 - mae: 0.1946 - mape: 7.9111\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2498 - mse: 0.1009 - mae: 0.2498 - mape: 8.2923\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2380 - mse: 0.1052 - mae: 0.2380 - mape: 6.6908\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2864 - mse: 0.1478 - mae: 0.2864 - mape: 10.0059\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2426 - mse: 0.1009 - mae: 0.2426 - mape: 7.8883\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2265 - mse: 0.0996 - mae: 0.2265 - mape: 8.0222\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3009 - mse: 0.1531 - mae: 0.3009 - mape: 11.9466\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3477 - mse: 0.2373 - mae: 0.3477 - mape: 8.2374\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2528 - mse: 0.1029 - mae: 0.2528 - mape: 6.3792\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2091 - mse: 0.0783 - mae: 0.2091 - mape: 6.0546\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3148 - mse: 0.1872 - mae: 0.3148 - mape: 7.3693\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2775 - mse: 0.1400 - mae: 0.2775 - mape: 10.0167\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2523 - mse: 0.1111 - mae: 0.2523 - mape: 9.6800\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2138 - mse: 0.0842 - mae: 0.2138 - mape: 8.4478\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2062 - mse: 0.0827 - mae: 0.2062 - mape: 7.3330\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3149 - mse: 0.1845 - mae: 0.3149 - mape: 8.1308\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3711 - mse: 0.2914 - mae: 0.3711 - mape: 7.2329\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3343 - mse: 0.2179 - mae: 0.3343 - mape: 9.2451\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3057 - mse: 0.1573 - mae: 0.3057 - mape: 9.8773\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.1086 - mae: 0.2407 - mape: 7.6914\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2325 - mse: 0.0937 - mae: 0.2325 - mape: 6.0678\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2383 - mse: 0.1087 - mae: 0.2383 - mape: 8.9841\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3056 - mse: 0.1715 - mae: 0.3056 - mape: 8.0191\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3587 - mse: 0.2662 - mae: 0.3587 - mape: 9.8056\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3141 - mse: 0.1812 - mae: 0.3141 - mape: 10.5628\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3271 - mse: 0.1855 - mae: 0.3271 - mape: 10.8246\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2988 - mse: 0.1782 - mae: 0.2988 - mape: 10.1578\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3217 - mse: 0.1910 - mae: 0.3217 - mape: 11.4268\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3135 - mse: 0.2079 - mae: 0.3135 - mape: 9.6722\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.0864 - mae: 0.2363 - mape: 9.4287\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2213 - mse: 0.0889 - mae: 0.2213 - mape: 8.7213\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2059 - mse: 0.0744 - mae: 0.2059 - mape: 9.7627\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3463 - mse: 0.2332 - mae: 0.3463 - mape: 8.2476\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.1239 - mae: 0.2667 - mape: 8.6188\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2952 - mse: 0.1771 - mae: 0.2952 - mape: 7.8864\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2988 - mse: 0.1635 - mae: 0.2988 - mape: 11.5826\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3139 - mse: 0.1978 - mae: 0.3139 - mape: 8.7993\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2810 - mse: 0.1464 - mae: 0.2810 - mape: 6.8413\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2546 - mse: 0.1100 - mae: 0.2546 - mape: 6.9966\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2433 - mse: 0.1201 - mae: 0.2433 - mape: 7.7546\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3192 - mse: 0.2367 - mae: 0.3192 - mape: 7.3559\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2892 - mse: 0.1585 - mae: 0.2892 - mape: 7.4303\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2489 - mse: 0.1306 - mae: 0.2489 - mape: 7.8917\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2714 - mse: 0.1362 - mae: 0.2714 - mape: 8.9307\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1827 - mse: 0.0638 - mae: 0.1827 - mape: 5.4199\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2266 - mse: 0.0824 - mae: 0.2266 - mape: 10.3988\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2387 - mse: 0.1039 - mae: 0.2387 - mape: 7.8870\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2797 - mse: 0.1461 - mae: 0.2797 - mape: 7.9748\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1958 - mse: 0.0617 - mae: 0.1958 - mape: 6.0013\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1755 - mse: 0.0551 - mae: 0.1755 - mape: 7.2073\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2078 - mse: 0.0716 - mae: 0.2078 - mape: 7.7452\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1797 - mse: 0.0562 - mae: 0.1797 - mape: 8.1941\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1906 - mse: 0.0609 - mae: 0.1906 - mape: 9.0581\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2168 - mse: 0.0888 - mae: 0.2168 - mape: 7.7985\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2298 - mse: 0.0965 - mae: 0.2298 - mape: 7.9911\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.0922 - mae: 0.2249 - mape: 8.6842\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2945 - mse: 0.1752 - mae: 0.2945 - mape: 9.3761\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2772 - mse: 0.1340 - mae: 0.2772 - mape: 7.6777\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2310 - mse: 0.1136 - mae: 0.2310 - mape: 7.0743\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2657 - mse: 0.1254 - mae: 0.2657 - mape: 8.7777\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2034 - mse: 0.0824 - mae: 0.2034 - mape: 7.4220\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2150 - mse: 0.0864 - mae: 0.2150 - mape: 8.7114\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2151 - mse: 0.0933 - mae: 0.2151 - mape: 6.3181\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3015 - mse: 0.1543 - mae: 0.3015 - mape: 7.9728\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1965 - mse: 0.0748 - mae: 0.1965 - mape: 4.6138\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1657 - mse: 0.0479 - mae: 0.1657 - mape: 6.6653\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1775 - mse: 0.0594 - mae: 0.1775 - mape: 5.3580\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1903 - mse: 0.0659 - mae: 0.1903 - mape: 7.2250\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2087 - mse: 0.0747 - mae: 0.2087 - mape: 8.3717\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1958 - mse: 0.0779 - mae: 0.1958 - mape: 7.4261\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1988 - mse: 0.0756 - mae: 0.1988 - mape: 6.7373\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1813 - mse: 0.0563 - mae: 0.1813 - mape: 6.1368\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2096 - mse: 0.0932 - mae: 0.2096 - mape: 4.9674\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1788 - mse: 0.0549 - mae: 0.1788 - mape: 6.7691\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1487 - mse: 0.0387 - mae: 0.1487 - mape: 4.8223\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1687 - mse: 0.0489 - mae: 0.1687 - mape: 4.9661\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2951 - mse: 0.1620 - mae: 0.2951 - mape: 6.3261\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2565 - mse: 0.1184 - mae: 0.2565 - mape: 6.9464\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2445 - mse: 0.1085 - mae: 0.2445 - mape: 6.2140\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2643 - mse: 0.1316 - mae: 0.2643 - mape: 6.1509\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3082 - mse: 0.1863 - mae: 0.3082 - mape: 7.9646\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2737 - mse: 0.1469 - mae: 0.2737 - mape: 6.5387\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3109 - mse: 0.1775 - mae: 0.3109 - mape: 9.1468\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.2753 - mse: 0.1441 - mae: 0.2753 - mape: 9.2239\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3564 - mse: 0.2347 - mae: 0.3564 - mape: 10.7213\n",
      "34/34 [==============================] - 0s 881us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Status: Current part: 1 out of : 2 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    8.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    8.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 26.5788 - mse: 1952.8370 - mae: 26.5788 - mape: 88.0680\n",
      "Epoch 2/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 7.5818 - mse: 157.1092 - mae: 7.5818 - mape: 94.3312\n",
      "Epoch 3/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 5.1578 - mse: 64.1699 - mae: 5.1578 - mape: 86.0530\n",
      "Epoch 4/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 3.5671 - mse: 28.0256 - mae: 3.5671 - mape: 92.5400\n",
      "Epoch 5/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 3.4629 - mse: 26.8465 - mae: 3.4629 - mape: 83.6535\n",
      "Epoch 6/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 3.1360 - mse: 29.9908 - mae: 3.1360 - mape: 72.1127\n",
      "Epoch 7/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 2.2885 - mse: 11.5347 - mae: 2.2885 - mape: 57.8631\n",
      "Epoch 8/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 2.2767 - mse: 11.5567 - mae: 2.2767 - mape: 55.7707\n",
      "Epoch 9/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.8503 - mse: 8.1819 - mae: 1.8503 - mape: 39.2805\n",
      "Epoch 10/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.8945 - mse: 8.7472 - mae: 1.8945 - mape: 33.6274\n",
      "Epoch 11/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.9393 - mse: 10.5944 - mae: 1.9393 - mape: 25.5947\n",
      "Epoch 12/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.7606 - mse: 7.1671 - mae: 1.7606 - mape: 26.5669\n",
      "Epoch 13/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.9545 - mse: 10.1466 - mae: 1.9545 - mape: 21.3867\n",
      "Epoch 14/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.3769 - mse: 4.4078 - mae: 1.3769 - mape: 14.1743\n",
      "Epoch 15/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.2908 - mse: 4.1111 - mae: 1.2908 - mape: 11.9991\n",
      "Epoch 16/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 4.1395 - mse: 72.2028 - mae: 4.1395 - mape: 24.0225\n",
      "Epoch 17/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2.0902 - mse: 12.7986 - mae: 2.0902 - mape: 23.4790\n",
      "Epoch 18/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 1.3239 - mse: 4.1224 - mae: 1.3239 - mape: 10.8897\n",
      "Epoch 19/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.0956 - mse: 3.0312 - mae: 1.0956 - mape: 15.3743\n",
      "Epoch 20/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.1039 - mse: 3.3401 - mae: 1.1039 - mape: 13.4329\n",
      "Epoch 21/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 4.5411 - mse: 78.5049 - mae: 4.5411 - mape: 25.2358\n",
      "Epoch 22/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.6562 - mse: 8.4085 - mae: 1.6562 - mape: 12.9890\n",
      "Epoch 23/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.0692 - mse: 3.5920 - mae: 1.0692 - mape: 19.6931\n",
      "Epoch 24/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0591 - mse: 2.8657 - mae: 1.0591 - mape: 14.4184\n",
      "Epoch 25/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 2.6586 - mse: 28.7510 - mae: 2.6586 - mape: 27.8488\n",
      "Epoch 26/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.5428 - mse: 8.8943 - mae: 1.5428 - mape: 17.0435\n",
      "Epoch 27/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 2.1557 - mse: 19.4751 - mae: 2.1557 - mape: 14.3460\n",
      "Epoch 28/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.3912 - mse: 6.5650 - mae: 1.3912 - mape: 12.8206\n",
      "Epoch 29/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.3246 - mse: 4.6143 - mae: 1.3246 - mape: 14.3351\n",
      "Epoch 30/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.1559 - mse: 3.3213 - mae: 1.1559 - mape: 13.6897\n",
      "Epoch 31/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8686 - mse: 1.7942 - mae: 0.8686 - mape: 11.0693\n",
      "Epoch 32/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 1.7208 - mse: 15.7913 - mae: 1.7208 - mape: 18.2449\n",
      "Epoch 33/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.1394 - mse: 4.6638 - mae: 1.1394 - mape: 9.2617\n",
      "Epoch 34/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.0471 - mse: 3.1463 - mae: 1.0471 - mape: 14.3136\n",
      "Epoch 35/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8068 - mse: 1.6360 - mae: 0.8068 - mape: 8.4264\n",
      "Epoch 36/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8093 - mse: 2.1705 - mae: 0.8093 - mape: 7.9013\n",
      "Epoch 37/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8963 - mse: 2.2556 - mae: 0.8963 - mape: 12.2260\n",
      "Epoch 38/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 3.7165 - mse: 87.6549 - mae: 3.7165 - mape: 13.9543\n",
      "Epoch 39/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 1.6540 - mse: 7.7887 - mae: 1.6540 - mape: 15.0282\n",
      "Epoch 40/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.7807 - mse: 2.0167 - mae: 0.7807 - mape: 6.9454\n",
      "Epoch 41/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6680 - mse: 1.4095 - mae: 0.6680 - mape: 8.9052\n",
      "Epoch 42/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6947 - mse: 1.4973 - mae: 0.6947 - mape: 10.3111\n",
      "Epoch 43/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7564 - mse: 1.8733 - mae: 0.7564 - mape: 8.9323\n",
      "Epoch 44/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7369 - mse: 1.3627 - mae: 0.7369 - mape: 7.3483\n",
      "Epoch 45/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7320 - mse: 1.3546 - mae: 0.7320 - mape: 5.9777\n",
      "Epoch 46/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7412 - mse: 1.7961 - mae: 0.7412 - mape: 8.5941\n",
      "Epoch 47/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0660 - mse: 4.3963 - mae: 1.0660 - mape: 13.3584\n",
      "Epoch 48/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8679 - mse: 3.0852 - mae: 0.8679 - mape: 12.3329\n",
      "Epoch 49/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8219 - mse: 1.5782 - mae: 0.8219 - mape: 6.3897\n",
      "Epoch 50/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6870 - mse: 1.1249 - mae: 0.6870 - mape: 9.4458\n",
      "Epoch 51/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7779 - mse: 1.6566 - mae: 0.7779 - mape: 6.8228\n",
      "Epoch 52/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.0404 - mse: 3.9762 - mae: 1.0404 - mape: 8.2962\n",
      "Epoch 53/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.1817 - mse: 4.0036 - mae: 1.1817 - mape: 10.2114\n",
      "Epoch 54/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8941 - mse: 2.0418 - mae: 0.8941 - mape: 9.9025\n",
      "Epoch 55/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6705 - mse: 1.1337 - mae: 0.6705 - mape: 5.2491\n",
      "Epoch 56/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7842 - mse: 2.5472 - mae: 0.7842 - mape: 6.7680\n",
      "Epoch 57/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.4354 - mse: 8.4544 - mae: 1.4354 - mape: 8.3124\n",
      "Epoch 58/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7790 - mse: 1.4199 - mae: 0.7790 - mape: 7.8957\n",
      "Epoch 59/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8230 - mse: 2.1364 - mae: 0.8230 - mape: 7.5257\n",
      "Epoch 60/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 1.8515 - mae: 0.8259 - mape: 7.8255\n",
      "Epoch 61/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6895 - mse: 1.2855 - mae: 0.6895 - mape: 6.7183\n",
      "Epoch 62/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 4.7942 - mse: 131.6868 - mae: 4.7942 - mape: 17.0018\n",
      "Epoch 63/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.6298 - mse: 13.4400 - mae: 1.6298 - mape: 14.9663\n",
      "Epoch 64/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7617 - mse: 1.4976 - mae: 0.7617 - mape: 9.2320\n",
      "Epoch 65/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8613 - mse: 2.3171 - mae: 0.8613 - mape: 8.6149\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: 0.9230 - mse: 2.4000 - mae: 0.9230 - mape: 6.0780\n",
      "Epoch 67/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7872 - mse: 2.3527 - mae: 0.7872 - mape: 5.6703\n",
      "Epoch 68/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.0389 - mse: 3.8505 - mae: 1.0389 - mape: 8.8277\n",
      "Epoch 69/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.9427 - mse: 2.4799 - mae: 0.9427 - mape: 10.7530\n",
      "Epoch 70/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8700 - mse: 1.9389 - mae: 0.8700 - mape: 6.3640\n",
      "Epoch 71/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.9138 - mse: 2.9038 - mae: 0.9138 - mape: 7.1202\n",
      "Epoch 72/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6459 - mse: 1.1077 - mae: 0.6459 - mape: 4.7401\n",
      "Epoch 73/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6368 - mse: 1.0437 - mae: 0.6368 - mape: 7.5797\n",
      "Epoch 74/200\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.7642 - mse: 2.1046 - mae: 0.7642 - mape: 9.8868\n",
      "Epoch 75/200\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1.0553 - mse: 3.0240 - mae: 1.0553 - mape: 14.2678\n",
      "Epoch 76/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.2697 - mse: 5.9414 - mae: 1.2697 - mape: 9.1923\n",
      "Epoch 77/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.2994 - mse: 6.3600 - mae: 1.2994 - mape: 8.4807\n",
      "Epoch 78/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7388 - mse: 1.6282 - mae: 0.7388 - mape: 5.4587\n",
      "Epoch 79/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6271 - mse: 1.2100 - mae: 0.6271 - mape: 4.7425\n",
      "Epoch 80/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.2368 - mse: 4.6349 - mae: 1.2368 - mape: 10.9416\n",
      "Epoch 81/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.9432 - mse: 2.2245 - mae: 0.9432 - mape: 4.9209\n",
      "Epoch 82/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7028 - mse: 1.2445 - mae: 0.7028 - mape: 9.6221\n",
      "Epoch 83/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.6752 - mse: 1.4691 - mae: 0.6752 - mape: 10.8889\n",
      "Epoch 84/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.1108 - mse: 3.8739 - mae: 1.1108 - mape: 11.9721\n",
      "Epoch 85/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.4946 - mse: 7.9877 - mae: 1.4946 - mape: 14.1240\n",
      "Epoch 86/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7429 - mse: 2.5111 - mae: 0.7429 - mape: 8.5172\n",
      "Epoch 87/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8709 - mse: 2.2191 - mae: 0.8709 - mape: 9.7490\n",
      "Epoch 88/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6448 - mse: 1.0424 - mae: 0.6448 - mape: 6.8863\n",
      "Epoch 89/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7129 - mse: 1.5782 - mae: 0.7129 - mape: 8.1079\n",
      "Epoch 90/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0053 - mse: 3.5933 - mae: 1.0053 - mape: 7.3625\n",
      "Epoch 91/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.2906 - mse: 6.4937 - mae: 1.2906 - mape: 9.1616\n",
      "Epoch 92/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 5.9087 - mse: 147.7734 - mae: 5.9087 - mape: 32.3285\n",
      "Epoch 93/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.3770 - mse: 6.2240 - mae: 1.3770 - mape: 10.2367\n",
      "Epoch 94/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7346 - mse: 1.6978 - mae: 0.7346 - mape: 7.1642\n",
      "Epoch 95/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6605 - mse: 1.5192 - mae: 0.6605 - mape: 9.0830\n",
      "Epoch 96/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.6926 - mse: 1.3182 - mae: 0.6926 - mape: 7.4274\n",
      "Epoch 97/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.4726 - mse: 0.5947 - mae: 0.4726 - mape: 5.6250\n",
      "Epoch 98/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.9164 - mse: 3.1113 - mae: 0.9164 - mape: 11.0652\n",
      "Epoch 99/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.9638 - mse: 2.9819 - mae: 0.9638 - mape: 6.0608\n",
      "Epoch 100/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7125 - mse: 1.4798 - mae: 0.7125 - mape: 6.3049\n",
      "Epoch 101/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8430 - mse: 2.2270 - mae: 0.8430 - mape: 8.3420\n",
      "Epoch 102/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6358 - mse: 1.1155 - mae: 0.6358 - mape: 4.5911\n",
      "Epoch 103/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6627 - mse: 1.4988 - mae: 0.6627 - mape: 4.4877\n",
      "Epoch 104/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7091 - mse: 1.5950 - mae: 0.7091 - mape: 9.8963\n",
      "Epoch 105/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6542 - mse: 1.4985 - mae: 0.6542 - mape: 5.6125\n",
      "Epoch 106/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.9377 - mse: 3.4309 - mae: 0.9377 - mape: 6.8415\n",
      "Epoch 107/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8210 - mse: 2.3899 - mae: 0.8210 - mape: 5.5175\n",
      "Epoch 108/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7483 - mse: 2.0549 - mae: 0.7483 - mape: 4.3999\n",
      "Epoch 109/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.9358 - mse: 2.7168 - mae: 0.9358 - mape: 9.0307\n",
      "Epoch 110/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6972 - mse: 1.4222 - mae: 0.6972 - mape: 6.2981\n",
      "Epoch 111/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6397 - mse: 1.1825 - mae: 0.6397 - mape: 6.1448\n",
      "Epoch 112/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7810 - mse: 1.8324 - mae: 0.7810 - mape: 7.9975\n",
      "Epoch 113/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.1254 - mse: 9.1413 - mae: 1.1254 - mape: 5.3689\n",
      "Epoch 114/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.6822 - mse: 26.6705 - mae: 1.6822 - mape: 10.5939\n",
      "Epoch 115/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6198 - mse: 1.5328 - mae: 0.6198 - mape: 6.3964\n",
      "Epoch 116/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.0836 - mse: 4.3098 - mae: 1.0836 - mape: 8.9148\n",
      "Epoch 117/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7908 - mse: 1.6802 - mae: 0.7908 - mape: 7.3152\n",
      "Epoch 118/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6243 - mse: 0.9348 - mae: 0.6243 - mape: 5.4081\n",
      "Epoch 119/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6677 - mse: 1.1362 - mae: 0.6677 - mape: 12.7570\n",
      "Epoch 120/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6811 - mse: 1.6666 - mae: 0.6811 - mape: 12.9219\n",
      "Epoch 121/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5286 - mse: 0.6752 - mae: 0.5286 - mape: 7.4604\n",
      "Epoch 122/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5753 - mse: 1.2561 - mae: 0.5753 - mape: 6.0041\n",
      "Epoch 123/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.2284 - mse: 4.5886 - mae: 1.2284 - mape: 8.0422\n",
      "Epoch 124/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.1964 - mse: 5.5887 - mae: 1.1964 - mape: 7.6176\n",
      "Epoch 125/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.3050 - mse: 8.6025 - mae: 1.3050 - mape: 9.2540\n",
      "Epoch 126/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 2.2069 - mse: 16.8579 - mae: 2.2069 - mape: 11.4720\n",
      "Epoch 127/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0018 - mse: 3.6942 - mae: 1.0018 - mape: 7.3834\n",
      "Epoch 128/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4471 - mse: 0.5989 - mae: 0.4471 - mape: 4.3379\n",
      "Epoch 129/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6281 - mse: 1.3011 - mae: 0.6281 - mape: 4.6358\n",
      "Epoch 130/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6007 - mse: 0.9329 - mae: 0.6007 - mape: 4.9106\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5242 - mse: 0.7664 - mae: 0.5242 - mape: 6.6691\n",
      "Epoch 132/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7565 - mse: 2.3700 - mae: 0.7565 - mape: 9.4822\n",
      "Epoch 133/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4416 - mse: 0.4804 - mae: 0.4416 - mape: 7.6347\n",
      "Epoch 134/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.9585 - mse: 31.0195 - mae: 1.9585 - mape: 8.2247\n",
      "Epoch 135/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.7111 - mse: 13.4933 - mae: 1.7111 - mape: 8.2932\n",
      "Epoch 136/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4929 - mse: 0.6425 - mae: 0.4929 - mape: 5.7151\n",
      "Epoch 137/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6527 - mse: 1.0282 - mae: 0.6527 - mape: 5.2180\n",
      "Epoch 138/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4665 - mse: 0.5286 - mae: 0.4665 - mape: 5.7466\n",
      "Epoch 139/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5011 - mse: 0.7902 - mae: 0.5011 - mape: 5.4911\n",
      "Epoch 140/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5966 - mse: 0.9229 - mae: 0.5966 - mape: 5.8778\n",
      "Epoch 141/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7156 - mse: 1.4579 - mae: 0.7156 - mape: 9.9026\n",
      "Epoch 142/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7927 - mse: 3.1617 - mae: 0.7927 - mape: 6.0941\n",
      "Epoch 143/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.1507 - mse: 4.6093 - mae: 1.1507 - mape: 11.3256\n",
      "Epoch 144/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8101 - mse: 2.0536 - mae: 0.8101 - mape: 8.9372\n",
      "Epoch 145/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6703 - mse: 2.0694 - mae: 0.6703 - mape: 8.0306\n",
      "Epoch 146/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5375 - mse: 0.9003 - mae: 0.5375 - mape: 6.2970\n",
      "Epoch 147/200\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.6492 - mse: 1.1407 - mae: 0.6492 - mape: 5.6503\n",
      "Epoch 148/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.0121 - mse: 2.8644 - mae: 1.0121 - mape: 6.9818\n",
      "Epoch 149/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8391 - mse: 1.7264 - mae: 0.8391 - mape: 7.1940\n",
      "Epoch 150/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5628 - mse: 0.9943 - mae: 0.5628 - mape: 7.1571\n",
      "Epoch 151/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4754 - mse: 0.6232 - mae: 0.4754 - mape: 6.6759\n",
      "Epoch 152/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6619 - mse: 1.7840 - mae: 0.6619 - mape: 5.5023\n",
      "Epoch 153/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5641 - mse: 0.8731 - mae: 0.5641 - mape: 4.4709\n",
      "Epoch 154/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9468 - mse: 3.3836 - mae: 0.9468 - mape: 7.6032\n",
      "Epoch 155/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6845 - mse: 1.2428 - mae: 0.6845 - mape: 4.8241\n",
      "Epoch 156/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7732 - mse: 1.6361 - mae: 0.7732 - mape: 7.5102\n",
      "Epoch 157/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6680 - mse: 1.0700 - mae: 0.6680 - mape: 8.5243\n",
      "Epoch 158/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6351 - mse: 1.0913 - mae: 0.6351 - mape: 7.3502\n",
      "Epoch 159/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8121 - mse: 2.2964 - mae: 0.8121 - mape: 5.5912\n",
      "Epoch 160/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9863 - mse: 3.8270 - mae: 0.9863 - mape: 11.1438\n",
      "Epoch 161/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8297 - mse: 1.8987 - mae: 0.8297 - mape: 8.9619\n",
      "Epoch 162/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6321 - mse: 1.0542 - mae: 0.6321 - mape: 5.7955\n",
      "Epoch 163/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5212 - mse: 0.7615 - mae: 0.5212 - mape: 4.3549\n",
      "Epoch 164/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 3.4272 - mse: 60.6824 - mae: 3.4272 - mape: 20.7365\n",
      "Epoch 165/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.9950 - mse: 3.3604 - mae: 0.9950 - mape: 7.8325\n",
      "Epoch 166/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7565 - mse: 2.3001 - mae: 0.7565 - mape: 5.9418\n",
      "Epoch 167/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5471 - mse: 1.0485 - mae: 0.5471 - mape: 5.4328\n",
      "Epoch 168/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.7153 - mse: 10.7291 - mae: 1.7153 - mape: 15.2501\n",
      "Epoch 169/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9294 - mse: 2.6492 - mae: 0.9294 - mape: 9.4575\n",
      "Epoch 170/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7146 - mse: 1.5024 - mae: 0.7146 - mape: 7.0519\n",
      "Epoch 171/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4715 - mse: 0.5946 - mae: 0.4715 - mape: 6.5287\n",
      "Epoch 172/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4531 - mse: 0.5444 - mae: 0.4531 - mape: 4.4579\n",
      "Epoch 173/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.4562 - mse: 7.6824 - mae: 1.4562 - mape: 8.3864\n",
      "Epoch 174/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0956 - mse: 3.6014 - mae: 1.0956 - mape: 8.0813\n",
      "Epoch 175/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6272 - mse: 1.1320 - mae: 0.6272 - mape: 5.8355\n",
      "Epoch 176/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4768 - mse: 0.8083 - mae: 0.4768 - mape: 4.3514\n",
      "Epoch 177/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6393 - mse: 1.4236 - mae: 0.6393 - mape: 4.3238\n",
      "Epoch 178/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4675 - mse: 0.6235 - mae: 0.4675 - mape: 5.6924\n",
      "Epoch 179/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.1037 - mse: 3.5125 - mae: 1.1037 - mape: 8.4660\n",
      "Epoch 180/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8138 - mse: 2.5430 - mae: 0.8138 - mape: 4.7664\n",
      "Epoch 181/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5409 - mse: 0.8114 - mae: 0.5409 - mape: 7.6829\n",
      "Epoch 182/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4629 - mse: 0.7393 - mae: 0.4629 - mape: 6.2869\n",
      "Epoch 183/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5362 - mse: 1.1913 - mae: 0.5362 - mape: 4.4745\n",
      "Epoch 184/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5358 - mse: 0.8242 - mae: 0.5358 - mape: 5.5260\n",
      "Epoch 185/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.5239 - mae: 0.4788 - mape: 7.1256\n",
      "Epoch 186/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4609 - mse: 0.6364 - mae: 0.4609 - mape: 3.5293\n",
      "Epoch 187/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4224 - mse: 0.5630 - mae: 0.4224 - mape: 4.3090\n",
      "Epoch 188/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5787 - mse: 1.0563 - mae: 0.5787 - mape: 4.3938\n",
      "Epoch 189/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4059 - mse: 0.4842 - mae: 0.4059 - mape: 4.5686\n",
      "Epoch 190/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4367 - mse: 0.5629 - mae: 0.4367 - mape: 5.2906\n",
      "Epoch 191/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5277 - mse: 0.8593 - mae: 0.5277 - mape: 6.0139\n",
      "Epoch 192/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5339 - mse: 0.8584 - mae: 0.5339 - mape: 5.3394\n",
      "Epoch 193/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5128 - mse: 0.8682 - mae: 0.5128 - mape: 6.9629\n",
      "Epoch 194/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5834 - mse: 0.8973 - mae: 0.5834 - mape: 7.1430\n",
      "Epoch 195/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5023 - mse: 0.8946 - mae: 0.5023 - mape: 5.0765\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6873 - mse: 1.7514 - mae: 0.6873 - mape: 4.8699\n",
      "Epoch 197/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5220 - mse: 0.9742 - mae: 0.5220 - mape: 8.8627\n",
      "Epoch 198/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6707 - mse: 1.3351 - mae: 0.6707 - mape: 7.9622\n",
      "Epoch 199/200\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5700 - mse: 0.9025 - mae: 0.5700 - mape: 5.7092\n",
      "Epoch 200/200\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.8901 - mse: 2.7686 - mae: 0.8901 - mape: 4.7321\n",
      "34/34 [==============================] - 0s 744us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      " \n",
      " \n",
      " \n",
      "----------------------------------------------------\n",
      "Feature Generation (Learning Phase): Score Generated\n",
      "----------------------------------------------------\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "for current_part in range(len(X_parts_list)):\n",
    "    #==============#\n",
    "    # Timer(begin) #\n",
    "    #==============#\n",
    "    current_part_training_time_for_parallel_begin = time.time()\n",
    "    \n",
    "    \n",
    "    # Initializations #\n",
    "    #-----------------#\n",
    "    # Reload Grid\n",
    "    exec(open('Grid_Enhanced_Network.py').read())\n",
    "    # Modify heights according to optimal (data-driven) rule (with threshold)\n",
    "    current_height = np.ceil(np.array(param_grid_Vanilla_Nets['height'])*N_ratios[current_part])\n",
    "    current_height_threshold = np.repeat(min_height,(current_height.shape[0]))\n",
    "    current_height = np.maximum(current_height,current_height_threshold)\n",
    "    current_height = current_height.astype(int).tolist()\n",
    "    param_grid_Vanilla_Nets['height'] = current_height\n",
    "    # Automatically Fix Input Dimension\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]\n",
    "    param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    \n",
    "    # Update User #\n",
    "    #-------------#\n",
    "    print('Status: Current part: ' + str(current_part) + ' out of : '+str(len(X_parts_list)) +' parts.')\n",
    "    print('Heights to iterate over: '+str(current_height))\n",
    "    \n",
    "    # Generate Prediction(s) on current Part #\n",
    "    #----------------------------------------#\n",
    "    # Failsafe (number of data-points)\n",
    "    CV_folds_failsafe = min(CV_folds,max(1,(X_train.shape[0]-1)))\n",
    "    # Train Network\n",
    "    y_hat_train_full_loop, y_hat_test_full_loop, N_params_Architope_loop = build_ffNN(n_folds = CV_folds_failsafe, \n",
    "                                                                                     n_jobs = n_jobs,\n",
    "                                                                                     n_iter = n_iter, \n",
    "                                                                                     param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                     X_train= X_parts_list[current_part], \n",
    "                                                                                     y_train=y_parts_list[current_part],\n",
    "                                                                                     X_test_partial=X_train,\n",
    "                                                                                     X_test=X_test)\n",
    "    \n",
    "    # Append predictions to data-frames\n",
    "    ## If first prediction we initialize data-frames\n",
    "    if current_part==0:\n",
    "        # Register quality\n",
    "        training_quality = np.array(np.abs(y_hat_train_full_loop-y_train))\n",
    "        training_quality = training_quality.reshape(training_quality.shape[0],1)\n",
    "\n",
    "        # Save Predictions\n",
    "        predictions_train = y_hat_train_full_loop\n",
    "        predictions_train = predictions_train.reshape(predictions_train.shape[0],1)\n",
    "        predictions_test = y_hat_test_full_loop\n",
    "        predictions_test = predictions_test.reshape(predictions_test.shape[0],1)\n",
    "        \n",
    "        \n",
    "    ## If not first prediction we append to already initialized dataframes\n",
    "    else:\n",
    "    # Register Best Scores\n",
    "        #----------------------#\n",
    "        # Write Predictions \n",
    "        # Save Predictions\n",
    "        y_hat_train_loop = y_hat_train_full_loop.reshape(predictions_train.shape[0],1)\n",
    "        predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "        y_hat_test_loop = y_hat_test_full_loop.reshape(predictions_test.shape[0],1)\n",
    "        predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "        \n",
    "        # Evaluate Errors #\n",
    "        #-----------------#\n",
    "        # Training\n",
    "        prediction_errors = np.abs(y_hat_train_loop.reshape(-1,)-y_train)\n",
    "        training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "        \n",
    "    #============#\n",
    "    # Timer(end) #\n",
    "    #============#\n",
    "    current_part_training_time_for_parallel = time.time() - current_part_training_time_for_parallel_begin\n",
    "    Architope_partitioning_max_time_running = max(Architope_partitioning_max_time_running,current_part_training_time_for_parallel)\n",
    "\n",
    "    #============---===============#\n",
    "    # N_parameter Counter (Update) #\n",
    "    #------------===---------------#\n",
    "    N_params_Architope = N_params_Architope + N_params_Architope_loop\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('----------------------------------------------------')\n",
    "print('Feature Generation (Learning Phase): Score Generated')\n",
    "print('----------------------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training on Each Part\n",
    "Architope_partition_training = time.time() - Architope_partition_training_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Classifier\n",
    "Prepare Labels/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training Deep Classifier\n",
    "Architope_deep_classifier_training_begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Classes Labels\n",
    "partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "# Build Classes\n",
    "for part_column_i in range(1,(training_quality.shape[1])):\n",
    "    partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                           (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                          ],axis=1)\n",
    "# Convert to integers\n",
    "partition_labels_training = partition_labels_training+0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-Load Grid and Redefine Relevant Input/Output dimensions in dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Re-Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Re-Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "\n",
    "# Redefine (Dimension-related) Elements of Grid\n",
    "param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "param_grid_Deep_Classifier['output_dim'] = [partition_labels_training.shape[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    2.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 2ms/step - loss: 1.0759 - accuracy: 0.4712\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8033 - accuracy: 0.5473\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6825 - accuracy: 0.6160\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.6698\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5862 - accuracy: 0.7161\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.7310\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5280 - accuracy: 0.7644\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.7792\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.7904\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.8033\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.7978\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4240 - accuracy: 0.8237\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4209 - accuracy: 0.8089\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8293\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3658 - accuracy: 0.8460\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8664\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8720\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.8850\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8831\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.8980\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2560 - accuracy: 0.9035\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.9109\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2279 - accuracy: 0.9239\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2142 - accuracy: 0.9276\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2028 - accuracy: 0.9295\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.9202\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2232 - accuracy: 0.9184\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9314\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1693 - accuracy: 0.9332\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1581 - accuracy: 0.9369\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1490 - accuracy: 0.9406\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9369\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9462\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1470 - accuracy: 0.9314\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1380 - accuracy: 0.9369\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9425\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9425\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.9425\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1073 - accuracy: 0.9481\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1050 - accuracy: 0.9443\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0989 - accuracy: 0.9462\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0967 - accuracy: 0.9481\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 0.9462\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.9462\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9462\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9462\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9462\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9481\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9462\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.9536\n",
      "WARNING:tensorflow:From /scratch/users/kratsioa/.local/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "34/34 [==============================] - 0s 688us/step\n",
      "4/4 [==============================] - 0s 816us/step\n"
     ]
    }
   ],
   "source": [
    "# Train simple deep classifier\n",
    "predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter =n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train, \n",
    "                                                                                                        y_train = partition_labels_training,\n",
    "                                                                                                        X_test = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training Deep Classifier\n",
    "Architope_deep_classifier_training = time.time() - Architope_deep_classifier_training_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Prediction(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "Architope_prediction_y_train = np.take_along_axis(predictions_train, predicted_classes_train[:,None], axis=1)\n",
    "# Testing Set\n",
    "Architope_prediction_y_test = np.take_along_axis(predictions_test, predicted_classes_test[:,None], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         train       test\n",
      "MAE   0.703017   3.631131\n",
      "MSE   1.588457  26.150923\n",
      "MAPE  7.282801  81.668825\n"
     ]
    }
   ],
   "source": [
    "# Compute Peformance\n",
    "performance_Architope = reporter(y_train_hat_in=Architope_prediction_y_train,\n",
    "                                    y_test_hat_in=Architope_prediction_y_test,\n",
    "                                    y_train_in=y_train,\n",
    "                                    y_test_in=y_test)\n",
    "# Write Performance\n",
    "performance_Architope.to_latex((results_tables_path+\"Architopes_full_performance.tex\"))\n",
    "\n",
    "# Update User\n",
    "print(performance_Architope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity/Efficiency Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      L-time     P-time  N_params_expt    AIC-like     Eff\n",
      "0  32.898192  18.757979          56712  113421.421  39.745\n"
     ]
    }
   ],
   "source": [
    "# Compute Parameters for composite models #\n",
    "#-----------------------------------------#\n",
    "N_params_Architope_full = N_params_Architope + N_params_deep_classifier\n",
    "\n",
    "# Build AIC-like Metric #\n",
    "#-----------------------#\n",
    "AIC_like = 2*(N_params_Architope_full - np.log((performance_Architope['test']['MAE'])))\n",
    "AIC_like = np.round(AIC_like,3)\n",
    "Efficiency = np.log(N_params_Architope_full) *(performance_Architope['test']['MAE'])\n",
    "Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "# Build Table #\n",
    "#-------------#\n",
    "Architope_Model_Complexity_full = pd.DataFrame({'L-time': [Architope_partition_training],\n",
    "                                                  'P-time':[Architope_partitioning_max_time_running],\n",
    "                                                  'N_params_expt': [N_params_Architope_full],\n",
    "                                                  'AIC-like': [AIC_like],\n",
    "                                                  'Eff': [Efficiency]})\n",
    "\n",
    "\n",
    "# Write Required Training Time(s)\n",
    "Architope_Model_Complexity_full.to_latex((results_tables_path+\"Architope_full_model_complexities.tex\"))\n",
    "\n",
    "#--------------======---------------#\n",
    "# Display Required Training Time(s) #\n",
    "#--------------======---------------#\n",
    "print(Architope_Model_Complexity_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Benchmark(s)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architope with Logistic-Classifier Partitioning\n",
    "#### Train Logistic Classifier (Benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training linear classifier\n",
    "Architope_logistic_classifier_training_begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'penalty': ['none','l1', 'l2'], 'C': [0.1, 0.5, 1.0, 10, 100, 1000]}\n",
    "lr = LogisticRegression(random_state=2020)\n",
    "cv = RepeatedStratifiedKFold(n_splits=CV_folds, n_repeats=n_iter, random_state=0)\n",
    "classifier = RandomizedSearchCV(lr, parameters, random_state=2020)\n",
    "\n",
    "# Initialize Classes Labels\n",
    "partition_labels_training = np.argmin(training_quality,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1\n",
      " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1\n",
      " 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1\n",
      " 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Update User on shape of learned partition\n",
    "print(partition_labels_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier and generating partition!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score=nan,\n",
       "                   estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                dual=False, fit_intercept=True,\n",
       "                                                intercept_scaling=1,\n",
       "                                                l1_ratio=None, max_iter=100,\n",
       "                                                multi_class='auto', n_jobs=None,\n",
       "                                                penalty='l2', random_state=2020,\n",
       "                                                solver='lbfgs', tol=0.0001,\n",
       "                                                verbose=0, warm_start=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'C': [0.1, 0.5, 1.0, 10, 100, 1000],\n",
       "                                        'penalty': ['none', 'l1', 'l2']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=2020, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print(\"Training classifier and generating partition!\")\n",
    "\n",
    "# Train Logistic Classifier #\n",
    "#---------------------------#\n",
    "# Supress warnings caused by \"ignoring C\" for 'none' penalty and similar obvious warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# Train Classifier\n",
    "classifier.fit(X_train, partition_labels_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Predicted Class(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "predicted_classes_train_logistic_BM = classifier.best_estimator_.predict(X_train)\n",
    "Architope_prediction_y_train_logistic_BM = np.take_along_axis(predictions_train, predicted_classes_train_logistic_BM[:,None], axis=1)\n",
    "\n",
    "# Testing Set\n",
    "predicted_classes_test_logistic_BM = classifier.best_estimator_.predict(X_test)\n",
    "Architope_prediction_y_test_logistic_BM = np.take_along_axis(predictions_test, predicted_classes_test_logistic_BM[:,None], axis=1)\n",
    "\n",
    "# Extract Number of Parameters Logistic Regressor\n",
    "N_params_best_logistic = (classifier.best_estimator_.coef_.shape[0])*(classifier.best_estimator_.coef_.shape[1]) + len(classifier.best_estimator_.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training linear classifier\n",
    "Architope_logistic_classifier_training = time.time() - Architope_logistic_classifier_training_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          train       test\n",
      "MAE    0.711305   3.885554\n",
      "MSE    1.416262  27.732233\n",
      "MAPE  10.640273  84.597842\n"
     ]
    }
   ],
   "source": [
    "# Compute Peformance\n",
    "performance_architope_ffNN_logistic = reporter(y_train_hat_in=Architope_prediction_y_train_logistic_BM,\n",
    "                                    y_test_hat_in=Architope_prediction_y_test_logistic_BM,\n",
    "                                    y_train_in=y_train,\n",
    "                                    y_test_in=y_test)\n",
    "# Write Performance\n",
    "performance_architope_ffNN_logistic.to_latex((results_tables_path+\"Architopes_logistic_performance.tex\"))\n",
    "\n",
    "# Update User\n",
    "print(performance_architope_ffNN_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bagged Feed-Forward Networks (ffNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for Bagging\n",
    "Bagging_ffNN_bagging_time_begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bagging Weights in-sample\n",
    "bagging_coefficients = LinearRegression().fit(predictions_train,y_train)\n",
    "\n",
    "# Predict Bagging Weights out-of-sample\n",
    "bagged_prediction_train = bagging_coefficients.predict(predictions_train)\n",
    "bagged_prediction_test = bagging_coefficients.predict(predictions_test)\n",
    "\n",
    "# Write number of trainable bagging parameters\n",
    "N_bagged_parameters = len(bagging_coefficients.coef_) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for Bagging\n",
    "Bagging_ffNN_bagging_time = time.time() - Bagging_ffNN_bagging_time_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written Bagged Performance\n",
      "           train       test\n",
      "MAE     0.759085   3.092605\n",
      "MSE     1.339658  18.547324\n",
      "MAPE  143.040009  47.131520\n"
     ]
    }
   ],
   "source": [
    "# Compute Peformance\n",
    "performance_bagged_ffNN = reporter(y_train_hat_in=bagged_prediction_train,\n",
    "                                    y_test_hat_in=bagged_prediction_test,\n",
    "                                    y_train_in=y_train,\n",
    "                                    y_test_in=y_test)\n",
    "# Write Performance\n",
    "performance_bagged_ffNN.to_latex((results_tables_path+\"ffNN_Bagged.tex\"))\n",
    "\n",
    "# Update User\n",
    "print(\"Written Bagged Performance\")\n",
    "print(performance_bagged_ffNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Partition: Generated!...Feature Generation Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Partition: Generated!...Feature Generation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla ffNN\n",
    "#### Reload Hyper-parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Re-Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Re-Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Update Dimensions\n",
    "param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for Bagging\n",
    "Vanilla_ffNN_time_beginn = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    8.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 16.6619 - mse: 902.7366 - mae: 16.6619 - mape: 95.9660\n",
      "Epoch 2/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 4.3196 - mse: 38.2548 - mae: 4.3196 - mape: 124.5451\n",
      "Epoch 3/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 2.9448 - mse: 17.1179 - mae: 2.9448 - mape: 95.5345\n",
      "Epoch 4/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 2.4650 - mse: 13.8265 - mae: 2.4650 - mape: 76.2711\n",
      "Epoch 5/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 2.4883 - mse: 16.0393 - mae: 2.4883 - mape: 76.5220\n",
      "Epoch 6/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 2.2012 - mse: 12.3020 - mae: 2.2012 - mape: 66.3522\n",
      "Epoch 7/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.6369 - mse: 6.7081 - mae: 1.6369 - mape: 53.0985\n",
      "Epoch 8/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.7808 - mse: 8.8747 - mae: 1.7808 - mape: 40.7486\n",
      "Epoch 9/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.4992 - mse: 5.8735 - mae: 1.4992 - mape: 37.5540\n",
      "Epoch 10/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.2881 - mse: 4.5141 - mae: 1.2881 - mape: 26.6637\n",
      "Epoch 11/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.2578 - mse: 3.9792 - mae: 1.2578 - mape: 25.1070\n",
      "Epoch 12/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.2372 - mse: 4.3188 - mae: 1.2372 - mape: 26.0452\n",
      "Epoch 13/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.3453 - mse: 4.8301 - mae: 1.3453 - mape: 25.1128\n",
      "Epoch 14/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.5177 - mse: 7.9307 - mae: 1.5177 - mape: 21.6112\n",
      "Epoch 15/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.3043 - mse: 5.0285 - mae: 1.3043 - mape: 23.3110\n",
      "Epoch 16/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.0012 - mse: 3.5198 - mae: 1.0012 - mape: 17.0612\n",
      "Epoch 17/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.3725 - mse: 6.5328 - mae: 1.3725 - mape: 17.2624\n",
      "Epoch 18/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.1803 - mse: 7.1229 - mae: 1.1803 - mape: 13.8650\n",
      "Epoch 19/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.3725 - mse: 7.8854 - mae: 1.3725 - mape: 18.1429\n",
      "Epoch 20/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.0247 - mse: 3.2375 - mae: 1.0247 - mape: 15.1577\n",
      "Epoch 21/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.9282 - mse: 2.5299 - mae: 0.9282 - mape: 12.8014\n",
      "Epoch 22/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7924 - mse: 1.7626 - mae: 0.7924 - mape: 13.5843\n",
      "Epoch 23/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.5276 - mse: 12.3038 - mae: 1.5276 - mape: 19.2900\n",
      "Epoch 24/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.1969 - mse: 5.1647 - mae: 1.1969 - mape: 14.6770\n",
      "Epoch 25/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.8225 - mse: 1.9975 - mae: 0.8225 - mape: 13.9505\n",
      "Epoch 26/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.9727 - mse: 3.4884 - mae: 0.9727 - mape: 14.2331\n",
      "Epoch 27/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7007 - mse: 1.2871 - mae: 0.7007 - mape: 11.1433\n",
      "Epoch 28/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6694 - mse: 1.2868 - mae: 0.6694 - mape: 10.2679\n",
      "Epoch 29/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6385 - mse: 1.1365 - mae: 0.6385 - mape: 11.2643\n",
      "Epoch 30/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7618 - mse: 2.0661 - mae: 0.7618 - mape: 12.0947\n",
      "Epoch 31/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6396 - mse: 1.3179 - mae: 0.6396 - mape: 8.0512\n",
      "Epoch 32/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.8591 - mse: 2.1377 - mae: 0.8591 - mape: 8.4309\n",
      "Epoch 33/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6712 - mse: 1.3037 - mae: 0.6712 - mape: 11.5793\n",
      "Epoch 34/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.7718 - mse: 1.6239 - mae: 0.7718 - mape: 10.5396\n",
      "Epoch 35/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6313 - mse: 1.4383 - mae: 0.6313 - mape: 10.2883\n",
      "Epoch 36/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.8618 - mse: 2.6143 - mae: 0.8618 - mape: 11.9808\n",
      "Epoch 37/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6093 - mse: 1.0483 - mae: 0.6093 - mape: 9.8426\n",
      "Epoch 38/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5888 - mse: 1.0731 - mae: 0.5888 - mape: 8.6843\n",
      "Epoch 39/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.8582 - mse: 2.7534 - mae: 0.8582 - mape: 8.6901\n",
      "Epoch 40/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7524 - mse: 1.6367 - mae: 0.7524 - mape: 11.4573\n",
      "Epoch 41/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6744 - mse: 1.4894 - mae: 0.6744 - mape: 7.6058\n",
      "Epoch 42/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7244 - mse: 1.5238 - mae: 0.7244 - mape: 9.6115\n",
      "Epoch 43/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7371 - mse: 1.9598 - mae: 0.7371 - mape: 7.3804\n",
      "Epoch 44/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.7563 - mse: 2.5528 - mae: 0.7563 - mape: 12.6428\n",
      "Epoch 45/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6474 - mse: 1.4113 - mae: 0.6474 - mape: 9.7108\n",
      "Epoch 46/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6331 - mse: 1.0396 - mae: 0.6331 - mape: 11.5905\n",
      "Epoch 47/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.0662 - mse: 4.4630 - mae: 1.0662 - mape: 11.7430\n",
      "Epoch 48/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7792 - mse: 2.3509 - mae: 0.7792 - mape: 10.4213\n",
      "Epoch 49/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6854 - mse: 1.7037 - mae: 0.6854 - mape: 7.5510\n",
      "Epoch 50/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7485 - mse: 1.9204 - mae: 0.7485 - mape: 9.1115\n",
      "Epoch 51/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.8159 - mse: 2.1896 - mae: 0.8159 - mape: 11.3223\n",
      "Epoch 52/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7725 - mse: 1.5995 - mae: 0.7725 - mape: 10.1523\n",
      "Epoch 53/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6175 - mse: 1.2908 - mae: 0.6175 - mape: 8.3773\n",
      "Epoch 54/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.8853 - mse: 2.7603 - mae: 0.8853 - mape: 7.8975\n",
      "Epoch 55/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7913 - mse: 2.6970 - mae: 0.7913 - mape: 9.2647\n",
      "Epoch 56/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6053 - mse: 1.0872 - mae: 0.6053 - mape: 7.2535\n",
      "Epoch 57/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6604 - mse: 1.2061 - mae: 0.6604 - mape: 8.9234\n",
      "Epoch 58/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6437 - mse: 1.2629 - mae: 0.6437 - mape: 10.3820\n",
      "Epoch 59/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6758 - mse: 1.7046 - mae: 0.6758 - mape: 6.5092\n",
      "Epoch 60/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9859 - mse: 3.1809 - mae: 0.9859 - mape: 12.6774\n",
      "Epoch 61/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8012 - mse: 2.4407 - mae: 0.8012 - mape: 7.7938\n",
      "Epoch 62/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7374 - mse: 1.7479 - mae: 0.7374 - mape: 7.2145\n",
      "Epoch 63/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6928 - mse: 1.9681 - mae: 0.6928 - mape: 6.5773\n",
      "Epoch 64/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8058 - mse: 2.1307 - mae: 0.8058 - mape: 7.2677\n",
      "Epoch 65/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7234 - mse: 1.7579 - mae: 0.7234 - mape: 7.7520\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6709 - mse: 1.2663 - mae: 0.6709 - mape: 7.1128\n",
      "Epoch 67/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.9811 - mse: 3.0402 - mae: 0.9811 - mape: 9.8283\n",
      "Epoch 68/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7194 - mse: 1.3852 - mae: 0.7194 - mape: 7.8421\n",
      "Epoch 69/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6880 - mse: 1.8176 - mae: 0.6880 - mape: 10.1175\n",
      "Epoch 70/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6288 - mse: 1.3551 - mae: 0.6288 - mape: 6.5860\n",
      "Epoch 71/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7192 - mse: 1.8498 - mae: 0.7192 - mape: 8.3804\n",
      "Epoch 72/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5224 - mse: 1.1818 - mae: 0.5224 - mape: 6.6911\n",
      "Epoch 73/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7934 - mse: 2.3638 - mae: 0.7934 - mape: 8.3734\n",
      "Epoch 74/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6151 - mse: 1.2980 - mae: 0.6151 - mape: 6.9455\n",
      "Epoch 75/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5181 - mse: 0.9021 - mae: 0.5181 - mape: 8.0189\n",
      "Epoch 76/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6440 - mse: 1.4546 - mae: 0.6440 - mape: 7.9304\n",
      "Epoch 77/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7154 - mse: 1.5641 - mae: 0.7154 - mape: 7.7318\n",
      "Epoch 78/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4770 - mse: 0.7247 - mae: 0.4770 - mape: 8.3960\n",
      "Epoch 79/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5235 - mse: 0.7286 - mae: 0.5235 - mape: 8.2677\n",
      "Epoch 80/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7032 - mse: 1.5030 - mae: 0.7032 - mape: 7.0725\n",
      "Epoch 81/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7121 - mse: 2.1764 - mae: 0.7121 - mape: 9.2884\n",
      "Epoch 82/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4667 - mse: 0.6891 - mae: 0.4667 - mape: 6.7673\n",
      "Epoch 83/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4702 - mse: 0.6130 - mae: 0.4702 - mape: 5.4357\n",
      "Epoch 84/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4672 - mse: 0.7817 - mae: 0.4672 - mape: 5.6565\n",
      "Epoch 85/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4450 - mse: 0.8985 - mae: 0.4450 - mape: 6.0205\n",
      "Epoch 86/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4591 - mse: 0.7742 - mae: 0.4591 - mape: 5.3313\n",
      "Epoch 87/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6153 - mse: 1.2827 - mae: 0.6153 - mape: 10.4153\n",
      "Epoch 88/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4073 - mse: 0.5183 - mae: 0.4073 - mape: 5.1831\n",
      "Epoch 89/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7489 - mse: 3.1363 - mae: 0.7489 - mape: 14.5862\n",
      "Epoch 90/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4750 - mse: 0.5833 - mae: 0.4750 - mape: 7.1092\n",
      "Epoch 91/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4870 - mse: 0.7875 - mae: 0.4870 - mape: 7.4924\n",
      "Epoch 92/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 1.0497 - mae: 0.5001 - mape: 4.5269\n",
      "Epoch 93/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6474 - mse: 1.8823 - mae: 0.6474 - mape: 6.6635\n",
      "Epoch 94/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4801 - mse: 0.7176 - mae: 0.4801 - mape: 5.2640\n",
      "Epoch 95/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6495 - mse: 1.1260 - mae: 0.6495 - mape: 11.3058\n",
      "Epoch 96/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5803 - mse: 0.9911 - mae: 0.5803 - mape: 8.5346\n",
      "Epoch 97/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5143 - mse: 0.7698 - mae: 0.5143 - mape: 7.3370\n",
      "Epoch 98/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5747 - mse: 1.3701 - mae: 0.5747 - mape: 7.3263\n",
      "Epoch 99/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4530 - mse: 0.9583 - mae: 0.4530 - mape: 5.1149\n",
      "Epoch 100/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6825 - mse: 1.6347 - mae: 0.6825 - mape: 8.1755\n",
      "Epoch 101/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6935 - mse: 1.6812 - mae: 0.6935 - mape: 9.9094\n",
      "Epoch 102/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3982 - mse: 0.4271 - mae: 0.3982 - mape: 5.9076\n",
      "Epoch 103/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.9529 - mae: 0.5107 - mape: 6.5095\n",
      "Epoch 104/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4843 - mse: 0.7039 - mae: 0.4843 - mape: 7.0152\n",
      "Epoch 105/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4851 - mse: 0.7565 - mae: 0.4851 - mape: 6.0052\n",
      "Epoch 106/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5273 - mse: 1.0818 - mae: 0.5273 - mape: 7.8621\n",
      "Epoch 107/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5613 - mse: 1.2151 - mae: 0.5613 - mape: 5.4655\n",
      "Epoch 108/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5611 - mse: 0.9646 - mae: 0.5611 - mape: 7.7142\n",
      "Epoch 109/200\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4313 - mse: 0.4770 - mae: 0.4313 - mape: 7.19 - 0s 2ms/step - loss: 0.4711 - mse: 0.6588 - mae: 0.4711 - mape: 6.5810\n",
      "Epoch 110/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.7062 - mse: 2.5266 - mae: 0.7062 - mape: 7.5840\n",
      "Epoch 111/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6149 - mse: 1.2101 - mae: 0.6149 - mape: 6.3434\n",
      "Epoch 112/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5411 - mse: 0.7665 - mae: 0.5411 - mape: 7.7400\n",
      "Epoch 113/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5381 - mse: 0.9517 - mae: 0.5381 - mape: 7.9690\n",
      "Epoch 114/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4501 - mse: 0.6467 - mae: 0.4501 - mape: 6.8182\n",
      "Epoch 115/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3675 - mse: 0.4401 - mae: 0.3675 - mape: 5.4944\n",
      "Epoch 116/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4697 - mse: 0.6713 - mae: 0.4697 - mape: 5.1861\n",
      "Epoch 117/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6654 - mse: 1.1453 - mae: 0.6654 - mape: 8.1053\n",
      "Epoch 118/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6058 - mse: 1.3003 - mae: 0.6058 - mape: 7.1337\n",
      "Epoch 119/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4505 - mse: 0.6122 - mae: 0.4505 - mape: 6.4551\n",
      "Epoch 120/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4916 - mse: 0.6696 - mae: 0.4916 - mape: 6.1120\n",
      "Epoch 121/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5127 - mse: 0.8989 - mae: 0.5127 - mape: 5.0617\n",
      "Epoch 122/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5606 - mse: 0.9268 - mae: 0.5606 - mape: 6.0562\n",
      "Epoch 123/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3527 - mse: 0.3483 - mae: 0.3527 - mape: 6.6058\n",
      "Epoch 124/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5711 - mse: 0.9687 - mae: 0.5711 - mape: 5.5083\n",
      "Epoch 125/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5174 - mse: 0.9720 - mae: 0.5174 - mape: 5.7568\n",
      "Epoch 126/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4732 - mse: 0.8593 - mae: 0.4732 - mape: 6.2711\n",
      "Epoch 127/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5386 - mse: 1.0317 - mae: 0.5386 - mape: 9.2964\n",
      "Epoch 128/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4055 - mse: 0.4839 - mae: 0.4055 - mape: 7.8055\n",
      "Epoch 129/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4507 - mse: 0.6116 - mae: 0.4507 - mape: 5.3540\n",
      "Epoch 130/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4930 - mse: 0.7260 - mae: 0.4930 - mape: 6.3775\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5592 - mse: 1.0882 - mae: 0.5592 - mape: 5.7931\n",
      "Epoch 132/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5183 - mse: 0.7616 - mae: 0.5183 - mape: 6.7383\n",
      "Epoch 133/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4590 - mse: 0.6282 - mae: 0.4590 - mape: 5.5308\n",
      "Epoch 134/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4999 - mse: 0.8267 - mae: 0.4999 - mape: 6.1082\n",
      "Epoch 135/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7544 - mse: 2.3511 - mae: 0.7544 - mape: 6.6611\n",
      "Epoch 136/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4877 - mse: 1.0408 - mae: 0.4877 - mape: 7.8620\n",
      "Epoch 137/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4068 - mse: 0.4919 - mae: 0.4068 - mape: 6.3422\n",
      "Epoch 138/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3556 - mse: 0.3702 - mae: 0.3556 - mape: 4.3772\n",
      "Epoch 139/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5063 - mse: 0.8775 - mae: 0.5063 - mape: 4.6120\n",
      "Epoch 140/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8735 - mse: 2.5509 - mae: 0.8735 - mape: 7.4071\n",
      "Epoch 141/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7965 - mse: 2.3649 - mae: 0.7965 - mape: 8.0673\n",
      "Epoch 142/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4154 - mse: 0.4919 - mae: 0.4154 - mape: 6.0366\n",
      "Epoch 143/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3435 - mse: 0.4233 - mae: 0.3435 - mape: 5.7242\n",
      "Epoch 144/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5118 - mse: 0.9290 - mae: 0.5118 - mape: 5.4453\n",
      "Epoch 145/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6220 - mse: 2.2060 - mae: 0.6220 - mape: 6.7345\n",
      "Epoch 146/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4165 - mse: 0.5700 - mae: 0.4165 - mape: 5.1590\n",
      "Epoch 147/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5157 - mse: 0.9222 - mae: 0.5157 - mape: 5.9083\n",
      "Epoch 148/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4381 - mse: 0.5938 - mae: 0.4381 - mape: 6.6753\n",
      "Epoch 149/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4361 - mse: 0.6306 - mae: 0.4361 - mape: 5.0511\n",
      "Epoch 150/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3562 - mse: 0.3487 - mae: 0.3562 - mape: 4.4168\n",
      "Epoch 151/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5123 - mse: 1.0747 - mae: 0.5123 - mape: 4.6525\n",
      "Epoch 152/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4475 - mse: 0.6667 - mae: 0.4475 - mape: 8.2436\n",
      "Epoch 153/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.9101 - mae: 0.5164 - mape: 5.7568\n",
      "Epoch 154/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3993 - mse: 0.4909 - mae: 0.3993 - mape: 7.6925\n",
      "Epoch 155/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5098 - mse: 1.1390 - mae: 0.5098 - mape: 6.7650\n",
      "Epoch 156/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5804 - mse: 0.9769 - mae: 0.5804 - mape: 4.9093\n",
      "Epoch 157/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6375 - mse: 1.6778 - mae: 0.6375 - mape: 7.7212\n",
      "Epoch 158/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7380 - mse: 2.1722 - mae: 0.7380 - mape: 6.4633\n",
      "Epoch 159/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4612 - mse: 0.8350 - mae: 0.4612 - mape: 5.0986\n",
      "Epoch 160/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4217 - mse: 0.5022 - mae: 0.4217 - mape: 4.5475\n",
      "Epoch 161/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4883 - mse: 0.6430 - mae: 0.4883 - mape: 8.3202\n",
      "Epoch 162/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5607 - mse: 1.0628 - mae: 0.5607 - mape: 5.6053\n",
      "Epoch 163/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4851 - mse: 0.6777 - mae: 0.4851 - mape: 7.4611\n",
      "Epoch 164/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3899 - mse: 0.4917 - mae: 0.3899 - mape: 5.7666\n",
      "Epoch 165/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4160 - mse: 0.5669 - mae: 0.4160 - mape: 4.3390\n",
      "Epoch 166/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5156 - mse: 0.9710 - mae: 0.5156 - mape: 8.2150\n",
      "Epoch 167/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5414 - mse: 1.1654 - mae: 0.5414 - mape: 7.3879\n",
      "Epoch 168/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5882 - mse: 1.3597 - mae: 0.5882 - mape: 7.4274\n",
      "Epoch 169/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4985 - mse: 1.0058 - mae: 0.4985 - mape: 5.8600\n",
      "Epoch 170/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3459 - mse: 0.4887 - mae: 0.3459 - mape: 5.2854\n",
      "Epoch 171/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4217 - mse: 0.6275 - mae: 0.4217 - mape: 6.2174\n",
      "Epoch 172/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4283 - mse: 0.9179 - mae: 0.4283 - mape: 4.6083\n",
      "Epoch 173/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3289 - mse: 0.2819 - mae: 0.3289 - mape: 4.2951\n",
      "Epoch 174/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4681 - mse: 0.8527 - mae: 0.4681 - mape: 4.6516\n",
      "Epoch 175/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5134 - mse: 0.8059 - mae: 0.5134 - mape: 6.9572\n",
      "Epoch 176/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6710 - mse: 1.6804 - mae: 0.6710 - mape: 4.3854\n",
      "Epoch 177/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8203 - mse: 2.2141 - mae: 0.8203 - mape: 7.9290\n",
      "Epoch 178/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7375 - mse: 1.5539 - mae: 0.7375 - mape: 6.0258\n",
      "Epoch 179/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4910 - mse: 0.7923 - mae: 0.4910 - mape: 5.6451\n",
      "Epoch 180/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4668 - mse: 0.6483 - mae: 0.4668 - mape: 4.7952\n",
      "Epoch 181/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8016 - mse: 2.4885 - mae: 0.8016 - mape: 7.8274\n",
      "Epoch 182/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7101 - mse: 2.4502 - mae: 0.7101 - mape: 8.4985\n",
      "Epoch 183/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5516 - mse: 1.2293 - mae: 0.5516 - mape: 6.4361\n",
      "Epoch 184/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3587 - mse: 0.4144 - mae: 0.3587 - mape: 6.5085\n",
      "Epoch 185/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4606 - mse: 0.6404 - mae: 0.4606 - mape: 6.8245\n",
      "Epoch 186/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4054 - mse: 0.5144 - mae: 0.4054 - mape: 4.3479\n",
      "Epoch 187/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3735 - mse: 0.6712 - mae: 0.3735 - mape: 5.5148\n",
      "Epoch 188/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4623 - mse: 0.8037 - mae: 0.4623 - mape: 4.6369\n",
      "Epoch 189/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4017 - mse: 0.6121 - mae: 0.4017 - mape: 5.0446\n",
      "Epoch 190/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7242 - mse: 2.0467 - mae: 0.7242 - mape: 6.7278\n",
      "Epoch 191/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5850 - mse: 1.0655 - mae: 0.5850 - mape: 5.1389\n",
      "Epoch 192/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4967 - mse: 0.7717 - mae: 0.4967 - mape: 5.3461\n",
      "Epoch 193/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.6123 - mae: 0.4999 - mape: 5.6619\n",
      "Epoch 194/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3805 - mse: 0.5276 - mae: 0.3805 - mape: 7.1034\n",
      "Epoch 195/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4877 - mse: 0.7577 - mae: 0.4877 - mape: 4.7862\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4611 - mse: 0.8388 - mae: 0.4611 - mape: 5.3030\n",
      "Epoch 197/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5570 - mse: 1.7331 - mae: 0.5570 - mape: 5.4423\n",
      "Epoch 198/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5126 - mse: 0.9307 - mae: 0.5126 - mape: 5.3564\n",
      "Epoch 199/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5605 - mse: 1.0170 - mae: 0.5605 - mape: 4.3216\n",
      "Epoch 200/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5496 - mse: 0.8847 - mae: 0.5496 - mape: 7.4250\n",
      "34/34 [==============================] - 0s 660us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#X_train vanilla ffNNs\n",
    "y_hat_train_Vanilla_ffNN, y_hat_test_Vanilla_ffNN, N_params_Vanilla_ffNN = build_ffNN(n_folds = CV_folds_failsafe, \n",
    "                                                                                   n_jobs = n_jobs, \n",
    "                                                                                   n_iter = n_iter, \n",
    "                                                                                   param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                   X_train=X_train, \n",
    "                                                                                   y_train=data_y, \n",
    "                                                                                   X_test_partial=X_train,\n",
    "                                                                                   X_test=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for Bagging\n",
    "Vanilla_ffNN_time = time.time() - Vanilla_ffNN_time_beginn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vanilla ffNNs\n"
     ]
    }
   ],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print(\"Trained vanilla ffNNs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written Bagged Vanilla ffNNs\n",
      "          train       test\n",
      "MAE    0.877264   3.057480\n",
      "MSE    2.589455  15.766277\n",
      "MAPE  10.028579  29.520332\n"
     ]
    }
   ],
   "source": [
    "# Compute Peformance\n",
    "performance_Vanilla_ffNN = reporter(y_train_hat_in=y_hat_train_Vanilla_ffNN,y_test_hat_in=y_hat_test_Vanilla_ffNN,y_train_in=y_train,y_test_in=y_test)\n",
    "# Write Performance\n",
    "performance_Vanilla_ffNN.to_latex((results_tables_path+\"ffNN_Vanilla.tex\"))\n",
    "\n",
    "# Update User #\n",
    "#-------------#\n",
    "print(\"Written Bagged Vanilla ffNNs\")\n",
    "print(performance_Vanilla_ffNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Required Training Time(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-Line #\n",
    "#---------#\n",
    "\n",
    "# Architope (Full) Time Lapse\n",
    "Architope_Full_Time = partitioning_time + Architope_partition_training + Architope_deep_classifier_training\n",
    "# Architope (Logistic) Time Lapse\n",
    "Architope_logistic_Time = partitioning_time + Architope_partition_training + Architope_logistic_classifier_training\n",
    "# Bagged ffNN Training Time\n",
    "Bagged_ffNN_Time = partitioning_time + Architope_partition_training + Bagging_ffNN_bagging_time\n",
    "# Vanilla ffNN\n",
    "Vanilla_ffNN_Time = Vanilla_ffNN_time\n",
    "\n",
    "# Parallel (Only if applicable) #\n",
    "#-------------------------------#\n",
    "\n",
    "# Architope (Full) Time Lapse\n",
    "Architope_Full_Time_parallel = partitioning_time + Architope_partitioning_max_time_running + Architope_deep_classifier_training\n",
    "# Architope (Logistic) Time Lapse\n",
    "Architope_logistic_Time_parallel = partitioning_time + Architope_partitioning_max_time_running + Architope_logistic_classifier_training\n",
    "# Bagged ffNN Training Time\n",
    "Bagged_ffNN_Time_parallel = partitioning_time + Architope_partitioning_max_time_running + Bagging_ffNN_bagging_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Required Training Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preliminary table: Required Training Times\n",
      "   Architope  Architope-logistic Vanilla ffNN  Bagged ffNN\n",
      "0     45.094              42.091       22.285       38.261\n",
      "0     30.954              27.951            -       24.121\n"
     ]
    }
   ],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print('Writing preliminary table: Required Training Times')\n",
    "\n",
    "# Format Required Training Time(s)\n",
    "training_times_In_Line = pd.DataFrame({'Architope': [round(Architope_Full_Time,3)],\n",
    "                                'Architope-logistic': [round(Architope_logistic_Time,3)],\n",
    "                                'Vanilla ffNN': [round(Vanilla_ffNN_Time,3)],\n",
    "                                'Bagged ffNN': [round(Bagged_ffNN_Time,3)]})\n",
    "training_times_Parallel = pd.DataFrame({'Architope': [round(Architope_Full_Time_parallel,3)],\n",
    "                                'Architope-logistic': [round(Architope_logistic_Time_parallel,3)],\n",
    "                                'Vanilla ffNN': ['-'],\n",
    "                                'Bagged ffNN': [round(Bagged_ffNN_Time_parallel,3)]})\n",
    "\n",
    "# Combine Training Times into Single Data-Frame #\n",
    "#-----------------------------------------------#\n",
    "Model_Training_times = training_times_In_Line.append(training_times_Parallel)\n",
    "\n",
    "# Write Required Training Time(s)\n",
    "Model_Training_times.to_latex((results_tables_path+\"Model_Training_Times.tex\"))\n",
    "# Display Required Training Time(s)\n",
    "print(Model_Training_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run: Gradient Boosted Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient-Boosted Random Forest: In-progress...\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees Model - Done CV!\n",
      "Random Forest Regressor uses: 34580 parameters.\n",
      "           train        test\n",
      "MAE    15.041325   19.473950\n",
      "MSE   523.639972  626.541744\n",
      "MAPE  298.530655  528.589826\n",
      "Training of Gradient-Boosted Random Forest: Complete!\n"
     ]
    }
   ],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print('Training Gradient-Boosted Random Forest: In-progress...')\n",
    "# Run from External Script\n",
    "exec(open('Gradient_Boosted_Random_Forest_Regressor.py').read())\n",
    "\n",
    "# Update User #\n",
    "#-------------#\n",
    "print('Training of Gradient-Boosted Random Forest: Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Result(s)\n",
    "#### (Update) Write Required Training Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing Table: Required Training Times\n",
      "                   In-Line (L-Time) Parallel (P-Time)\n",
      "Vanilla ffNN                 22.285                 -\n",
      "Grad.Bstd Rand.F              0.934                 -\n",
      "Bagged ffNN                  38.261            24.121\n",
      "Architope-logistic           42.091            27.951\n",
      "Architope                    45.094            30.954\n"
     ]
    }
   ],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print('Completing Table: Required Training Times')\n",
    "\n",
    "# Format Required Training Time(s)\n",
    "training_times_In_Line = pd.DataFrame({'Vanilla ffNN': [round(Vanilla_ffNN_Time,3)],\n",
    "                                       'Grad.Bstd Rand.F': [round(Gradient_boosted_Random_forest_time,3)],\n",
    "                                       'Bagged ffNN': [round(Bagged_ffNN_Time,3)],\n",
    "                                       'Architope-logistic': [round(Architope_logistic_Time,3)],\n",
    "                                       'Architope': [round(Architope_Full_Time,3)]\n",
    "                                      },index=['In-Line (L-Time)'])\n",
    "\n",
    "training_times_Parallel = pd.DataFrame({'Vanilla ffNN': ['-'],\n",
    "                                        'Grad.Bstd Rand.F': ['-'],\n",
    "                                        'Bagged ffNN': [round(Bagged_ffNN_Time_parallel,3)],\n",
    "                                        'Architope-logistic': [round(Architope_logistic_Time_parallel,3)],\n",
    "                                        'Architope': [round(Architope_Full_Time_parallel,3)]},index=['Parallel (P-Time)'])\n",
    "\n",
    "# Combine Training Times into Single Data-Frame #\n",
    "#-----------------------------------------------#\n",
    "Model_Training_times = training_times_In_Line.append(training_times_Parallel)\n",
    "Model_Training_times = Model_Training_times.transpose()\n",
    "\n",
    "# Write Required Training Time(s)\n",
    "Model_Training_times.to_latex((results_tables_path+\"Model_Training_Times.tex\"))\n",
    "# Display Required Training Time(s)\n",
    "print(Model_Training_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Metric(s)\n",
    "#### Write Predictive Performance Dataframe(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                MAE         MSE        MAPE\n",
      "ffNN       0.877264    2.589455   10.028579\n",
      "GBRF      15.041325  523.639972  298.530655\n",
      "ffNN-bag   0.759085    1.339658  143.040009\n",
      "ffNN-lgt   0.711305    1.416262   10.640273\n",
      "tope       0.703017    1.588457    7.282801\n"
     ]
    }
   ],
   "source": [
    "# Write Training Performance\n",
    "predictive_performance_training = pd.DataFrame({'ffNN': performance_Vanilla_ffNN.train,\n",
    "                                                'GBRF': Gradient_boosted_tree.train,\n",
    "                                                'ffNN-bag': performance_bagged_ffNN.train,\n",
    "                                                'ffNN-lgt': performance_architope_ffNN_logistic.train,\n",
    "                                                'tope': performance_Architope.train})\n",
    "predictive_performance_training = predictive_performance_training.transpose()\n",
    "\n",
    "# Write Testing Performance\n",
    "predictive_performance_test = pd.DataFrame({'ffNN': performance_Vanilla_ffNN.test,\n",
    "                                            'GBRF': Gradient_boosted_tree.test,\n",
    "                                            'ffNN-bag': performance_bagged_ffNN.test,\n",
    "                                            'ffNN-lgt': performance_architope_ffNN_logistic.test,\n",
    "                                            'tope': performance_Architope.test})\n",
    "predictive_performance_test = predictive_performance_test.transpose()\n",
    "\n",
    "# Write into one Dataframe #\n",
    "#--------------------------#\n",
    "predictive_performance_training.to_latex((results_tables_path+\"Models_predictive_performance_training.tex\"))\n",
    "predictive_performance_test.to_latex((results_tables_path+\"Models_predictive_performance_testing.tex\"))\n",
    "\n",
    "# Update User #\n",
    "#-------------#\n",
    "print(predictive_performance_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity/Efficiency Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   In-Line (L-Time) Parallel (P-Time)  N_par    AIC_like  \\\n",
      "Vanilla ffNN                 22.285                 -  27351   54699.765   \n",
      "Grad.Bstd Rand.F              0.934                 -  34580   69154.062   \n",
      "Bagged ffNN                  38.261            24.121  54705  109407.742   \n",
      "Architope-logistic           42.091            27.951  55197  110391.285   \n",
      "Architope                    45.094            30.954  56712  113421.421   \n",
      "\n",
      "                        Eff  \n",
      "Vanilla ffNN         31.237  \n",
      "Grad.Bstd Rand.F    203.523  \n",
      "Bagged ffNN          33.739  \n",
      "Architope-logistic   42.425  \n",
      "Architope            39.745  \n"
     ]
    }
   ],
   "source": [
    "# Compute Parameters for composite models #\n",
    "#-----------------------------------------#\n",
    "N_params_Architope_full = N_params_Architope + N_params_deep_classifier\n",
    "N_params_Architope_logistic = N_params_Architope + N_params_best_logistic\n",
    "N_params_bagged_ffNN = N_params_Architope + N_bagged_parameters\n",
    "\n",
    "# Build Table #\n",
    "#-------------#\n",
    "Number_of_model_parameters = pd.DataFrame({'Vanilla ffNN': [N_params_Vanilla_ffNN],\n",
    "                                           'Grad.Bstd Rand.F': [N_tot_params_in_forest],\n",
    "                                           'Bagged ffNN': [N_params_bagged_ffNN],\n",
    "                                           'Architope-logistic': [N_params_Architope_logistic],\n",
    "                                           'Architope': [N_params_Architope_full]},\n",
    "                                            index=['N_par'])\n",
    "\n",
    "Number_of_model_parameters = Number_of_model_parameters.transpose()\n",
    "\n",
    "# Append to Dataframe #\n",
    "#---------------------#\n",
    "Model_Complexity_Metrics = Model_Training_times\n",
    "Model_Complexity_Metrics['N_par']=Number_of_model_parameters.values\n",
    "\n",
    "# Build AIC-like Metric #\n",
    "#-----------------------#\n",
    "AIC_like = 2*((Model_Complexity_Metrics.N_par.values) - np.log(predictive_performance_test.MAE.values))\n",
    "AIC_like = np.round(AIC_like,3)\n",
    "Efficiency = np.log(Model_Complexity_Metrics.N_par.values) *predictive_performance_test.MAE.values\n",
    "Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "# Update Training Metrics Dataframe #\n",
    "#-----------------------------------#\n",
    "Model_Complexity_Metrics['AIC_like'] = AIC_like\n",
    "Model_Complexity_Metrics['Eff'] = Efficiency\n",
    "\n",
    "# Write Required Training Time(s)\n",
    "Model_Complexity_Metrics.to_latex((results_tables_path+\"Model_Complexity_Metrics.tex\"))\n",
    "\n",
    "#--------------======---------------#\n",
    "# Display Required Training Time(s) #\n",
    "#--------------======---------------#\n",
    "print(Model_Complexity_Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "#-------------------#\n",
      " PERFORMANCE SUMMARY:\n",
      "#-------------------#\n",
      " \n",
      " \n",
      "#===================#\n",
      " Individual Metrics: \n",
      "#======-============#\n",
      " \n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Architope (Full)\n",
      "----------------------------------------\n",
      "         train       test\n",
      "MAE   0.703017   3.631131\n",
      "MSE   1.588457  26.150923\n",
      "MAPE  7.282801  81.668825\n",
      "----------------------------------------\n",
      "Architope - Naive Logistic\n",
      "----------------------------------------\n",
      "          train       test\n",
      "MAE    0.711305   3.885554\n",
      "MSE    1.416262  27.732233\n",
      "MAPE  10.640273  84.597842\n",
      "----------------------------------------\n",
      "Vanilla ffNN\n",
      "----------------------------------------\n",
      "          train       test\n",
      "MAE    0.877264   3.057480\n",
      "MSE    2.589455  15.766277\n",
      "MAPE  10.028579  29.520332\n",
      "----------------------------------------\n",
      "Bagged ffNN\n",
      "----------------------------------------\n",
      "           train       test\n",
      "MAE     0.759085   3.092605\n",
      "MSE     1.339658  18.547324\n",
      "MAPE  143.040009  47.131520\n",
      "----------------------------------------\n",
      "Gradient Boosted Random Forest Regressor\n",
      "----------------------------------------\n",
      "           train        test\n",
      "MAE    15.041325   19.473950\n",
      "MSE   523.639972  626.541744\n",
      "MAPE  298.530655  528.589826\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      " \n",
      " \n",
      "#==================#\n",
      " Overview  Metrics : \n",
      "#==================#\n",
      " \n",
      "----------------------------------------\n",
      "Training Performance: \n",
      "----------------------------------------\n",
      "                MAE         MSE        MAPE\n",
      "ffNN       0.877264    2.589455   10.028579\n",
      "GBRF      15.041325  523.639972  298.530655\n",
      "ffNN-bag   0.759085    1.339658  143.040009\n",
      "ffNN-lgt   0.711305    1.416262   10.640273\n",
      "tope       0.703017    1.588457    7.282801\n",
      "----------------------------------------\n",
      "Testing Performance: \n",
      "----------------------------------------\n",
      "                MAE         MSE        MAPE\n",
      "ffNN       3.057480   15.766277   29.520332\n",
      "GBRF      19.473950  626.541744  528.589826\n",
      "ffNN-bag   3.092605   18.547324   47.131520\n",
      "ffNN-lgt   3.885554   27.732233   84.597842\n",
      "tope       3.631131   26.150923   81.668825\n",
      "----------------------------------------\n",
      " \n",
      " \n",
      "#====================#\n",
      " Efficiency Metrics: \n",
      "#====================#\n",
      " \n",
      "Model Training Times:\n",
      "----------------------------------------\n",
      "                   In-Line (L-Time) Parallel (P-Time)  N_par    AIC_like  \\\n",
      "Vanilla ffNN                 22.285                 -  27351   54699.765   \n",
      "Grad.Bstd Rand.F              0.934                 -  34580   69154.062   \n",
      "Bagged ffNN                  38.261            24.121  54705  109407.742   \n",
      "Architope-logistic           42.091            27.951  55197  110391.285   \n",
      "Architope                    45.094            30.954  56712  113421.421   \n",
      "\n",
      "                        Eff  \n",
      "Vanilla ffNN         31.237  \n",
      "Grad.Bstd Rand.F    203.523  \n",
      "Bagged ffNN          33.739  \n",
      "Architope-logistic   42.425  \n",
      "Architope            39.745  \n",
      " \n",
      " \n",
      "😃😃 Have a great day!! 😃😃 \n"
     ]
    }
   ],
   "source": [
    "print(' ')\n",
    "print(' ')\n",
    "print('#-------------------#')\n",
    "print(' PERFORMANCE SUMMARY:')\n",
    "print('#-------------------#')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('#===================#')\n",
    "print(' Individual Metrics: ')\n",
    "print('#======-============#')\n",
    "print(' ')\n",
    "print('----------------------------------------')\n",
    "print('----------------------------------------')\n",
    "print('Architope (Full)')\n",
    "print('----------------------------------------')\n",
    "print(performance_Architope)\n",
    "print('----------------------------------------')\n",
    "print('Architope - Naive Logistic')\n",
    "print('----------------------------------------')\n",
    "print(performance_architope_ffNN_logistic)\n",
    "print('----------------------------------------')\n",
    "print('Vanilla ffNN')\n",
    "print('----------------------------------------')\n",
    "print(performance_Vanilla_ffNN)\n",
    "print('----------------------------------------')\n",
    "print('Bagged ffNN')\n",
    "print('----------------------------------------')\n",
    "print(performance_bagged_ffNN)\n",
    "print('----------------------------------------')\n",
    "print('Gradient Boosted Random Forest Regressor')\n",
    "print('----------------------------------------')\n",
    "print(Gradient_boosted_tree)\n",
    "print('----------------------------------------')\n",
    "print('----------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "#\n",
    "print('#==================#')\n",
    "print(' Overview  Metrics : ')\n",
    "print('#==================#')\n",
    "print(' ')\n",
    "print('----------------------------------------')\n",
    "print('Training Performance: ')\n",
    "print('----------------------------------------')\n",
    "print(predictive_performance_training)\n",
    "print('----------------------------------------')\n",
    "print('Testing Performance: ')\n",
    "print('----------------------------------------')\n",
    "print(predictive_performance_test)\n",
    "print('----------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "#\n",
    "print('#====================#')\n",
    "print(' Efficiency Metrics: ')\n",
    "print('#====================#')\n",
    "print(' ')\n",
    "print('Model Training Times:')\n",
    "print('----------------------------------------')\n",
    "print(Model_Complexity_Metrics)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('😃😃 Have a great day!! 😃😃 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
