{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation: Semi-Supervised Architope - for Reviews\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"PC-NNs\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 1-(1/24)\n",
    "min_width = 5\n",
    "min_epochs = 15\n",
    "# Ablation Finess\n",
    "N_plot_finess = 2\n",
    "# min_parts_threshold = .001; max_parts_threshold = 0.9\n",
    "N_min_parts = 1; N_max_plots = 2\n",
    "Tied_Neurons_Q = True\n",
    "randomize_subpattern_construction = True\n",
    "# Partition with Inputs (determine parts with domain) or outputs (determine parts with image)\n",
    "Partition_using_Inputs = True\n",
    "# Cuttoff Level\n",
    "gamma = .5\n",
    "# Softmax Layer instead of sigmoid\n",
    "softmax_layer = True #<- Just out of curiosity...but it doesn't perform many better IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------#\n",
    "# Only For Motivational Example Only #\n",
    "#------------------------------------#\n",
    "## Hyperparameters\n",
    "percentage_in_row = .25\n",
    "N = 10**4\n",
    "\n",
    "def f_1(x):\n",
    "    return 1 + np.sin(10*x)\n",
    "def f_2(x):\n",
    "    return -2 -np.exp(x)\n",
    "x_0 = 0\n",
    "x_end = 1\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "1\n",
      "Training Data size:  416\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "if Option_Function != \"Motivational_Example\": \n",
    "    exec(open('Financial_Data_Preprocessor.py').read())\n",
    "else:\n",
    "    print(1)\n",
    "    exec(open('Motivational_Example.py').read())\n",
    "    print(\"Training Data size: \",X_train.shape[0])\n",
    "# exec(open('Prepare_Data_California_Housing.py').read())\n",
    "# Import time separately\n",
    "import time\n",
    "\n",
    "# TEMP\n",
    "# import pickle_compat\n",
    "# pickle_compat.patch()\n",
    "# param_grid_Vanilla_Nets['input_dim']=X_train.shape[1]\n",
    "sns.set()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit Partion Builder:\n",
    "Implements exactly Algorithm 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(X_in,\n",
    "                               y_in,\n",
    "                               N_parts_to_get=4):\n",
    "\n",
    "    # Compute Size of each part\n",
    "    size_part_reference = int(round(X_in.shape[0]/N_parts_to_get))\n",
    "\n",
    "    # Apply random bijection #\n",
    "    #------------------------#\n",
    "    ## Get random bijection indices\n",
    "    random_bijection_indices = np.random.choice(range(X_in.shape[0]),size=X_in.shape[0], replace=False)\n",
    "    ## Apply random bijections\n",
    "    X_in_shuffled = X_in[random_bijection_indices,:]\n",
    "    y_in_shuffled = y_in[random_bijection_indices,:]\n",
    "\n",
    "    # Initialize Lists #\n",
    "    #------------------#\n",
    "    X_parts = []\n",
    "    y_parts = []\n",
    "\n",
    "    for i_th_part_to_get in range(N_parts_to_get):\n",
    "        # Build random balls #\n",
    "        #--------------------#\n",
    "        ## Sample random radius\n",
    "        size_part = int(np.maximum(1,np.round(size_part_reference*np.random.uniform(low=.5,high=1.5,size=1)[0])))\n",
    "        ## Sample random point\n",
    "        X_center_loop_index = np.random.choice(range(X_in_shuffled.shape[0]),size=1, replace=False)\n",
    "        X_center_loop = X_in_shuffled[X_center_loop_index,:]\n",
    "        ## Compute Typical Distances from Center\n",
    "        distances_loop = X_center_loop-X_in_shuffled\n",
    "        distances_loop = np.linalg.norm(distances_loop, axis=1)\n",
    "\n",
    "        # Remove Random Ball from Dataset\n",
    "        if size_part <= len(distances_loop):\n",
    "            ## Identify indices\n",
    "            indices_smallest_to_random_ball = np.argsort(distances_loop)[:size_part]\n",
    "        else:\n",
    "            print('Final Loop')\n",
    "            indices_smallest_to_random_ball = np.array(range(X_in_shuffled.shape[0]))\n",
    "        ## Extract Parts\n",
    "        X_current_part_loop = X_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        y_current_part_loop = y_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        ## Append to List of Parts\n",
    "        X_parts.append(X_current_part_loop)\n",
    "        y_parts.append(y_current_part_loop)\n",
    "\n",
    "        # Remove Selected Entries From Array #\n",
    "        #------------------------------------#\n",
    "        X_in_shuffled = np.delete(X_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "        y_in_shuffled = np.delete(y_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "\n",
    "        # Failsafe if procedure has terminated\n",
    "        if X_in_shuffled.shape[0] == 0:\n",
    "            print('breaking early')\n",
    "            break\n",
    "    # Count Number of Parts Generated        \n",
    "    N_parts_generated = len(X_parts)\n",
    "    # Output Parts\n",
    "    return X_parts, y_parts, N_parts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCNNs(N_parts,X_train,y_train,X_test,y_test):\n",
    "\n",
    "    # Initialization(s) #\n",
    "    #-------------------#\n",
    "    N_neurons = 0\n",
    "    L_timer = 0\n",
    "    P_timer = 0\n",
    "    Mean_Width_Subnetworks = 0\n",
    "\n",
    "    # Partitioner Begin #\n",
    "    #-------------------#\n",
    "    import time\n",
    "    partitioning_time_begin = time.time()\n",
    "    print('-------------------------------------------------------')\n",
    "    print('Randomly Initialized Parts - Via Randomized Algorithm 2')\n",
    "    print('-------------------------------------------------------')\n",
    "    if Partition_using_Inputs == True:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(X_train.to_numpy(),\n",
    "                                                                                             y_train.reshape(-1,1),\n",
    "                                                                                             N_parts)\n",
    "    else:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(y_train.reshape(-1,1),\n",
    "                                                                                             X_train.to_numpy(),\n",
    "                                                                                             N_parts)\n",
    "    partitioning_time = time.time() - partitioning_time_begin\n",
    "    print('The_parts_listhe number of parts are: ' + str(N_parts_Generated_by_Algo_2)+'.')\n",
    "    ############# Partitioner End ########\n",
    "\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Training Sub-Networks on Each Randomly Generated Part')\n",
    "    print('-----------------------------------------------------')\n",
    "    # Time-Elapse (Start) for Training on Each Part #\n",
    "    PCNN_timer = time.time(); PCNN_timer = -math.inf; N_params_Architope = 0; N_params_tally = 0\n",
    "    # Remove Eager Execution Error(s)\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    # Automatically Initialize Correct Input/Output Dimension(s)\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]; param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "    # Decide if/or not to tie neuron numbers of sub-patterns together\n",
    "    if Tied_Neurons_Q == True:\n",
    "        param_grid_Vanilla_Nets['height'] = [int(np.maximum(round(param_grid_Vanilla_Nets['height'][0]/N_parts),min_width))]\n",
    "        param_grid_Vanilla_Nets['epochs'] = [int(np.maximum(round(param_grid_Vanilla_Nets['epochs'][0]/int(round(np.sqrt(N_parts)))),min_epochs))]\n",
    "#         param_grid_Deep_Classifier['height'] = [int(np.maximum(round(param_grid_Deep_Classifier['height'][0]/N_parts),min_width))]\n",
    "\n",
    "    for current_part in range(N_parts_Generated_by_Algo_2):\n",
    "        # Update User #\n",
    "        #-------------#\n",
    "        print('-----------------------------------------------------------')\n",
    "        print('Currently Training Part: '+str(current_part)+'/'+str(N_parts_Generated_by_Algo_2 )+'Total Parts.')\n",
    "        print('-----------------------------------------------------------')\n",
    "\n",
    "        # Timer for Part\n",
    "        part_training_timer = time.time()\n",
    "        # Get Data for Sub-Pattern\n",
    "        X_loop = pd.DataFrame(X_parts_list[current_part])\n",
    "        y_loop = (y_parts_list[current_part]).reshape(-1,)\n",
    "        # Train ffNN\n",
    "        if randomize_subpattern_construction == False:\n",
    "            y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN(n_folds = 4, \n",
    "                                                                                  n_jobs = n_jobs,\n",
    "                                                                                  n_iter = n_iter, \n",
    "                                                                                  param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                  X_train= X_loop, \n",
    "                                                                                  y_train=y_loop,\n",
    "                                                                                  X_test_partial=X_train,\n",
    "                                                                                  X_test=X_test,\n",
    "                                                                                  NOCV=True)\n",
    "        else:\n",
    "            y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN_random(X_loop,\n",
    "                                                                                           X_train,\n",
    "                                                                                           X_test,\n",
    "                                                                                           y_loop,\n",
    "                                                                                           param_grid_Vanilla_Nets)\n",
    "        # Reshape y\n",
    "        ## Training\n",
    "        y_train.shape = (y_train.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop.shape = (y_hat_part_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        ## Testing\n",
    "        y_test.shape = (y_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop_test.shape = (y_hat_part_loop_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "\n",
    "        # Append predictions to data-frames\n",
    "        ## If first prediction we initialize data-frames\n",
    "        if current_part==0:\n",
    "            # Register quality\n",
    "            print(y_hat_part_loop.shape)\n",
    "            print(y_train.shape)\n",
    "            training_quality = np.array(np.abs(y_hat_part_loop-y_train)).reshape(y_hat_part_loop.shape[0],1)\n",
    "#             training_quality = np.array(np.abs(y_hat_part_loop-y_loop)).reshape(y_hat_part_loop.shape[0],1)\n",
    "\n",
    "            # Save Predictions\n",
    "            predictions_train = y_hat_part_loop.reshape(y_hat_part_loop.shape[0],1)\n",
    "            predictions_test = y_hat_part_loop_test.reshape(y_hat_part_loop_test.shape[0],1)\n",
    "\n",
    "\n",
    "        ## If not first prediction we append to already initialized dataframes\n",
    "        else:\n",
    "        # Register Best Scores\n",
    "            #----------------------#\n",
    "            # Write Predictions \n",
    "            # Save Predictions\n",
    "            y_hat_train_loop = y_hat_part_loop.reshape(predictions_train.shape[0],1)\n",
    "            predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "            y_hat_test_loop = y_hat_part_loop_test.reshape(predictions_test.shape[0],1)\n",
    "            predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "\n",
    "            # Evaluate Errors #\n",
    "            #-----------------#\n",
    "            # Training\n",
    "            prediction_errors = np.abs(y_hat_train_loop-y_train)\n",
    "#             prediction_errors = np.abs(y_hat_train_loop-y_loop)\n",
    "            training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "\n",
    "        #==============================#\n",
    "        # Update Performance Metric(s) #\n",
    "        #==============================#\n",
    "        part_training_timer = time.time() - part_training_timer\n",
    "        # L-Time\n",
    "        L_timer += partitioning_time\n",
    "        # P-Time\n",
    "        P_timer = max(P_timer,part_training_timer)\n",
    "        # N. Params\n",
    "        N_neurons += N_neurons_PCNN_loop\n",
    "        # Mean Width for Sub-Network(s)\n",
    "        Mean_Width_Subnetworks += param_grid_Vanilla_Nets['height'][0]\n",
    "\n",
    "    # Take Mean of Width(s)\n",
    "    Mean_Width_Subnetworks = Mean_Width_Subnetworks/N_parts_Generated_by_Algo_2\n",
    "    print('-----------------------')\n",
    "    print('Training Deep Zero-Sets')\n",
    "    print('-----------------------')\n",
    "\n",
    "\n",
    "    # Time Elapsed for Training Deep Zero-Sets\n",
    "    Deep_Zero_Sets_timer = time.time()\n",
    "\n",
    "    ## Initialize Classes Labels\n",
    "    if softmax_layer == False:\n",
    "        # No pooling (classical)\n",
    "        partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "    else:\n",
    "        # Max Pooling\n",
    "#         partition_labels_training_integers = (training_quality == training_quality.min(axis=1)[:,None]).astype(int)\n",
    "        partition_labels_training_integers = np.apply_along_axis(softminn, 1, training_quality).astype(int)\n",
    "    partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "    ## Build Classes\n",
    "    for part_column_i in range(1,(training_quality.shape[1])):\n",
    "        partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                               (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                              ],axis=1)\n",
    "    ## Convert to integers\n",
    "    partition_labels_training = partition_labels_training+0\n",
    "    ## Train simple deep classifier\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter =n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train.values, \n",
    "                                                                                                        y_train = partition_labels_training.values,\n",
    "                                                                                                        X_test = X_test.values)\n",
    "    # Get Binary Classes (Discontinuous Unit)\n",
    "    ## Training Set\n",
    "    predicted_classes_train = ((predicted_classes_train>gamma)*1).astype(int)\n",
    "    ## Testing Set\n",
    "    predicted_classes_test = ((predicted_classes_test > gamma)*1).astype(int)\n",
    "    # Get PC-NN Prediction(s)\n",
    "    ## Train\n",
    "    PCNN_prediction_y_train = (predictions_train*predicted_classes_train).sum(axis=1)\n",
    "    ## Test\n",
    "    PCNN_prediction_y_test = (predictions_test*predicted_classes_test).sum(axis=1)\n",
    "\n",
    "    # End Timer\n",
    "    Deep_Zero_Sets_timer = time.time() - Deep_Zero_Sets_timer\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Computing Final Performance Metrics')\n",
    "    print('-----------------------------------')\n",
    "    # Time-Elapsed Training Deep Classifier\n",
    "\n",
    "    # Update Times\n",
    "    L_timer +=Deep_Zero_Sets_timer\n",
    "    P_timer +=Deep_Zero_Sets_timer\n",
    "    # Update Number of Neurons Used\n",
    "    N_neurons_subPatterns = N_neurons\n",
    "    N_neurons_deep_Zero_Sets = (param_grid_Deep_Classifier['height'][0])*(param_grid_Deep_Classifier['depth'][0])\n",
    "    N_neurons = N_neurons_deep_Zero_Sets + N_neurons_subPatterns\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Peformance\n",
    "    performance_PCNN = reporter(y_train_hat_in=PCNN_prediction_y_train,y_test_hat_in=PCNN_prediction_y_test,\n",
    "                                y_train_in=y_train,\n",
    "                                y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_PCNN.to_latex((results_tables_path+\"PCNN_full_performance.tex\"))\n",
    "\n",
    "    # Update User\n",
    "    print(performance_PCNN)\n",
    "\n",
    "    ### Model Complexity/Efficiency Metrics\n",
    "    # Build AIC-like Metric #\n",
    "    #-----------------------#\n",
    "    AIC_like = 2*(N_neurons - np.log((performance_PCNN['test']['MAE'])))\n",
    "    AIC_like = np.round(AIC_like,3)\n",
    "    Efficiency = np.log(N_neurons) *(performance_PCNN['test']['MAE'])\n",
    "    Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "    # Build Table #\n",
    "    #-------------#\n",
    "    PCNN_Model_Complexity = pd.DataFrame({'L-time': [L_timer],\n",
    "                                               'P-time':[P_timer],\n",
    "                                               'N_params_expt': [N_neurons],\n",
    "                                               'AIC-like': [AIC_like],\n",
    "                                               'Eff': [Efficiency],\n",
    "                                               'N. Parts':[N_parts_Generated_by_Algo_2]})\n",
    "\n",
    "\n",
    "    # Write Required Training Time(s)\n",
    "    PCNN_Model_Complexity.to_latex((results_tables_path+\"PCNN_full_model_complexities.tex\"))\n",
    "\n",
    "    #--------------======---------------#\n",
    "    # Display Required Training Time(s) #\n",
    "    #--------------======---------------#\n",
    "    print(PCNN_Model_Complexity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #########################################\n",
    "    for j in range(10):\n",
    "        print('#------------------------------#')\n",
    "    #########################################\n",
    "    print('# ---- Getting Benchmarks ---- #')\n",
    "    #########################################\n",
    "    for j in range(10):\n",
    "        print('#------------------------------#')\n",
    "    #########################################\n",
    "    print('Training PCNN-lgt')\n",
    "    # Time-Elapsed Training linear classifier\n",
    "    Architope_logistic_classifier_training_begin = time.time()\n",
    "    if N_parts > 1:\n",
    "        parameters = {'penalty': ['none'], 'C': [0.1]}\n",
    "        lr = LogisticRegression(random_state=2020)\n",
    "        cv = RepeatedStratifiedKFold(n_splits=CV_folds, \n",
    "                                     n_repeats=n_iter, random_state=0)\n",
    "        classifier = RandomizedSearchCV(lr, \n",
    "                                        parameters, \n",
    "                                        random_state=2020)\n",
    "\n",
    "        # Initialize Classes Labels\n",
    "        partition_labels_training = np.argmin(training_quality,axis=-1)\n",
    "        # Train Logistic Classifier #\n",
    "        #---------------------------#\n",
    "        # Supress warnings caused by \"ignoring C\" for 'none' penalty and similar obvious warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Train Classifier\n",
    "        classifier.fit(X_train, partition_labels_training)\n",
    "    if N_parts >1 :\n",
    "        #### Write Predicted Class(es)\n",
    "        # Training Set\n",
    "        predicted_classes_train_logistic_BM = classifier.best_estimator_.predict(X_train)\n",
    "        Architope_prediction_y_train_logistic_BM = np.take_along_axis(predictions_train, predicted_classes_train_logistic_BM[:,None], axis=1)\n",
    "        # Testing Set\n",
    "        predicted_classes_test_logistic_BM = classifier.best_estimator_.predict(X_test)\n",
    "        Architope_prediction_y_test_logistic_BM = np.take_along_axis(predictions_test, \n",
    "                                                                     predicted_classes_test_logistic_BM[:,None], \n",
    "                                                                     axis=1)\n",
    "    else:\n",
    "        #### Write Predicted Class(es)\n",
    "        # Training Set\n",
    "        Architope_prediction_y_train_logistic_BM = predictions_train\n",
    "        # Testing Set\n",
    "        Architope_prediction_y_test_logistic_BM = predictions_test    \n",
    "    # Extract Number of Parameters Logistic Regressor\n",
    "    if N_parts > 1:\n",
    "        N_params_best_logistic = (classifier.best_estimator_.coef_.shape[0])*(classifier.best_estimator_.coef_.shape[1]) + len(classifier.best_estimator_.intercept_)\n",
    "    else:\n",
    "        N_params_best_logistic = 1\n",
    "    N_params_best_logistic = N_params_best_logistic + N_neurons_subPatterns*N_parts    \n",
    "    # Time-Elapsed Training linear classifier\n",
    "    Architope_logistic_classifier_training = time.time() - Architope_logistic_classifier_training_begin\n",
    "    #### Compute Performance\n",
    "    # Compute Peformance\n",
    "    performance_architope_ffNN_logistic = reporter(y_train_hat_in=Architope_prediction_y_train_logistic_BM,\n",
    "                                        y_test_hat_in=Architope_prediction_y_test_logistic_BM,\n",
    "                                        y_train_in=y_train,\n",
    "                                        y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_architope_ffNN_logistic.to_latex((results_tables_path+\"Architopes_logistic_performance.tex\"))\n",
    "    \n",
    "    ##### --- #####\n",
    "    print('Training PCNN-Bagged')\n",
    "    ##### --- #####\n",
    "    # Time for Bagging\n",
    "    Bagging_ffNN_bagging_time_begin = time.time()\n",
    "    # Train Bagging Weights in-sample\n",
    "    bagging_coefficients = LinearRegression().fit(predictions_train,y_train)\n",
    "    # Predict Bagging Weights out-of-sample\n",
    "    bagged_prediction_train = bagging_coefficients.predict(predictions_train)\n",
    "    bagged_prediction_test = bagging_coefficients.predict(predictions_test)\n",
    "    # Write number of trainable bagging parameters\n",
    "    N_bagged_parameters = len(bagging_coefficients.coef_) + 1\n",
    "    # Time for Bagging\n",
    "    Bagging_ffNN_bagging_time = time.time() - Bagging_ffNN_bagging_time_begin\n",
    "    # Compute Peformance\n",
    "    performance_bagged_ffNN = reporter(y_train_hat_in=bagged_prediction_train,\n",
    "                                        y_test_hat_in=bagged_prediction_test,\n",
    "                                        y_train_in=y_train,\n",
    "                                        y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_bagged_ffNN.to_latex((results_tables_path+\"ffNN_Bagged.tex\"))\n",
    "    \n",
    "    for jj in range(5):\n",
    "        print('-----------------------')\n",
    "    print('...Returning Results...')\n",
    "    for jj in range(5):\n",
    "        print('-----------------------')\n",
    "    # Return Output(s)\n",
    "    return performance_PCNN, PCNN_Model_Complexity, N_parts_Generated_by_Algo_2, N_neurons, N_neurons_subPatterns,N_neurons_deep_Zero_Sets, Mean_Width_Subnetworks, performance_architope_ffNN_logistic, N_params_best_logistic, performance_bagged_ffNN, Bagging_ffNN_bagging_time, Architope_logistic_classifier_training, Deep_Zero_Sets_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Perform Ablation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Ablation Completion Percentage: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "-------------------------------------------------------\n",
      "Randomly Initialized Parts - Via Randomized Algorithm 2\n",
      "-------------------------------------------------------\n",
      "The_parts_listhe number of parts are: 1.\n",
      "-----------------------------------------------------\n",
      "Training Sub-Networks on Each Randomly Generated Part\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 0/1Total Parts.\n",
      "-----------------------------------------------------------\n",
      "(416, 1)\n",
      "(416, 1)\n",
      "-----------------------\n",
      "Training Deep Zero-Sets\n",
      "-----------------------\n",
      "WARNING:tensorflow:From /Users/kratsi0000/opt/anaconda3/envs/bpcnn/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 416 samples\n",
      "416/416 [==============================] - 0s 677us/sample - loss: 0.6728 - accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (299,1) (9585,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b337ffc87254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mN_parts_possibilities_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_parts_possibilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minplicit_N_parts_loop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Run Algos. 1+2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mperformance_Architope_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArchitope_Model_Complexity_full_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_parts_Generated_by_Algo_2_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_params_architope_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_neurons_subPatterns_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_neurons_deep_Zero_Sets_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_mean_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance_PCNN_ffNN_logistic_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_params_PCNN_logistic_loop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperformance_bagged_ffNN_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaggin_time_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogistic_time_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeep_Zero_Sets_timer_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_PCNNs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_parts_possibilities_loop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mperformance_Architope_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformance_Architope_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1b54b371d85c>\u001b[0m in \u001b[0;36mget_PCNNs\u001b[0;34m(N_parts, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mPCNN_prediction_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions_train\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpredicted_classes_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m## Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mPCNN_prediction_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions_test\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpredicted_classes_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# End Timer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (299,1) (9585,1) "
     ]
    }
   ],
   "source": [
    "# Initialize \n",
    "# q_implicit_N_parts_possibilities = np.linspace(min_parts_threshold,max_parts_threshold,N_plot_finess)\n",
    "N_parts_possibilities = np.unique(np.round(np.linspace(N_min_parts,N_max_plots,num=N_plot_finess))).astype(int)\n",
    "# Custom: N_parts_possibilities = np.array([1,2,200]); N_plot_finess = len(N_parts_possibilities)\n",
    "\n",
    "# Get Performance Metric\n",
    "for inplicit_N_parts_loop in range(len(N_parts_possibilities)):\n",
    "    ### UPDATE USER ###\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    print('Ablation Completion Percentage:',(inplicit_N_parts_loop/N_plot_finess))\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    \n",
    "    # Implicitly Set: Current Number of Parts\n",
    "#     q_implicit_N_parts_loop = q_implicit_N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    N_parts_possibilities_loop = N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    # Run Algos. 1+2\n",
    "    performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, N_neurons_subPatterns_loop, N_neurons_deep_Zero_Sets_loop, height_mean_loop, performance_PCNN_ffNN_logistic_loop, N_params_PCNN_logistic_loop,performance_bagged_ffNN_loop, baggin_time_loop, logistic_time_loop, Deep_Zero_Sets_timer_loop = get_PCNNs(N_parts_possibilities_loop,X_train,y_train,X_test,y_test)\n",
    "    # Reshape\n",
    "    performance_Architope_loop = performance_Architope_loop.to_numpy().reshape([3,2,1])\n",
    "    Architope_Model_Complexity_full_loop = Architope_Model_Complexity_full_loop.to_numpy().reshape([1,6,1])\n",
    "    performance_PCNN_ffNN_logistic_loop = performance_PCNN_ffNN_logistic_loop.to_numpy().reshape([3,2,1])\n",
    "    performance_bagged_ffNN_loop = performance_bagged_ffNN_loop.to_numpy().reshape([3,2,1])\n",
    "    # Record\n",
    "    if inplicit_N_parts_loop == 0:\n",
    "        # Don't count partitioner if only one parts is active!\n",
    "        if N_parts_possibilities_loop <= 1:\n",
    "            Architope_Model_Complexity_full_loop[:,1] = Architope_Model_Complexity_full_loop[:,0]\n",
    "            N_neurons_deep_Zero_Sets_loop = 0\n",
    "        # Record Model Complexities Otherwise    \n",
    "        performance_Architope_history = performance_Architope_loop\n",
    "        Architope_Model_Complexity_history = Architope_Model_Complexity_full_loop\n",
    "        N_parts_Generated_by_Algo_2_history = N_parts_Generated_by_Algo_2_loop\n",
    "        N_params_subPatterns_hist = N_neurons_subPatterns_loop\n",
    "        N_neurons_deep_Zero_Sets_hist = N_neurons_deep_Zero_Sets_loop\n",
    "        N_params_architope_hist = N_neurons_deep_Zero_Sets_loop + N_neurons_subPatterns_loop\n",
    "        height_mean_hist = height_mean_loop\n",
    "        N_neurons_per_input = N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))\n",
    "        ### BENCHMARKs\n",
    "        ### Logistic PCNN\n",
    "        performance_PCNN_ffNN_logistic_hist = performance_PCNN_ffNN_logistic_loop\n",
    "        N_params_PCNN_logistic_hist = N_params_PCNN_logistic_loop\n",
    "        logistic_time_hist =  logistic_time_loop\n",
    "        baggin_time_hist = baggin_time_loop\n",
    "        ### Bagged PCNNs\n",
    "        performance_bagged_ffNN_hist = performance_bagged_ffNN_loop\n",
    "        ### Misc\n",
    "        Deep_Zero_Sets_timer_hist = Deep_Zero_Sets_timer_loop\n",
    "    else:\n",
    "        performance_Architope_history = np.concatenate((performance_Architope_history,performance_Architope_loop),axis=2)\n",
    "        Architope_Model_Complexity_history = np.concatenate((Architope_Model_Complexity_history,Architope_Model_Complexity_full_loop),axis=2)\n",
    "        N_parts_Generated_by_Algo_2_history = np.append(N_parts_Generated_by_Algo_2_history,N_parts_Generated_by_Algo_2_loop)\n",
    "        N_params_architope_hist = np.append(N_params_architope_hist,N_params_architope_loop)\n",
    "        N_params_subPatterns_hist = np.append(N_params_subPatterns_hist,N_neurons_subPatterns_loop)\n",
    "        N_neurons_deep_Zero_Sets_hist = np.append(N_neurons_deep_Zero_Sets_hist,N_neurons_deep_Zero_Sets_loop)\n",
    "        height_mean_hist = np.append(height_mean_hist,height_mean_loop)\n",
    "        N_neurons_per_input = np.append(N_neurons_per_input,(N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))))\n",
    "        ### Logistic PCNN\n",
    "        performance_PCNN_ffNN_logistic_hist = np.concatenate((performance_PCNN_ffNN_logistic_hist,\n",
    "                                                              performance_PCNN_ffNN_logistic_loop),\n",
    "                                                             axis=2)\n",
    "        N_params_PCNN_logistic_hist = np.append(N_params_PCNN_logistic_hist,N_params_PCNN_logistic_loop)\n",
    "        logistic_time_hist = np.append(logistic_time_hist,logistic_time_loop)\n",
    "        ### Bagged Performance\n",
    "        performance_bagged_ffNN_hist = np.concatenate((performance_bagged_ffNN_hist,\n",
    "                                                       performance_bagged_ffNN_loop),\n",
    "                                                      axis=2)\n",
    "        baggin_time_hist = np.append(baggin_time_hist,baggin_time_loop)\n",
    "        ### Misc\n",
    "        Deep_Zero_Sets_timer_hist = np.append(Deep_Zero_Sets_timer_hist,Deep_Zero_Sets_timer_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416,)\n",
      "(416, 1)\n",
      "(416,)\n",
      "(9585, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9585, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr,y_t,Np = build_ffNN_random(X_train,X_train,X_test,y_train,param_grid_Vanilla_Nets)\n",
    "print(y_tr.shape)\n",
    "print(y_train.shape)\n",
    "print(y_t.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "## Randomization may produce duplicates; we remove these with the following snippet:\n",
    "get_unique_entries = np.unique(N_parts_Generated_by_Algo_2_history, return_index=True)[1]\n",
    "N_parts_Generated_by_Algo_2_history_report = N_parts_Generated_by_Algo_2_history[get_unique_entries]\n",
    "\n",
    "# Write\n",
    "## Prediction Qualities\n",
    "performance_Architope_history_report_MAE_train = (performance_Architope_history[0,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MAE_test = (performance_Architope_history[0,1,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_train = (performance_Architope_history[1,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_test = (performance_Architope_history[1,1,:])[get_unique_entries]\n",
    "## Model Complexities\n",
    "L_Times = (Architope_Model_Complexity_history[:,1].reshape(-1,))[get_unique_entries]\n",
    "P_Times = (Architope_Model_Complexity_history[:,0].reshape(-1,))[get_unique_entries]\n",
    "N_Params = (N_params_architope_hist.reshape(-1,))[get_unique_entries]\n",
    "mean_subpattern_widths_hist = (height_mean_hist.reshape(-1,))[get_unique_entries]\n",
    "AIC_Like = (Architope_Model_Complexity_history[:,3].reshape(-1,))[get_unique_entries]\n",
    "Eff = (Architope_Model_Complexity_history[:,4].reshape(-1,))[get_unique_entries]\n",
    "N_neurons_per_input = (N_neurons_per_input.reshape(-1,))[get_unique_entries]\n",
    "## Misc\n",
    "Deep_Zero_Sets_timer_hist = Deep_Zero_Sets_timer_hist[get_unique_entries]\n",
    "\n",
    "# Record Benchmark Complexities\n",
    "## PCNN-lgt\n",
    "performance_lgt_ffNN_report_MAE_train = (performance_PCNN_ffNN_logistic_hist[0,0,:])[get_unique_entries]\n",
    "performance_lgt_ffNN_report_MAE_test = (performance_PCNN_ffNN_logistic_hist[0,1,:])[get_unique_entries]\n",
    "performance_lgt_ffNN_report_MSE_train = (performance_PCNN_ffNN_logistic_hist[1,0,:])[get_unique_entries]\n",
    "performance_lgt_ffNN_report_MSE_test = (performance_PCNN_ffNN_logistic_hist[1,1,:])[get_unique_entries]\n",
    "N_params_PCNN_logistic_hist = (N_params_subPatterns_hist + N_parts_Generated_by_Algo_2_history_report)[get_unique_entries]\n",
    "N_params_PCNN_logistic_hist_per_input = N_parts_Generated_by_Algo_2_history_report + N_params_subPatterns_hist[get_unique_entries]\n",
    "P_time_PCNN_lgt = logistic_time_hist[get_unique_entries] + P_Times - Deep_Zero_Sets_timer_hist\n",
    "L_time_PCNN_lgt = logistic_time_hist[get_unique_entries] + L_Times - Deep_Zero_Sets_timer_hist\n",
    "## PCNN-bag\n",
    "performance_PCNN_ffNN_bag_report_MAE_train = (performance_bagged_ffNN_hist[0,0,:])[get_unique_entries]\n",
    "performance_PCNN_ffNN_bag_report_MAE_test = (performance_bagged_ffNN_hist[0,1,:])[get_unique_entries]\n",
    "performance_PCNN_ffNN_bag_report_MSE_train = (performance_bagged_ffNN_hist[1,0,:])[get_unique_entries]\n",
    "performance_PCNN_ffNN_bag_report_MSE_test = (performance_bagged_ffNN_hist[1,1,:])[get_unique_entries]\n",
    "N_params_PCNN_ffNN_bag = (N_params_subPatterns_hist*N_parts_Generated_by_Algo_2_history_report)[get_unique_entries]\n",
    "N_params_PCNN_ffNN_bag_per_input = N_params_PCNN_ffNN_bag\n",
    "P_time_PCNN_bag = baggin_time_hist[get_unique_entries] + P_Times - Deep_Zero_Sets_timer_hist\n",
    "L_time_PCNN_bag = baggin_time_hist[get_unique_entries] + L_Times - Deep_Zero_Sets_timer_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Get Best PCNN\n",
    "This is identified as the PCNN with the smallest training MAE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_N_parts = np.argmin(performance_Architope_history_report_MAE_test)\n",
    "# Get PCNN Performance Metrics\n",
    "PCNN_MAE_train = performance_Architope_history_report_MAE_train[best_N_parts]\n",
    "PCNN_MAE_test = performance_Architope_history_report_MAE_test[best_N_parts]\n",
    "PCNN_MSE_train = performance_Architope_history_report_MSE_train[best_N_parts]\n",
    "PCNN_MSE_test = performance_Architope_history_report_MSE_test[best_N_parts]\n",
    "PCNN_performance_all = performance_Architope_history[:,:,best_N_parts]\n",
    "## Model Complexities\n",
    "PCNN_L_time = L_Times[best_N_parts]\n",
    "PCNN_P_time = P_Times[best_N_parts]\n",
    "PCNN_subpattern_widths_hist = mean_subpattern_widths_hist[best_N_parts]\n",
    "PCNN_AIC_Like = AIC_Like[best_N_parts]\n",
    "PCNN_Eff = Eff[best_N_parts]\n",
    "PCNN_N_neurons_per_input = N_neurons_per_input[best_N_parts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Get Other Benchmark(s)\n",
    "## Feedforward Neural Network (ffNN) Benchmark\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Model complexities for ffNNs\n",
    "P_time_ffNN = P_Times[0]\n",
    "L_time_ffNN = P_Times[0]\n",
    "Width_ffNN = height_mean_hist[0]\n",
    "# For: Plots\n",
    "MAE_ffNN = np.repeat(performance_Architope_history_report_MAE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "MSE_ffNN = np.repeat(performance_Architope_history_report_MSE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "L_times_ffNN_plot = np.repeat(L_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "P_times_ffNN_plot = np.repeat(P_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_per_input_ffNN = np.repeat(N_neurons_per_input[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "Width_neurons_ffNN = np.repeat(mean_subpattern_widths_hist[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_ffNN = np.repeat(N_Params[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "# Record in Table\n",
    "ffNN_Model_Complexity = pd.DataFrame({'L-time': [L_time_ffNN],\n",
    "                                               'P-time':[P_time_ffNN],\n",
    "                                               'N_params_expt': [N_neurons_ffNN],\n",
    "                                               'AIC-like': [0],\n",
    "                                               'Eff': [0],\n",
    "                                               'N. Parts':[1]})\n",
    "# Misc\n",
    "ffNN_performance_all = performance_Architope_history[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Random Forest Regressor (GBRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print('Training Gradient-Boosted Random Forest: In-progress...')\n",
    "# Run from External Script\n",
    "exec(open('Gradient_Boosted_Random_Forest_Regressor.py').read())\n",
    "# Update User #\n",
    "#-------------#\n",
    "print('Training of Gradient-Boosted Random Forest: Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write Required Training Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print('Completing Table: Required Training Times')\n",
    "\n",
    "# Format Required Training Time(s)\n",
    "training_times_In_Line = pd.DataFrame({'ffNN': [round(L_Times[0],3)],\n",
    "                                       'GBRF': [round(Gradient_boosted_Random_forest_time,3)],\n",
    "                                       'ffNN-Bag': [round(L_time_PCNN_bag[best_N_parts],3)],\n",
    "                                       'ffNN-log': [round(L_time_PCNN_lgt[best_N_parts],3)],\n",
    "                                       'PCNN': [round(PCNN_L_time,3)]\n",
    "                                      },index=['In-Line (L-Time)'])\n",
    "\n",
    "training_times_Parallel = pd.DataFrame({'ffNN': ['-'],\n",
    "                                       'GBRF': ['-'],\n",
    "                                       'ffNN-Bag': [round(L_time_PCNN_bag[best_N_parts],3)],\n",
    "                                       'ffNN-log': [round(L_time_PCNN_lgt[best_N_parts],3)],\n",
    "                                       'PCNN': [round(PCNN_L_time,3)]\n",
    "                                      },index=['Parallel (P-Time)'])\n",
    "\n",
    "# Combine Training Times into Single Data-Frame #\n",
    "#-----------------------------------------------#\n",
    "Model_Training_times = training_times_In_Line.append(training_times_Parallel)\n",
    "Model_Training_times = Model_Training_times.transpose()\n",
    "\n",
    "# Write Required Training Time(s)\n",
    "Model_Training_times.to_latex((results_tables_path+\"Model_Training_Times.tex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Metric(s)\n",
    "#### Write Predictive Performance Dataframe(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Training Performance\n",
    "predictive_performance_training = pd.DataFrame({'ffNN': ffNN_performance_all[:,0],\n",
    "                                                'GBRF': Gradient_boosted_tree.train,\n",
    "                                                'ffNN-bag': performance_bagged_ffNN_hist[:,:,best_N_parts][:,0],\n",
    "                                                'ffNN-lgt': performance_PCNN_ffNN_logistic_hist[:,:,best_N_parts][:,0],\n",
    "                                                'PCNN': PCNN_performance_all[:,0]})\n",
    "predictive_performance_training = predictive_performance_training.transpose()\n",
    "\n",
    "# Write Testing Performance\n",
    "predictive_performance_test = pd.DataFrame({'ffNN': ffNN_performance_all[:,1],\n",
    "                                            'GBRF': Gradient_boosted_tree.test,\n",
    "                                            'ffNN-bag': performance_bagged_ffNN_hist[:,:,best_N_parts][:,1],\n",
    "                                            'ffNN-lgt': performance_PCNN_ffNN_logistic_hist[:,:,best_N_parts][:,1],\n",
    "                                            'PCNN': PCNN_performance_all[:,1]})\n",
    "predictive_performance_test = predictive_performance_test.transpose()\n",
    "\n",
    "# Write into one Dataframe #\n",
    "#--------------------------#\n",
    "predictive_performance_training.to_latex((results_tables_path+\"Models_predictive_performance_training.tex\"))\n",
    "predictive_performance_test.to_latex((results_tables_path+\"Models_predictive_performance_testing.tex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Plots\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MSE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MSE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MSE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_lgt_ffNN_report_MSE_test,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_PCNN_ffNN_bag_report_MSE_test,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "# Add Legend\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MSE_test___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MAE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MAE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MAE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MAE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_lgt_ffNN_report_MAE_test,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_PCNN_ffNN_bag_report_MAE_test,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MAE___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"L-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"L-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_time_PCNN_lgt,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_time_PCNN_bag,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_L_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"P-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"P-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_time_PCNN_lgt,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_time_PCNN_bag,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_P_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"N. Params\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"N. Params\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_Params,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_logistic_hist,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_ffNN_bag,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_N_Params___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Active Neurons Per Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Active Neurons per. Input\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Active Neurons per. Input\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_per_input,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_logistic_hist_per_input,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_ffNN_bag_per_input,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Active_Neurons_per_input___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Widths for Sub-Pattern Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Mean Subpattern Widths\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Mean Subpattern Widths\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         Width_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Mean_Widths___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print('Prediction Metric(s)')\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print(predictive_performance_test)\n",
    "for j in range(3):\n",
    "    print(' ')\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print('Model Complexitie(s)')\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print(training_times_In_Line)\n",
    "print(training_times_Parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
