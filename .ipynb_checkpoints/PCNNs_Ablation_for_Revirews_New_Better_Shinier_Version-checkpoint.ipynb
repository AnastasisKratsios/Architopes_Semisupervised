{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation: Semi-Supervised Architope - for Reviews\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"PC-NNs\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f182e5072129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Softmax Layer instead of sigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msoftmax_layer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax_layer' is not defined"
     ]
    }
   ],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 1-(1/48)\n",
    "min_width = 200\n",
    "# Ablation Finess\n",
    "N_plot_finess = 4\n",
    "# min_parts_threshold = .001; max_parts_threshold = 0.9\n",
    "N_min_parts = 1; N_max_plots = 4\n",
    "Tied_Neurons_Q = True\n",
    "# Partition with Inputs (determine parts with domain) or outputs (determine parts with image)\n",
    "Partition_using_Inputs = True\n",
    "# Cuttoff Level\n",
    "gamma = .5\n",
    "# Softmax Layer instead of sigmoid\n",
    "softmax_layer = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------#\n",
    "# Only For Motivational Example Only #\n",
    "#------------------------------------#\n",
    "## Hyperparameters\n",
    "percentage_in_row = .25\n",
    "N = 10**4\n",
    "\n",
    "def f_1(x):\n",
    "    return x\n",
    "def f_2(x):\n",
    "    return np.exp(-x)\n",
    "x_0 = 0\n",
    "x_end = 1\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "if Option_Function != \"Motivational_Example\": \n",
    "    exec(open('Financial_Data_Preprocessor.py').read())\n",
    "else:\n",
    "    print(1)\n",
    "    exec(open('Motivational_Example.py').read())\n",
    "    print(\"Training Data size: \",X_train.shape[0])\n",
    "# exec(open('Prepare_Data_California_Housing.py').read())\n",
    "# Import time separately\n",
    "import time\n",
    "\n",
    "# TEMP\n",
    "# import pickle_compat\n",
    "# pickle_compat.patch()\n",
    "# param_grid_Vanilla_Nets['input_dim']=X_train.shape[1]\n",
    "sns.set()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit Partion Builder:\n",
    "Implements exactly Algorithm 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(X_in,\n",
    "                               y_in,\n",
    "                               N_parts_to_get=4):\n",
    "\n",
    "    # Compute Size of each part\n",
    "    size_part_reference = int(round(X_in.shape[0]/N_parts_to_get))\n",
    "\n",
    "    # Apply random bijection #\n",
    "    #------------------------#\n",
    "    ## Get random bijection indices\n",
    "    random_bijection_indices = np.random.choice(range(X_in.shape[0]),size=X_in.shape[0], replace=False)\n",
    "    ## Apply random bijections\n",
    "    X_in_shuffled = X_in[random_bijection_indices,:]\n",
    "    y_in_shuffled = y_in[random_bijection_indices,:]\n",
    "\n",
    "    # Initialize Lists #\n",
    "    #------------------#\n",
    "    X_parts = []\n",
    "    y_parts = []\n",
    "\n",
    "    for i_th_part_to_get in range(N_parts_to_get):\n",
    "        # Build random balls #\n",
    "        #--------------------#\n",
    "        ## Sample random radius\n",
    "        size_part = int(np.maximum(1,np.round(size_part_reference*np.random.uniform(low=.5,high=1.5,size=1)[0])))\n",
    "        ## Sample random point\n",
    "        X_center_loop_index = np.random.choice(range(X_in_shuffled.shape[0]),size=1, replace=False)\n",
    "        X_center_loop = X_in_shuffled[X_center_loop_index,:]\n",
    "        ## Compute Typical Distances from Center\n",
    "        distances_loop = X_center_loop-X_in_shuffled\n",
    "        distances_loop = np.linalg.norm(distances_loop, axis=1)\n",
    "\n",
    "        # Remove Random Ball from Dataset\n",
    "        if size_part <= len(distances_loop):\n",
    "            ## Identify indices\n",
    "            indices_smallest_to_random_ball = np.argsort(distances_loop)[:size_part]\n",
    "        else:\n",
    "            print('Final Loop')\n",
    "            indices_smallest_to_random_ball = np.array(range(X_in_shuffled.shape[0]))\n",
    "        ## Extract Parts\n",
    "        X_current_part_loop = X_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        y_current_part_loop = y_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        ## Append to List of Parts\n",
    "        X_parts.append(X_current_part_loop)\n",
    "        y_parts.append(y_current_part_loop)\n",
    "\n",
    "        # Remove Selected Entries From Array #\n",
    "        #------------------------------------#\n",
    "        X_in_shuffled = np.delete(X_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "        y_in_shuffled = np.delete(y_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "\n",
    "        # Failsafe if procedure has terminated\n",
    "        if X_in_shuffled.shape[0] == 0:\n",
    "            print('breaking early')\n",
    "            break\n",
    "    # Count Number of Parts Generated        \n",
    "    N_parts_generated = len(X_parts)\n",
    "    # Output Parts\n",
    "    return X_parts, y_parts, N_parts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCNNs(N_parts,X_train,y_train,X_test,y_test):\n",
    "\n",
    "    # Initialization(s) #\n",
    "    #-------------------#\n",
    "    N_neurons = 0\n",
    "    L_timer = 0\n",
    "    P_timer = 0\n",
    "    Mean_Width_Subnetworks = 0\n",
    "\n",
    "    # Partitioner Begin #\n",
    "    #-------------------#\n",
    "    import time\n",
    "    partitioning_time_begin = time.time()\n",
    "    print('-------------------------------------------------------')\n",
    "    print('Randomly Initialized Parts - Via Randomized Algorithm 2')\n",
    "    print('-------------------------------------------------------')\n",
    "    if Partition_using_Inputs == True:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(X_train.to_numpy(),\n",
    "                                                                                             y_train.reshape(-1,1),\n",
    "                                                                                             N_parts)\n",
    "    else:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(y_train.reshape(-1,1),\n",
    "                                                                                             X_train.to_numpy(),\n",
    "                                                                                             N_parts)\n",
    "    partitioning_time = time.time() - partitioning_time_begin\n",
    "    print('The_parts_listhe number of parts are: ' + str(N_parts_Generated_by_Algo_2)+'.')\n",
    "    ############# Partitioner End ########\n",
    "\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Training Sub-Networks on Each Randomly Generated Part')\n",
    "    print('-----------------------------------------------------')\n",
    "    # Time-Elapse (Start) for Training on Each Part #\n",
    "    PCNN_timer = time.time(); PCNN_timer = -math.inf; N_params_Architope = 0; N_params_tally = 0\n",
    "    # Remove Eager Execution Error(s)\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    # Automatically Initialize Correct Input/Output Dimension(s)\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]; param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "    # Decide if/or not to tie neuron numbers of sub-patterns together\n",
    "    if Tied_Neurons_Q == True:\n",
    "        param_grid_Vanilla_Nets['height'] = [int(np.maximum(round(param_grid_Vanilla_Nets['height'][0]/N_parts),min_width))]\n",
    "        param_grid_Vanilla_Nets['epochs'] = [int(np.maximum(round(param_grid_Vanilla_Nets['epochs'][0]/int(round(np.sqrt(N_parts)))),min_width))]\n",
    "#         param_grid_Deep_Classifier['height'] = [int(np.maximum(round(param_grid_Deep_Classifier['height'][0]/N_parts),min_width))]\n",
    "\n",
    "    for current_part in range(N_parts_Generated_by_Algo_2):\n",
    "        # Update User #\n",
    "        #-------------#\n",
    "        print('-----------------------------------------------------------')\n",
    "        print('Currently Training Part: '+str(current_part)+'/'+str(N_parts_Generated_by_Algo_2 )+'Total Parts.')\n",
    "        print('-----------------------------------------------------------')\n",
    "\n",
    "        # Timer for Part\n",
    "        part_training_timer = time.time()\n",
    "        # Get Data for Sub-Pattern\n",
    "        X_loop = pd.DataFrame(X_parts_list[current_part])\n",
    "        y_loop = (y_parts_list[current_part]).reshape(-1,)\n",
    "        # Train ffNN\n",
    "        y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN(n_folds = 4, \n",
    "                                                                              n_jobs = n_jobs,\n",
    "                                                                              n_iter = n_iter, \n",
    "                                                                              param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                              X_train= X_loop, \n",
    "                                                                              y_train=y_loop,\n",
    "                                                                              X_test_partial=X_train,\n",
    "                                                                              X_test=X_test,\n",
    "                                                                              NOCV=True)\n",
    "        # Reshape y\n",
    "        ## Training\n",
    "        y_train.shape = (y_train.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop.shape = (y_hat_part_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        ## Testing\n",
    "        y_test.shape = (y_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop_test.shape = (y_hat_part_loop_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "\n",
    "        # Append predictions to data-frames\n",
    "        ## If first prediction we initialize data-frames\n",
    "        if current_part==0:\n",
    "            # Register quality\n",
    "            training_quality = np.array(np.abs(y_hat_part_loop-y_train)).reshape(y_hat_part_loop.shape[0],1)\n",
    "\n",
    "            # Save Predictions\n",
    "            predictions_train = y_hat_part_loop.reshape(y_hat_part_loop.shape[0],1)\n",
    "            predictions_test = y_hat_part_loop_test.reshape(y_hat_part_loop_test.shape[0],1)\n",
    "\n",
    "\n",
    "        ## If not first prediction we append to already initialized dataframes\n",
    "        else:\n",
    "        # Register Best Scores\n",
    "            #----------------------#\n",
    "            # Write Predictions \n",
    "            # Save Predictions\n",
    "            y_hat_train_loop = y_hat_part_loop.reshape(predictions_train.shape[0],1)\n",
    "            predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "            y_hat_test_loop = y_hat_part_loop_test.reshape(predictions_test.shape[0],1)\n",
    "            predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "\n",
    "            # Evaluate Errors #\n",
    "            #-----------------#\n",
    "            # Training\n",
    "            prediction_errors = np.abs(y_hat_train_loop-y_train)\n",
    "            training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "\n",
    "        #==============================#\n",
    "        # Update Performance Metric(s) #\n",
    "        #==============================#\n",
    "        part_training_timer = time.time() - part_training_timer\n",
    "        # L-Time\n",
    "        L_timer += partitioning_time\n",
    "        # P-Time\n",
    "        P_timer = max(P_timer,part_training_timer)\n",
    "        # N. Params\n",
    "        N_neurons += N_neurons_PCNN_loop\n",
    "        # Mean Width for Sub-Network(s)\n",
    "        Mean_Width_Subnetworks += param_grid_Vanilla_Nets['height'][0]\n",
    "\n",
    "    # Take Mean of Width(s)\n",
    "    Mean_Width_Subnetworks = Mean_Width_Subnetworks/N_parts_Generated_by_Algo_2\n",
    "    print('-----------------------')\n",
    "    print('Training Deep Zero-Sets')\n",
    "    print('-----------------------')\n",
    "\n",
    "\n",
    "    # Time Elapsed for Training Deep Zero-Sets\n",
    "    Deep_Zero_Sets_timer = time.time()\n",
    "\n",
    "    ## Initialize Classes Labels\n",
    "    if softmax_layer == False:\n",
    "        # No pooling (classical)\n",
    "        partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "    else:\n",
    "        # Max Pooling\n",
    "#         partition_labels_training_integers = (training_quality == training_quality.min(axis=1)[:,None]).astype(int)\n",
    "        partition_labels_training_integers = np.apply_along_axis(softminn, 1, training_quality).astype(int)\n",
    "    partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "    ## Build Classes\n",
    "    for part_column_i in range(1,(training_quality.shape[1])):\n",
    "        partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                               (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                              ],axis=1)\n",
    "    ## Convert to integers\n",
    "    partition_labels_training = partition_labels_training+0\n",
    "    ## Train simple deep classifier\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter =n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train.values, \n",
    "                                                                                                        y_train = partition_labels_training.values,\n",
    "                                                                                                        X_test = X_test.values)\n",
    "    # Get Binary Classes (Discontinuous Unit)\n",
    "    ## Training Set\n",
    "    predicted_classes_train = ((predicted_classes_train>gamma)*1).astype(int)\n",
    "    ## Testing Set\n",
    "    predicted_classes_test = ((predicted_classes_test > gamma)*1).astype(int)\n",
    "    # Get PC-NN Prediction(s)\n",
    "    ## Train\n",
    "    PCNN_prediction_y_train = (predictions_train*predicted_classes_train).sum(axis=1)\n",
    "    ## Test\n",
    "    PCNN_prediction_y_test = (predictions_test*predicted_classes_test).sum(axis=1)\n",
    "\n",
    "    # End Timer\n",
    "    Deep_Zero_Sets_timer = time.time() - Deep_Zero_Sets_timer\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Computing Final Performance Metrics')\n",
    "    print('-----------------------------------')\n",
    "    # Time-Elapsed Training Deep Classifier\n",
    "\n",
    "    # Update Times\n",
    "    L_timer +=Deep_Zero_Sets_timer\n",
    "    P_timer +=Deep_Zero_Sets_timer\n",
    "    # Update Number of Neurons Used\n",
    "    N_neurons_subPatterns = N_neurons\n",
    "    N_neurons_deep_Zero_Sets = (param_grid_Deep_Classifier['height'][0])*(param_grid_Deep_Classifier['depth'][0])\n",
    "    N_neurons = N_neurons_deep_Zero_Sets + N_neurons_subPatterns\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Peformance\n",
    "    performance_PCNN = reporter(y_train_hat_in=PCNN_prediction_y_train,y_test_hat_in=PCNN_prediction_y_test,\n",
    "                                y_train_in=y_train,\n",
    "                                y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_PCNN.to_latex((results_tables_path+\"PCNN_full_performance.tex\"))\n",
    "\n",
    "    # Update User\n",
    "    print(performance_PCNN)\n",
    "\n",
    "    ### Model Complexity/Efficiency Metrics\n",
    "    # Build AIC-like Metric #\n",
    "    #-----------------------#\n",
    "    AIC_like = 2*(N_neurons - np.log((performance_PCNN['test']['MAE'])))\n",
    "    AIC_like = np.round(AIC_like,3)\n",
    "    Efficiency = np.log(N_neurons) *(performance_PCNN['test']['MAE'])\n",
    "    Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "    # Build Table #\n",
    "    #-------------#\n",
    "    PCNN_Model_Complexity = pd.DataFrame({'L-time': [L_timer],\n",
    "                                               'P-time':[P_timer],\n",
    "                                               'N_params_expt': [N_neurons],\n",
    "                                               'AIC-like': [AIC_like],\n",
    "                                               'Eff': [Efficiency],\n",
    "                                               'N. Parts':[N_parts_Generated_by_Algo_2]})\n",
    "\n",
    "\n",
    "    # Write Required Training Time(s)\n",
    "    PCNN_Model_Complexity.to_latex((results_tables_path+\"PCNN_full_model_complexities.tex\"))\n",
    "\n",
    "    #--------------======---------------#\n",
    "    # Display Required Training Time(s) #\n",
    "    #--------------======---------------#\n",
    "    print(PCNN_Model_Complexity)\n",
    "    \n",
    "    \n",
    "    # Return Output(s)\n",
    "    return performance_PCNN, PCNN_Model_Complexity, N_parts_Generated_by_Algo_2, N_neurons, N_neurons_subPatterns,N_neurons_deep_Zero_Sets, Mean_Width_Subnetworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Perform Ablation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize \n",
    "# q_implicit_N_parts_possibilities = np.linspace(min_parts_threshold,max_parts_threshold,N_plot_finess)\n",
    "N_parts_possibilities = np.unique(np.round(np.linspace(N_min_parts,N_max_plots,num=N_plot_finess))).astype(int)\n",
    "# Custom: N_parts_possibilities = np.array([1,2,3,4,5,8]); N_plot_finess = len(N_parts_possibilities)\n",
    "\n",
    "# Get Performance Metric\n",
    "for inplicit_N_parts_loop in range(len(N_parts_possibilities)):\n",
    "    ### UPDATE USER ###\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    print('Ablation Completion Percentage:',(inplicit_N_parts_loop/N_plot_finess))\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    \n",
    "    # Implicitly Set: Current Number of Parts\n",
    "#     q_implicit_N_parts_loop = q_implicit_N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    N_parts_possibilities_loop = N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    # Run Algos. 1+2\n",
    "    performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, N_neurons_subPatterns_loop, N_neurons_deep_Zero_Sets_loop, height_mean_loop = get_PCNNs(N_parts_possibilities_loop,X_train,y_train,X_test,y_test)\n",
    "#     performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, height_mean_loop = Ablate_PCNNs(q_implicit_N_parts_loop,data_y,X_train,X_test,y_test)\n",
    "    # Reshape\n",
    "    performance_Architope_loop = performance_Architope_loop.to_numpy().reshape([3,2,1])\n",
    "    Architope_Model_Complexity_full_loop = Architope_Model_Complexity_full_loop.to_numpy().reshape([1,6,1])\n",
    "\n",
    "    # Record\n",
    "    if inplicit_N_parts_loop == 0:\n",
    "        # Don't count partitioner if only one parts is active!\n",
    "        if N_parts_possibilities_loop <= 1:\n",
    "            Architope_Model_Complexity_full_loop[:,1] = Architope_Model_Complexity_full_loop[:,0]\n",
    "            N_neurons_deep_Zero_Sets_loop = 0\n",
    "        # Record Model Complexities Otherwise    \n",
    "        performance_Architope_history = performance_Architope_loop\n",
    "        Architope_Model_Complexity_history = Architope_Model_Complexity_full_loop\n",
    "        N_parts_Generated_by_Algo_2_history = N_parts_Generated_by_Algo_2_loop\n",
    "        N_params_subPatterns_hist = N_neurons_subPatterns_loop\n",
    "        N_neurons_deep_Zero_Sets_hist = N_neurons_deep_Zero_Sets_loop\n",
    "        N_params_architope_hist = N_neurons_deep_Zero_Sets_loop + N_neurons_subPatterns_loop\n",
    "        height_mean_hist = height_mean_loop\n",
    "        N_neurons_per_input = N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))\n",
    "    else:\n",
    "        performance_Architope_history = np.concatenate((performance_Architope_history,performance_Architope_loop),axis=2)\n",
    "        Architope_Model_Complexity_history = np.concatenate((Architope_Model_Complexity_history,Architope_Model_Complexity_full_loop),axis=2)\n",
    "        N_parts_Generated_by_Algo_2_history = np.append(N_parts_Generated_by_Algo_2_history,N_parts_Generated_by_Algo_2_loop)\n",
    "        N_params_architope_hist = np.append(N_params_architope_hist,N_params_architope_loop)\n",
    "        N_params_subPatterns_hist = np.append(N_params_subPatterns_hist,N_neurons_subPatterns_loop)\n",
    "        N_neurons_deep_Zero_Sets_hist = np.append(N_neurons_deep_Zero_Sets_hist,N_neurons_deep_Zero_Sets_loop)\n",
    "        height_mean_hist = np.append(height_mean_hist,height_mean_loop)\n",
    "        N_neurons_per_input = np.append(N_neurons_per_input,(N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))))\n",
    "\n",
    "# Cleanup\n",
    "## Randomization may produce duplicates; we remove these with the following snippet:\n",
    "get_unique_entries = np.unique(N_parts_Generated_by_Algo_2_history, return_index=True)[1]\n",
    "N_parts_Generated_by_Algo_2_history_report = N_parts_Generated_by_Algo_2_history[get_unique_entries]\n",
    "\n",
    "# Write\n",
    "## Prediction Qualities\n",
    "performance_Architope_history_report_MAE_train = (performance_Architope_history[0,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MAE_test = (performance_Architope_history[0,1,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_train = (performance_Architope_history[1,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_test = (performance_Architope_history[1,1,:])[get_unique_entries]\n",
    "## Model Complexities\n",
    "L_Times = (Architope_Model_Complexity_history[:,1].reshape(-1,))[get_unique_entries]\n",
    "P_Times = (Architope_Model_Complexity_history[:,0].reshape(-1,))[get_unique_entries]\n",
    "N_Params = (N_params_architope_hist.reshape(-1,))[get_unique_entries]\n",
    "mean_subpattern_widths_hist = (height_mean_hist.reshape(-1,))[get_unique_entries]\n",
    "AIC_Like = (Architope_Model_Complexity_history[:,3].reshape(-1,))[get_unique_entries]\n",
    "Eff = (Architope_Model_Complexity_history[:,4].reshape(-1,))[get_unique_entries]\n",
    "N_neurons_per_input = (N_neurons_per_input.reshape(-1,))[get_unique_entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Feedforward Neural Network (ffNN) Benchmark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Model complexities for ffNNs\n",
    "P_time_ffNN = P_Times[0]\n",
    "L_time_ffNN = P_Times[0]\n",
    "Width_ffNN = height_mean_hist[0]\n",
    "# For: Plots\n",
    "MAE_ffNN = np.repeat(performance_Architope_history_report_MAE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "MSE_ffNN = np.repeat(performance_Architope_history_report_MSE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "L_times_ffNN_plot = np.repeat(L_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "P_times_ffNN_plot = np.repeat(P_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_per_input_ffNN = np.repeat(N_neurons_per_input[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "Width_neurons_ffNN = np.repeat(mean_subpattern_widths_hist[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_ffNN = np.repeat(N_Params[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "# Record in Table\n",
    "ffNN_Model_Complexity = pd.DataFrame({'L-time': [L_time_ffNN],\n",
    "                                               'P-time':[P_time_ffNN],\n",
    "                                               'N_params_expt': [N_neurons_ffNN],\n",
    "                                               'AIC-like': [0],\n",
    "                                               'Eff': [0],\n",
    "                                               'N. Parts':[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Plots\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MSE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MSE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MSE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "# Add Legend\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MSE_test___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MAE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MAE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MAE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MAE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MAE___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"L-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"L-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_L_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"P-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"P-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_P_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"N. Params\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"N. Params\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_Params,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_N_Params___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Active Neurons Per Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Active Neurons per. Input\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Active Neurons per. Input\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_per_input,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Active_Neurons_per_input___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Widths for Sub-Pattern Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Mean Subpattern Widths\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Mean Subpattern Widths\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         Width_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Mean_Widths___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
