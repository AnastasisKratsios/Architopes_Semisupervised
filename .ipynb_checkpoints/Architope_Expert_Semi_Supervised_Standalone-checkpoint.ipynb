{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Architope: Semi-Supervised\n",
    "---\n",
    "\n",
    "**What this code does:** *This Code Implements the Expert-provided partition version of the architope.  Then, it uses it to (accelarate the learning of) learn the optimal model-dependant partition $K_{\\cdot}^{\\star}$.*\n",
    "\n",
    "**Why is it separate?**  *It can be run in parallel with the other codes since all seeds and states are the same.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 0.3\n",
    "min_height = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "exec(open('Prepare_Data_California_Housing.py').read())\n",
    "# Import time separately\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Times and Other Trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapse (Start) for Training on Each Part\n",
    "Architope_partition_training_begin_expt = time.time()\n",
    "# Initialize running max for Parallel time\n",
    "Architope_partitioning_max_time_running_exp = -math.inf # Initialize slowest-time at - infinity to force updating!\n",
    "# Initialize N_parameter counter for Architope\n",
    "N_params_Architope_deep_expt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Dataset using Expert Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Column Names #\n",
    "#--------------------#\n",
    "# Read from X\n",
    "X_col_names = (X.drop(['median_house_value'],axis=1).columns).values\n",
    "# Write to X_train and X_test\n",
    "X_train.columns= X_col_names\n",
    "X_test.columns= X_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Lists\n",
    "#------------------#\n",
    "## Initialize Data Lists\n",
    "X_train_list_expt = list()\n",
    "y_train_list_expt = list()\n",
    "X_test_list_expt = list()\n",
    "y_test_list_expt = list()\n",
    "\n",
    "## Initialize Predictions Lists\n",
    "y_hat_train_list_expt = list()\n",
    "y_hat_test_list_expt = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parts (expert opinion) and append to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------#\n",
    "# 1H Ocean #\n",
    "#----------#\n",
    "\n",
    "# Extract Train\n",
    "X_train_list_expt.append(X_train[X_train['ocean_proximity_<1H OCEAN']==1])\n",
    "y_train_list_expt.append(y_train[X_train['ocean_proximity_<1H OCEAN']==1])\n",
    "# Extract Test\n",
    "X_test_list_expt.append(X_test[X_test['ocean_proximity_<1H OCEAN']==1])\n",
    "y_test_list_expt.append(y_test[X_test['ocean_proximity_<1H OCEAN']==1])\n",
    "\n",
    "#--------#\n",
    "# INLAND #\n",
    "#--------#\n",
    "\n",
    "# Extract Train\n",
    "X_train_list_expt.append(X_train[X_train['ocean_proximity_INLAND']==1])\n",
    "y_train_list_expt.append(y_train[X_train['ocean_proximity_INLAND']==1])\n",
    "# Extract Test\n",
    "X_test_list_expt.append(X_test[X_test['ocean_proximity_INLAND']==1])\n",
    "y_test_list_expt.append(y_test[X_test['ocean_proximity_INLAND']==1])\n",
    "\n",
    "\n",
    "\n",
    "#----------#\n",
    "# NEAR BAY #\n",
    "#----------#\n",
    "\n",
    "# Extract Train\n",
    "X_train_list_expt.append(X_train[X_train['ocean_proximity_NEAR BAY']==1])\n",
    "y_train_list_expt.append(y_train[X_train['ocean_proximity_NEAR BAY']==1])\n",
    "# Extract Test\n",
    "X_test_list_expt.append(X_test[X_test['ocean_proximity_NEAR BAY']==1])\n",
    "y_test_list_expt.append(y_test[X_test['ocean_proximity_NEAR BAY']==1])\n",
    "\n",
    "\n",
    "#------------------------#\n",
    "# NEAR OCEAN & on ISLAND #\n",
    "#------------------------#\n",
    "\n",
    "# Extract Train\n",
    "X_train_list_expt.append(X_train[np.logical_or(X_train['ocean_proximity_ISLAND']==1,(X_train['ocean_proximity_NEAR OCEAN']==1))])\n",
    "y_train_list_expt.append(y_train[np.logical_or(X_train['ocean_proximity_ISLAND']==1,(X_train['ocean_proximity_NEAR OCEAN']==1))])\n",
    "# Extract Test\n",
    "X_test_list_expt.append(X_test[np.logical_or(X_test['ocean_proximity_ISLAND']==1,(X_test['ocean_proximity_NEAR OCEAN']==1))])\n",
    "y_test_list_expt.append(y_test[np.logical_or(X_test['ocean_proximity_ISLAND']==1,(X_test['ocean_proximity_NEAR OCEAN']==1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ratios\n",
    "N_ratios = np.array([])\n",
    "\n",
    "# Build Ratios\n",
    "for Number_part_i in range(len(X_train_list_expt)):\n",
    "    # Update Ratios\n",
    "    N_ratios = np.append(N_ratios,((X_train_list_expt[Number_part_i].shape[0])/X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Timers and Benchmarking tool(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapse (Start) for Training on Each Part\n",
    "Architope_partition_training_begin = time.time()\n",
    "# Initialize running max for Parallel time\n",
    "Architope_partitioning_max_time_running = -math.inf # Initialize slowest-time at - infinity to force updating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Export Architope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Current part: 0 out of : 4 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    8.6s remaining:    8.6s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    8.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "805/805 [==============================] - 1s 1ms/step - loss: 0.6261 - mse: 0.8063 - mae: 0.6261 - mape: 26.9881\n",
      "Epoch 2/2\n",
      "805/805 [==============================] - 1s 1ms/step - loss: 0.5742 - mse: 0.6635 - mae: 0.5742 - mape: 24.9241\n",
      "1806/1806 [==============================] - 1s 735us/step\n",
      "774/774 [==============================] - 1s 794us/step\n",
      "Status: Current part: 1 out of : 4 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    6.0s remaining:    6.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    6.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "572/572 [==============================] - 1s 1ms/step - loss: 0.3951 - mse: 0.3817 - mae: 0.3951 - mape: 33.2971\n",
      "Epoch 2/2\n",
      "572/572 [==============================] - 1s 1ms/step - loss: 0.3314 - mse: 0.2550 - mae: 0.3314 - mape: 26.9170\n",
      "1806/1806 [==============================] - 1s 756us/step\n",
      "774/774 [==============================] - 1s 945us/step\n",
      "Status: Current part: 2 out of : 4 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    4.0s remaining:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    4.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    4.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "198/198 [==============================] - 0s 1ms/step - loss: 0.8756 - mse: 1.3440 - mae: 0.8756 - mape: 39.7707\n",
      "Epoch 2/2\n",
      "198/198 [==============================] - 0s 1ms/step - loss: 0.7115 - mse: 0.9058 - mae: 0.7115 - mape: 31.2646\n",
      "1806/1806 [==============================] - 1s 759us/step\n",
      "774/774 [==============================] - 1s 934us/step\n",
      "Status: Current part: 3 out of : 4 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    4.8s remaining:    4.8s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    4.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8578 - mse: 1.3777 - mae: 0.8578 - mape: 39.7889\n",
      "Epoch 2/2\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 0.7575 - mse: 1.1480 - mae: 0.7575 - mape: 34.9338\n",
      "1806/1806 [==============================] - 1s 790us/step\n",
      "774/774 [==============================] - 1s 840us/step\n",
      " \n",
      " \n",
      " \n",
      "----------------------------------------------------\n",
      "Feature Generation (Learning Phase): Score Generated\n",
      "----------------------------------------------------\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "for current_part in range(len(X_train_list_expt)):\n",
    "    #==============#\n",
    "    # Timer(begin) #\n",
    "    #==============#\n",
    "    current_part_training_time_for_parallel_begin = time.time()\n",
    "    \n",
    "    \n",
    "    # Initializations #\n",
    "    #-----------------#\n",
    "    # Reload Grid\n",
    "    exec(open('Grid_Enhanced_Network.py').read())\n",
    "    # Modify heights according to optimal (data-driven) rule (with threshold)\n",
    "    current_height = np.ceil(np.array(param_grid_Vanilla_Nets['height'])*N_ratios[current_part])\n",
    "    current_height_threshold = np.repeat(min_height,(current_height.shape[0]))\n",
    "    current_height = np.maximum(current_height,current_height_threshold)\n",
    "    current_height = current_height.astype(int).tolist()\n",
    "    param_grid_Vanilla_Nets['height'] = current_height\n",
    "    # Automatically Fix Input Dimension\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]\n",
    "    param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    \n",
    "    # Update User #\n",
    "    #-------------#\n",
    "    print('Status: Current part: ' + str(current_part) + ' out of : '+str(len(X_train_list_expt)) +' parts.')\n",
    "    print('Heights to iterate over: '+str(current_height))\n",
    "    \n",
    "    # Generate Prediction(s) on current Part #\n",
    "    #----------------------------------------#\n",
    "    # Failsafe (number of data-points)\n",
    "    CV_folds_failsafe = min(CV_folds,max(1,(X_train.shape[0]-1)))\n",
    "    # Train Network\n",
    "    y_hat_train_full_loop, y_hat_test_full_loop, N_params_Architope_loop = build_ffNN(n_folds = CV_folds_failsafe, \n",
    "                                                                                     n_jobs = n_jobs,\n",
    "                                                                                     n_iter = n_iter, \n",
    "                                                                                     param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                     X_train= X_train_list_expt[current_part], \n",
    "                                                                                     y_train=y_train_list_expt[current_part],\n",
    "                                                                                     X_test_partial=X_train,\n",
    "                                                                                     X_test=X_test)\n",
    "    # Append predictions to data-frames\n",
    "    ## If first prediction we initialize data-frames\n",
    "    if current_part==0:\n",
    "        # Register quality\n",
    "        training_quality = np.array(np.abs(y_hat_train_full_loop-y_train))\n",
    "        training_quality = training_quality.reshape(training_quality.shape[0],1)\n",
    "\n",
    "        # Save Predictions\n",
    "        predictions_train = y_hat_train_full_loop\n",
    "        predictions_train = predictions_train.reshape(predictions_train.shape[0],1)\n",
    "        predictions_test = y_hat_test_full_loop\n",
    "        predictions_test = predictions_test.reshape(predictions_test.shape[0],1)\n",
    "        \n",
    "        \n",
    "    ## If not first prediction we append to already initialized dataframes\n",
    "    else:\n",
    "    # Register Best Scores\n",
    "        #----------------------#\n",
    "        # Write Predictions \n",
    "        # Save Predictions\n",
    "        y_hat_train_loop = y_hat_train_full_loop.reshape(predictions_train.shape[0],1)\n",
    "        predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "        y_hat_test_loop = y_hat_test_full_loop.reshape(predictions_test.shape[0],1)\n",
    "        predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "        \n",
    "        # Evaluate Errors #\n",
    "        #-----------------#\n",
    "        # Training\n",
    "        prediction_errors = np.abs(y_hat_train_loop.reshape(-1,)-y_train)\n",
    "        training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "        \n",
    "    #============#\n",
    "    # Timer(end) #\n",
    "    #============#\n",
    "    current_part_training_time_for_parallel = time.time() - current_part_training_time_for_parallel_begin\n",
    "    Architope_partitioning_max_time_running = max(Architope_partitioning_max_time_running,current_part_training_time_for_parallel)\n",
    "\n",
    "    #============---===============#\n",
    "    # N_parameter Counter (Update) #\n",
    "    #------------===---------------#\n",
    "    N_params_Architope_deep_expt = N_params_Architope_deep_expt + N_params_Architope_loop\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('----------------------------------------------------')\n",
    "print('Feature Generation (Learning Phase): Score Generated')\n",
    "print('----------------------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training on Each Part\n",
    "Architope_partition_training_expt = time.time() - Architope_partition_training_begin_expt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2) Train Deep Classifier with Expert \"Prior\" Partition $K_{\\cdot}^{exp}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-Elapsed Training Deep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Architope_deep_classifier_training_begin_deep_exp = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Classes (aka targets for classification task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Classes Labels\n",
    "partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "# Build Classes\n",
    "for part_column_i in range(1,(training_quality.shape[1])):\n",
    "    partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                           (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                          ],axis=1)\n",
    "# Convert to integers\n",
    "partition_labels_training = partition_labels_training+0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-Load Grid and Redefine Relevant Input/Output dimensions in dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Re-Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Re-Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "\n",
    "# Redefine (Dimension-related) Elements of Grid\n",
    "param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "param_grid_Deep_Classifier['output_dim'] = [partition_labels_training.shape[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   15.4s remaining:   15.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   15.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5399 - accuracy: 0.3498\n",
      "Epoch 2/2\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5053 - accuracy: 0.4079\n",
      "1806/1806 [==============================] - 1s 765us/step\n",
      "774/774 [==============================] - 1s 758us/step\n"
     ]
    }
   ],
   "source": [
    "# Train simple deep classifier\n",
    "predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter =n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train, \n",
    "                                                                                                        y_train = partition_labels_training,\n",
    "                                                                                                        X_test = X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-Elapsed Training Deep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Architope_deep_classifier_training_deep_exp = time.time() - Architope_deep_classifier_training_begin_deep_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Prediction(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "Architope_prediction_y_train_deep_exp = np.take_along_axis(predictions_train, predicted_classes_train[:,None], axis=1)\n",
    "# Testing Set\n",
    "Architope_prediction_y_test_deep_exp = np.take_along_axis(predictions_test, predicted_classes_test[:,None], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          train       test\n",
      "MAE    0.604607   0.598524\n",
      "MSE    0.706434   0.695422\n",
      "MAPE  33.865293  34.046186\n"
     ]
    }
   ],
   "source": [
    "# Compute Peformance\n",
    "performance_Architope_deep_exp = reporter(y_train_hat_in=Architope_prediction_y_train_deep_exp,\n",
    "                                    y_test_hat_in=Architope_prediction_y_test_deep_exp,\n",
    "                                    y_train_in=y_train,\n",
    "                                    y_test_in=y_test)\n",
    "# Write Performance\n",
    "performance_Architope_deep_exp.to_latex((results_tables_path+\"Architopes_deep_expert_performance.tex\"))\n",
    "\n",
    "# Update User\n",
    "print(performance_Architope_deep_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Required Training Time(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity/Efficiency Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      L-time  P-time  N_params_expt  AIC-like    Eff\n",
      "0  44.503067    -inf           3404  6809.027  4.868\n"
     ]
    }
   ],
   "source": [
    "# Compute Parameters for composite models #\n",
    "#-----------------------------------------#\n",
    "\n",
    "# Build AIC-like Metric #\n",
    "#-----------------------#\n",
    "AIC_like = 2*(N_params_Architope_deep_expt - np.log((performance_Architope_deep_exp['test']['MAE'])))\n",
    "AIC_like = np.round(AIC_like,3)\n",
    "Efficiency = np.log(N_params_Architope_deep_expt) *(performance_Architope_deep_exp['test']['MAE'])\n",
    "Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "# Build Table #\n",
    "#-------------#\n",
    "Architope_Model_Complexity_Deep_Expert = pd.DataFrame({'L-time': [Architope_partition_training_expt],\n",
    "                                                  'P-time':[Architope_partitioning_max_time_running_exp],\n",
    "                                                  'N_params_expt': [N_params_Architope_deep_expt],\n",
    "                                                  'AIC-like': [AIC_like],\n",
    "                                                  'Eff': [Efficiency]})\n",
    "\n",
    "\n",
    "# Write Required Training Time(s)\n",
    "Architope_Model_Complexity_Deep_Expert.to_latex((results_tables_path+\"Architope_DEEP_Expert_model_complexities.tex\"))\n",
    "\n",
    "#--------------======---------------#\n",
    "# Display Required Training Time(s) #\n",
    "#--------------======---------------#\n",
    "print(Architope_Model_Complexity_Deep_Expert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "#===============#\n",
      "# Model Summary #\n",
      "#===============#\n",
      " \n",
      "------------------------------------\n",
      "Model Performance: Expert Architope\n",
      "------------------------------------\n",
      "          train       test\n",
      "MAE    0.604607   0.598524\n",
      "MSE    0.706434   0.695422\n",
      "MAPE  33.865293  34.046186\n",
      " \n",
      "------------------------------------\n",
      "Model Complexity: Expert Architope\n",
      "------------------------------------\n",
      "      L-time  P-time  N_params_expt  AIC-like    Eff\n",
      "0  44.503067    -inf           3404  6809.027  4.868\n",
      " \n",
      " \n",
      "ðŸ˜ƒðŸ˜ƒ Have a wonderful day!! ðŸ˜ƒðŸ˜ƒ\n"
     ]
    }
   ],
   "source": [
    "print(' ')\n",
    "print('#===============#')\n",
    "print('# Model Summary #')\n",
    "print('#===============#')\n",
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('Model Performance: Expert Architope')\n",
    "print('------------------------------------')\n",
    "print(performance_Architope_deep_exp)\n",
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('Model Complexity: Expert Architope')\n",
    "print('------------------------------------')\n",
    "print(Architope_Model_Complexity_Deep_Expert)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('ðŸ˜ƒðŸ˜ƒ Have a wonderful day!! ðŸ˜ƒðŸ˜ƒ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
