{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Architope\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"Architopes\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 0.3\n",
    "min_height = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "exec(open('Prepare_Data_California_Housing.py').read())\n",
    "# Import time separately\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use $\\Delta_{in} = Q_{q}\\left(\\Delta(\\mathbb{X})\\right)$ where $\\Delta(\\mathbb{X})$ is the vector of (Euclidean) distances between the given data-points, $q \\in (0,1)$ is a hyper-parameter, and $Q$ is the empirical quantile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(Min_data_size_percentage,q_in, X_train_in,y_train_in, CV_folds_failsafe, min_size):\n",
    "    #-----------------------#\n",
    "    # Reset Seed Internally #\n",
    "    #-----------------------#\n",
    "    random.seed(2020)\n",
    "    np.random.seed(2020)\n",
    "\n",
    "    #-------------------------------------------#\n",
    "    #-------------------------------------------#\n",
    "    # 1) Sample radius from unifom distribution #\n",
    "    #-------------------------------------------#\n",
    "    #-------------------------------------------#\n",
    "    alpha = np.random.uniform(low=.25,high=.5,size=1)[0]\n",
    "\n",
    "    #-------------------------------------#\n",
    "    #-------------------------------------#\n",
    "    # 2) Apply Random Bijection (Shuffle) #\n",
    "    #-------------------------------------#\n",
    "    #-------------------------------------#\n",
    "    X_train_in_shuffled = X_train_in#.sample(frac=1)\n",
    "    y_train_in_shuffled = y_train_in#.sample(frac=1)\n",
    "\n",
    "    #--------------------#\n",
    "    #--------------------#\n",
    "    # X) Initializations #\n",
    "    #--------------------#\n",
    "    #--------------------#\n",
    "    # Compute-data-driven radius\n",
    "    Delta_X = distance_matrix(X_train_in_shuffled,X_train_in_shuffled)[::,0]\n",
    "    Delta_in = np.quantile(Delta_X,q_in)\n",
    "\n",
    "    # Initialize Random Radius\n",
    "    rand_radius = Delta_in*alpha\n",
    "\n",
    "    # Initialize Data_sizes & ratios\n",
    "    N_tot = X_train_in.shape[0] #<- Total number of data-points in input data-set!\n",
    "    N_radios = np.array([])\n",
    "    N_pool_train_loop = N_tot\n",
    "    # Initialize List of Dataframes\n",
    "    X_internal_train_list = list()\n",
    "    y_internal_train_list = list()\n",
    "\n",
    "    # Initialize Partioned Data-pool\n",
    "    X_internal_train_pool = X_train_in_shuffled\n",
    "    y_internal_train_pool = y_train_in_shuffled\n",
    "\n",
    "    # Initialize counter \n",
    "    part_current_loop = 0\n",
    "\n",
    "    #----------------------------#\n",
    "    #----------------------------#\n",
    "    # 3) Iteratively Build Parts #\n",
    "    #----------------------------#\n",
    "    #----------------------------#\n",
    "\n",
    "    while ((N_pool_train_loop/N_tot > Min_data_size_percentage) or (X_internal_train_pool.empty == False)):\n",
    "        # Extract Current Center\n",
    "        center_loop = X_internal_train_pool.iloc[0]\n",
    "        # Compute Distances\n",
    "        ## Training\n",
    "        distances_pool_loop_train = X_internal_train_pool.sub(center_loop)\n",
    "        distances_pool_loop_train = np.array(np.sqrt(np.square(distances_pool_loop_train).sum(axis=1)))\n",
    "        # Evaluate which Distances are less than the given random radius\n",
    "        Part_train_loop = X_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "        Part_train_loop_y = y_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "\n",
    "        # Remove all data-points which are \"too small\"\n",
    "        if X_internal_train_pool.shape[0] > max(CV_folds,4):\n",
    "            # Append Current part to list\n",
    "            X_internal_train_list.append(Part_train_loop)\n",
    "            y_internal_train_list.append(Part_train_loop_y)\n",
    "\n",
    "        # Remove current part from pool \n",
    "        X_internal_train_pool = X_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "        y_internal_train_pool = y_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "\n",
    "        # Update Current size of pool of training data\n",
    "        N_pool_train_loop = X_internal_train_pool.shape[0]\n",
    "        N_radios = np.append(N_radios,(N_pool_train_loop/N_tot))\n",
    "\n",
    "        # Update Counter\n",
    "        part_current_loop = part_current_loop +1\n",
    "        \n",
    "        # Update User\n",
    "        print((N_pool_train_loop/N_tot))\n",
    "\n",
    "\n",
    "    # Post processing #\n",
    "    #-----------------#\n",
    "    # Remove Empty Partitions\n",
    "    N_radios = N_radios[N_radios>0]\n",
    "    \n",
    "    \n",
    "    #-----------------------------------------------------------------#\n",
    "    # Combine parts which are too small to perform CV without an error\n",
    "    #-----------------------------------------------------------------#\n",
    "    # Initialize lists (partitions) with \"enough\" datums per part\n",
    "    X_internal_train_list_good = list()\n",
    "    y_internal_train_list_good = list()\n",
    "    # Initialize first list item test\n",
    "    is_first = True\n",
    "    # Initialize counter\n",
    "    goods_counter = 0\n",
    "    for search_i in range(len(X_internal_train_list)):\n",
    "        number_of_instances_in_part = len(X_internal_train_list[search_i]) \n",
    "        if number_of_instances_in_part < max(CV_folds_failsafe,min_size):\n",
    "            # Check if first \n",
    "            if is_first:\n",
    "                # Initialize set of small X_parts\n",
    "                X_small_parts = X_internal_train_list[search_i]\n",
    "                # Initialize set of small y_parts\n",
    "                y_small_parts = y_internal_train_list[search_i]\n",
    "\n",
    "                # Set is_first to false\n",
    "                is_first = False\n",
    "            else:\n",
    "                X_small_parts = X_small_parts.append(X_internal_train_list[search_i])\n",
    "                y_small_parts = np.append(y_small_parts,y_internal_train_list[search_i])\n",
    "        else:\n",
    "            # Append to current list\n",
    "            X_internal_train_list_good.append(X_internal_train_list[search_i])\n",
    "            y_internal_train_list_good.append(y_internal_train_list[search_i])\n",
    "            # Update goods counter \n",
    "            goods_counter = goods_counter +1\n",
    "\n",
    "    # Append final one to good list\n",
    "    X_internal_train_list_good.append(X_small_parts)\n",
    "    y_internal_train_list_good.append(y_small_parts)\n",
    "\n",
    "    # reset is_first to false (inscase we want to re-run this particular block)\n",
    "    is_first = True\n",
    "\n",
    "    # Set good lists to regular lists\n",
    "    X_internal_train_list = X_internal_train_list_good\n",
    "    y_internal_train_list = y_internal_train_list_good\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return Value #\n",
    "    #--------------#\n",
    "    return [X_internal_train_list, y_internal_train_list, N_radios]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Random Partitioner to the given Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "partitioning_time_begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8909883720930233\n",
      "0.7688261351052049\n",
      "0.33755537098560356\n",
      "0.04699612403100775\n",
      "0.04270487264673311\n",
      "0.021387043189368772\n",
      "0.010451273532668882\n",
      "0.009274640088593578\n",
      "0.007890365448504983\n",
      "0.006782945736434108\n",
      "0.004914174972314507\n",
      "0.004637320044296789\n",
      "0.004083610188261351\n",
      "0.0035299003322259138\n",
      "0.0025609080841638983\n",
      "0.0020764119601328905\n",
      "0.001937984496124031\n",
      "0.001522702104097453\n",
      "0.0011766334440753046\n",
      "0.0008997785160575858\n",
      "0.0007613510520487265\n",
      "0.0006921373200442968\n",
      "0.00048449612403100775\n",
      "0.0004152823920265781\n",
      "0.00020764119601328904\n",
      "0.00013842746400885935\n",
      "6.921373200442968e-05\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X_parts_list, y_parts_list, N_ratios = Random_Lipschitz_Partioner(Min_data_size_percentage=.5, \n",
    "                                                                  q_in=.8, \n",
    "                                                                  X_train_in=X_train, \n",
    "                                                                  y_train_in=y_train, \n",
    "                                                                  CV_folds_failsafe=CV_folds,\n",
    "                                                                  min_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioning_time = time.time() - partitioning_time_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_parts_listhe number of parts are: 5.\n"
     ]
    }
   ],
   "source": [
    "print('The_parts_listhe number of parts are: ' + str(len(X_parts_list))+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Training Predictions on each part\n",
    "- Train locally (on each \"naive part\")\n",
    "- Generate predictions for (full) training and testings sets respectively, to be used in training the classifer and for prediction, respectively.  \n",
    "- Generate predictions on all of testing-set (will be selected between later using classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapse (Start) for Training on Each Part\n",
    "Architope_partition_training_begin = time.time()\n",
    "# Initialize running max for Parallel time\n",
    "Architope_partitioning_max_time_running = -math.inf # Initialize slowest-time at - infinity to force updating!\n",
    "# Initialize N_parameter counter for Architope\n",
    "N_params_Architope = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Current part: 0 out of : 5 parts.\n",
      "Heights to iterate over: [90, 179, 357, 535, 713]\n",
      "Fitting 4 folds for each of 50 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=70)]: Using backend LokyBackend with 70 concurrent workers.\n",
      "[Parallel(n_jobs=70)]: Done  76 out of 140 | elapsed:  1.6min remaining:  1.3min\n",
      "[Parallel(n_jobs=70)]: Done  91 out of 140 | elapsed:  1.6min remaining:   51.6s\n",
      "[Parallel(n_jobs=70)]: Done 106 out of 140 | elapsed:  1.6min remaining:   30.7s\n",
      "[Parallel(n_jobs=70)]: Done 121 out of 140 | elapsed:  1.6min remaining:   15.0s\n",
      "[Parallel(n_jobs=70)]: Done 136 out of 140 | elapsed:  1.6min remaining:    2.8s\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/users/kratsioa/.local/lib/python3.7/site-packages/joblib/parallel.py\", line 909, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/scratch/users/kratsioa/.local/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 562, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/usr/lib64/python3.7/concurrent/futures/_base.py\", line 430, in result\n",
      "    self._condition.wait(timeout)\n",
      "  File \"/usr/lib64/python3.7/threading.py\", line 296, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-396c6d7cfc74>\", line 39, in <module>\n",
      "    X_test=X_test)\n",
      "  File \"<string>\", line 154, in build_ffNN\n",
      "  File \"/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 710, in fit\n",
      "    self._run_search(evaluate_candidates)\n",
      "  File \"/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 1484, in _run_search\n",
      "    random_state=self.random_state))\n",
      "  File \"/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 689, in evaluate_candidates\n",
      "    cv.split(X, y, groups)))\n",
      "  File \"/scratch/users/kratsioa/.local/lib/python3.7/site-packages/joblib/parallel.py\", line 1017, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/scratch/users/kratsioa/.local/lib/python3.7/site-packages/joblib/parallel.py\", line 931, in retrieve\n",
      "    backend.abort_everything(ensure_ready=ensure_ready)\n",
      "  File \"/scratch/users/kratsioa/.local/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 579, in abort_everything\n",
      "    self._workers.shutdown(kill_workers=True)\n",
      "  File \"/scratch/users/kratsioa/.local/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 1101, in shutdown\n",
      "    qmt.join()\n",
      "  File \"/usr/lib64/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib64/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib64/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib64/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib64/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib64/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib64/python3.7/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/usr/lib64/python3.7/posixpath.py\", line 385, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/usr/lib64/python3.7/posixpath.py\", line 366, in normpath\n",
      "    new_comps.append(comp)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3266\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3267\u001b[0;31m                     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3268\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-396c6d7cfc74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                                                                      \u001b[0mX_test_partial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                                                                      X_test=X_test)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mbuild_ffNN\u001b[0;34m(n_folds, n_jobs, n_iter, param_grid_in, X_train, y_train, X_test_partial, X_test)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    930\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \"\"\"\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                 \u001b[0mqmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2017\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2018\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2019\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3283\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3284\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3286\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2019\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2021\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1379\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1282\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m             )\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m             \u001b[0mformatted_exceptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m             \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "for current_part in range(len(X_parts_list)):\n",
    "    #==============#\n",
    "    # Timer(begin) #\n",
    "    #==============#\n",
    "    current_part_training_time_for_parallel_begin = time.time()\n",
    "    \n",
    "    \n",
    "    # Initializations #\n",
    "    #-----------------#\n",
    "    # Reload Grid\n",
    "    exec(open('Grid_Enhanced_Network.py').read())\n",
    "    # Modify heights according to optimal (data-driven) rule (with threshold)\n",
    "    current_height = np.ceil(np.array(param_grid_Vanilla_Nets['height'])*N_ratios[current_part])\n",
    "    current_height_threshold = np.repeat(min_height,(current_height.shape[0]))\n",
    "    current_height = np.maximum(current_height,current_height_threshold)\n",
    "    current_height = current_height.astype(int).tolist()\n",
    "    param_grid_Vanilla_Nets['height'] = current_height\n",
    "    # Automatically Fix Input Dimension\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]\n",
    "    param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    \n",
    "    # Update User #\n",
    "    #-------------#\n",
    "    print('Status: Current part: ' + str(current_part) + ' out of : '+str(len(X_parts_list)) +' parts.')\n",
    "    print('Heights to iterate over: '+str(current_height))\n",
    "    \n",
    "    # Generate Prediction(s) on current Part #\n",
    "    #----------------------------------------#\n",
    "    # Failsafe (number of data-points)\n",
    "    CV_folds_failsafe = min(CV_folds,max(1,(X_train.shape[0]-1)))\n",
    "    # Train Network\n",
    "    y_hat_train_full_loop, y_hat_test_full_loop, N_params_Architope_loop = build_ffNN(n_folds = CV_folds_failsafe, \n",
    "                                                                                     n_jobs = n_jobs,\n",
    "                                                                                     n_iter = n_iter, \n",
    "                                                                                     param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                     X_train= X_parts_list[current_part], \n",
    "                                                                                     y_train=y_parts_list[current_part],\n",
    "                                                                                     X_test_partial=X_train,\n",
    "                                                                                     X_test=X_test)\n",
    "    \n",
    "    # Append predictions to data-frames\n",
    "    ## If first prediction we initialize data-frames\n",
    "    if current_part==0:\n",
    "        # Register quality\n",
    "        training_quality = np.array(np.abs(y_hat_train_full_loop-y_train))\n",
    "        training_quality = training_quality.reshape(training_quality.shape[0],1)\n",
    "\n",
    "        # Save Predictions\n",
    "        predictions_train = y_hat_train_full_loop\n",
    "        predictions_train = predictions_train.reshape(predictions_train.shape[0],1)\n",
    "        predictions_test = y_hat_test_full_loop\n",
    "        predictions_test = predictions_test.reshape(predictions_test.shape[0],1)\n",
    "        \n",
    "        \n",
    "    ## If not first prediction we append to already initialized dataframes\n",
    "    else:\n",
    "    # Register Best Scores\n",
    "        #----------------------#\n",
    "        # Write Predictions \n",
    "        # Save Predictions\n",
    "        y_hat_train_loop = y_hat_train_full_loop.reshape(predictions_train.shape[0],1)\n",
    "        predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "        y_hat_test_loop = y_hat_test_full_loop.reshape(predictions_test.shape[0],1)\n",
    "        predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "        \n",
    "        # Evaluate Errors #\n",
    "        #-----------------#\n",
    "        # Training\n",
    "        prediction_errors = np.abs(y_hat_train_loop.reshape(-1,)-y_train)\n",
    "        training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "        \n",
    "    #============#\n",
    "    # Timer(end) #\n",
    "    #============#\n",
    "    current_part_training_time_for_parallel = time.time() - current_part_training_time_for_parallel_begin\n",
    "    Architope_partitioning_max_time_running = max(Architope_partitioning_max_time_running,current_part_training_time_for_parallel)\n",
    "\n",
    "    #============---===============#\n",
    "    # N_parameter Counter (Update) #\n",
    "    #------------===---------------#\n",
    "    N_params_Architope = N_params_Architope + N_params_Architope_loop\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('----------------------------------------------------')\n",
    "print('Feature Generation (Learning Phase): Score Generated')\n",
    "print('----------------------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training on Each Part\n",
    "Architope_partition_training = time.time() - Architope_partition_training_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Classifier\n",
    "Prepare Labels/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training Deep Classifier\n",
    "Architope_deep_classifier_training_begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Classes Labels\n",
    "partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "# Build Classes\n",
    "for part_column_i in range(1,(training_quality.shape[1])):\n",
    "    partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                           (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                          ],axis=1)\n",
    "# Convert to integers\n",
    "partition_labels_training = partition_labels_training+0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-Load Grid and Redefine Relevant Input/Output dimensions in dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Re-Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "\n",
    "# Redefine (Dimension-related) Elements of Grid\n",
    "param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "param_grid_Deep_Classifier['output_dim'] = [partition_labels_training.shape[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train simple deep classifier\n",
    "predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter =n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train, \n",
    "                                                                                                        y_train = partition_labels_training,\n",
    "                                                                                                        X_test = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training Deep Classifier\n",
    "Architope_deep_classifier_training = time.time() - Architope_deep_classifier_training_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Prediction(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "Architope_prediction_y_train = np.take_along_axis(predictions_train, predicted_classes_train[:,None], axis=1)\n",
    "# Testing Set\n",
    "Architope_prediction_y_test = np.take_along_axis(predictions_test, predicted_classes_test[:,None], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Peformance\n",
    "performance_Architope = reporter(y_train_hat_in=Architope_prediction_y_train,\n",
    "                                    y_test_hat_in=Architope_prediction_y_test,\n",
    "                                    y_train_in=y_train,\n",
    "                                    y_test_in=y_test)\n",
    "# Write Performance\n",
    "performance_Architope.to_latex((results_tables_path+\"Architopes_full_performance.tex\"))\n",
    "\n",
    "# Update User\n",
    "print(performance_Architope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity/Efficiency Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Parameters for composite models #\n",
    "#-----------------------------------------#\n",
    "N_params_Architope_full = N_params_Architope + N_params_deep_classifier\n",
    "\n",
    "# Build AIC-like Metric #\n",
    "#-----------------------#\n",
    "AIC_like = 2*(N_params_Architope_full - np.log((performance_Architope['test']['MAE'])))\n",
    "AIC_like = np.round(AIC_like,3)\n",
    "Efficiency = np.log(N_params_Architope_full) *(performance_Architope['test']['MAE'])\n",
    "Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "# Build Table #\n",
    "#-------------#\n",
    "Architope_Model_Complexity_full = pd.DataFrame({'L-time': [Architope_partition_training],\n",
    "                                                  'P-time':[Architope_partitioning_max_time_running],\n",
    "                                                  'N_params_expt': [N_params_Architope_full],\n",
    "                                                  'AIC-like': [AIC_like],\n",
    "                                                  'Eff': [Efficiency]})\n",
    "\n",
    "\n",
    "# Write Required Training Time(s)\n",
    "Architope_Model_Complexity_full.to_latex((results_tables_path+\"Architope_full_model_complexities.tex\"))\n",
    "\n",
    "#--------------======---------------#\n",
    "# Display Required Training Time(s) #\n",
    "#--------------======---------------#\n",
    "print(Architope_Model_Complexity_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' ')\n",
    "print('#===============#')\n",
    "print('# Model Summary #')\n",
    "print('#===============#')\n",
    "print(' ')\n",
    "print('---------------------------------------------')\n",
    "print('Model Performance: Semi-Supervised Architope')\n",
    "print('---------------------------------------------')\n",
    "print(performance_Architope)\n",
    "print(' ')\n",
    "print('--------------------------------------------')\n",
    "print('Model Complexity: Semi-Supervised Architope')\n",
    "print('--------------------------------------------')\n",
    "print(Architope_Model_Complexity_full)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('😃😃 Have a wonderful day!! 😃😃')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
