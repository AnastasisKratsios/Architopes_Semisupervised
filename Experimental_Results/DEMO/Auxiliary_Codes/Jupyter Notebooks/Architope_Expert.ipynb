{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Architope\n",
    "---\n",
    "\n",
    "**What this code does:** *This Code Implements the Expert-provided partition version of the architope.*\n",
    "\n",
    "**Why is it separate?**  *It can be run in parallel with the other codes since all seeds and states are the same.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 0.3\n",
    "min_height = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "exec(open('Prepare_Data_California_Housing.py').read())\n",
    "# Import time separately\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Times and Other Trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapse (Start) for Training on Each Part\n",
    "Architope_partition_training_begin_expt = time.time()\n",
    "# Initialize running max for Parallel time\n",
    "Architope_partitioning_max_time_running_exp = -math.inf # Initialize slowest-time at - infinity to force updating!\n",
    "# Initialize N_parameter counter for Architope\n",
    "N_params_Architope_expt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Dataset using Expert Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Column Names #\n",
    "#--------------------#\n",
    "# Read from X\n",
    "X_col_names = (X.drop(['median_house_value'],axis=1).columns).values\n",
    "# Write to X_train and X_test\n",
    "X_train.columns= X_col_names\n",
    "X_test.columns= X_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Lists\n",
    "#------------------#\n",
    "## Initialize Data Lists\n",
    "X_train_list_expt = list()\n",
    "y_train_list_expt = list()\n",
    "X_test_list_expt = list()\n",
    "y_test_list_expt = list()\n",
    "\n",
    "## Initialize Predictions Lists\n",
    "y_hat_train_list_expt = list()\n",
    "y_hat_test_list_expt = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parts (expert opinion) and append to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------#\n",
    "# 1H Ocean #\n",
    "#----------#\n",
    "\n",
    "# Extract Train\n",
    "X_train_list_expt.append(X_train[X_train['ocean_proximity_<1H OCEAN']==1])\n",
    "y_train_list_expt.append(y_train[X_train['ocean_proximity_<1H OCEAN']==1])\n",
    "# Extract Test\n",
    "X_test_list_expt.append(X_test[X_test['ocean_proximity_<1H OCEAN']==1])\n",
    "y_test_list_expt.append(y_test[X_test['ocean_proximity_<1H OCEAN']==1])\n",
    "\n",
    "#--------#\n",
    "# INLAND #\n",
    "#--------#\n",
    "\n",
    "# Extract Train\n",
    "X_train_list_expt.append(X_train[X_train['ocean_proximity_INLAND']==1])\n",
    "y_train_list_expt.append(y_train[X_train['ocean_proximity_INLAND']==1])\n",
    "# Extract Test\n",
    "X_test_list_expt.append(X_test[X_test['ocean_proximity_INLAND']==1])\n",
    "y_test_list_expt.append(y_test[X_test['ocean_proximity_INLAND']==1])\n",
    "\n",
    "\n",
    "\n",
    "#----------#\n",
    "# NEAR BAY #\n",
    "#----------#\n",
    "\n",
    "# Extract Train\n",
    "X_train_list_expt.append(X_train[X_train['ocean_proximity_NEAR BAY']==1])\n",
    "y_train_list_expt.append(y_train[X_train['ocean_proximity_NEAR BAY']==1])\n",
    "# Extract Test\n",
    "X_test_list_expt.append(X_test[X_test['ocean_proximity_NEAR BAY']==1])\n",
    "y_test_list_expt.append(y_test[X_test['ocean_proximity_NEAR BAY']==1])\n",
    "\n",
    "\n",
    "#------------------------#\n",
    "# NEAR OCEAN & on ISLAND #\n",
    "#------------------------#\n",
    "\n",
    "# Extract Train\n",
    "X_train_list_expt.append(X_train[np.logical_or(X_train['ocean_proximity_ISLAND']==1,(X_train['ocean_proximity_NEAR OCEAN']==1))])\n",
    "y_train_list_expt.append(y_train[np.logical_or(X_train['ocean_proximity_ISLAND']==1,(X_train['ocean_proximity_NEAR OCEAN']==1))])\n",
    "# Extract Test\n",
    "X_test_list_expt.append(X_test[np.logical_or(X_test['ocean_proximity_ISLAND']==1,(X_test['ocean_proximity_NEAR OCEAN']==1))])\n",
    "y_test_list_expt.append(y_test[np.logical_or(X_test['ocean_proximity_ISLAND']==1,(X_test['ocean_proximity_NEAR OCEAN']==1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ratios\n",
    "N_ratios = np.array([])\n",
    "\n",
    "# Build Ratios\n",
    "for Number_part_i in range(len(X_train_list_expt)):\n",
    "    # Update Ratios\n",
    "    N_ratios = np.append(N_ratios,((X_train_list_expt[Number_part_i].shape[0])/X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Export Architope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = np.array([])\n",
    "y_hat_test = np.array([])\n",
    "y_train_target_reordered = np.array([])\n",
    "y_test_target_reordered = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Current part: 0 out of : 4 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   12.6s remaining:   12.6s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   13.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   13.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "805/805 [==============================] - 1s 1ms/step - loss: 0.6414 - mse: 0.8202 - mae: 0.6414 - mape: 27.8690\n",
      "Epoch 2/2\n",
      "805/805 [==============================] - 1s 973us/step - loss: 0.5929 - mse: 0.7562 - mae: 0.5929 - mape: 25.4486\n",
      "805/805 [==============================] - 0s 613us/step\n",
      "338/338 [==============================] - 0s 645us/step\n",
      "Status: Current part: 1 out of : 4 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    5.9s remaining:    5.9s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    6.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.3860 - mse: 0.3722 - mae: 0.3860 - mape: 31.9335\n",
      "Epoch 2/2\n",
      "572/572 [==============================] - 1s 1ms/step - loss: 0.3772 - mse: 0.3967 - mae: 0.3772 - mape: 30.3616\n",
      "572/572 [==============================] - 0s 695us/step\n",
      "248/248 [==============================] - 0s 719us/step\n",
      "Status: Current part: 2 out of : 4 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    3.1s remaining:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    3.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "198/198 [==============================] - 0s 2ms/step - loss: 0.8844 - mse: 1.3975 - mae: 0.8844 - mape: 39.3112\n",
      "Epoch 2/2\n",
      "198/198 [==============================] - 0s 2ms/step - loss: 0.7028 - mse: 0.8947 - mae: 0.7028 - mape: 31.4144\n",
      "198/198 [==============================] - 0s 1ms/step\n",
      "89/89 [==============================] - 0s 755us/step\n",
      "Status: Current part: 3 out of : 4 parts.\n",
      "Heights to iterate over: [50]\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    3.6s remaining:    3.6s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    3.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8383 - mse: 1.2915 - mae: 0.8383 - mape: 39.2837\n",
      "Epoch 2/2\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7239 - mse: 0.9419 - mae: 0.7239 - mape: 33.7746\n",
      "233/233 [==============================] - 0s 870us/step\n",
      "101/101 [==============================] - 0s 930us/step\n",
      " \n",
      " \n",
      " \n",
      "----------------------------------------------------\n",
      "Feature Generation (Learning Phase): Score Generated\n",
      "----------------------------------------------------\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "for current_part in range(len(X_train_list_expt)):\n",
    "    #==============#\n",
    "    # Timer(begin) #\n",
    "    #==============#\n",
    "    current_part_training_time_for_parallel_begin = time.time()\n",
    "    \n",
    "    \n",
    "    # Initializations #\n",
    "    #-----------------#\n",
    "    # Reload Grid\n",
    "    exec(open('Grid_Enhanced_Network.py').read())\n",
    "    # Modify heights according to optimal (data-driven) rule (with threshold)\n",
    "    current_height = np.ceil(np.array(param_grid_Vanilla_Nets['height'])*N_ratios[current_part])\n",
    "    current_height_threshold = np.repeat(min_height,(current_height.shape[0]))\n",
    "    current_height = np.maximum(current_height,current_height_threshold)\n",
    "    current_height = current_height.astype(int).tolist()\n",
    "    param_grid_Vanilla_Nets['height'] = current_height\n",
    "    # Automatically Fix Input Dimension\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]\n",
    "    param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    \n",
    "    # Update User #\n",
    "    #-------------#\n",
    "    print('Status: Current part: ' + str(current_part) + ' out of : '+str(len(X_train_list_expt)) +' parts.')\n",
    "    print('Heights to iterate over: '+str(current_height))\n",
    "    \n",
    "    # Generate Prediction(s) on current Part #\n",
    "    #----------------------------------------#\n",
    "    # Failsafe (number of data-points)\n",
    "    CV_folds_failsafe = min(CV_folds,max(1,(X_train.shape[0]-1)))\n",
    "    # Train Network\n",
    "    y_hat_train_full_loop, y_hat_test_full_loop, N_params_Architope_loop = build_ffNN(n_folds = CV_folds_failsafe, \n",
    "                                                                                     n_jobs = n_jobs,\n",
    "                                                                                     n_iter = n_iter, \n",
    "                                                                                     param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                     X_train= X_train_list_expt[current_part], \n",
    "                                                                                     y_train=y_train_list_expt[current_part],\n",
    "                                                                                     X_test_partial=X_train_list_expt[current_part],\n",
    "                                                                                     X_test=X_test_list_expt[current_part])\n",
    "    # Update Predictions and Ordering of Targets\n",
    "    y_hat_train = np.append(y_hat_train,y_hat_train_full_loop)\n",
    "    y_train_target_reordered = np.append(y_train_target_reordered,(y_train_list_expt[current_part]))\n",
    "    y_hat_test = np.append(y_hat_test,y_hat_test_full_loop)\n",
    "    y_test_target_reordered = np.append(y_test_target_reordered,(y_test_list_expt[current_part]))\n",
    "\n",
    "        \n",
    "    #============#\n",
    "    # Timer(end) #\n",
    "    #============#\n",
    "    current_part_training_time_for_parallel = time.time() - current_part_training_time_for_parallel_begin\n",
    "    Architope_partitioning_max_time_running_exp = max(Architope_partitioning_max_time_running_exp,current_part_training_time_for_parallel)\n",
    "\n",
    "    #============---===============#\n",
    "    # N_parameter Counter (Update) #\n",
    "    #------------===---------------#\n",
    "    N_params_Architope_expt = N_params_Architope_expt + N_params_Architope_loop\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('----------------------------------------------------')\n",
    "print('Feature Generation (Learning Phase): Score Generated')\n",
    "print('----------------------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training on Each Part\n",
    "Architope_partition_training_expt = time.time() - Architope_partition_training_begin_expt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          train       test\n",
      "MAE    0.516594   0.500537\n",
      "MSE    0.593178   0.513258\n",
      "MAPE  29.976897  26.479500\n"
     ]
    }
   ],
   "source": [
    "# Compute Peformance\n",
    "performance_Architope_exp = reporter(y_train_hat_in=y_hat_train,\n",
    "                                    y_test_hat_in=y_hat_test,\n",
    "                                    y_train_in=y_train_target_reordered,\n",
    "                                    y_test_in=y_test_target_reordered)\n",
    "# Write Performance\n",
    "performance_Architope_exp.to_latex((results_tables_path+\"Architopes_expert_performance.tex\"))\n",
    "\n",
    "# Update User\n",
    "print(performance_Architope_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Required Training Time(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity/Efficiency Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      L-time     P-time  N_params_expt  AIC-like    Eff\n",
      "0  40.228501  16.548173           3404  6809.384  4.071\n"
     ]
    }
   ],
   "source": [
    "# Compute Parameters for composite models #\n",
    "#-----------------------------------------#\n",
    "\n",
    "# Build AIC-like Metric #\n",
    "#-----------------------#\n",
    "AIC_like = 2*(N_params_Architope_expt - np.log((performance_Architope_exp['test']['MAE'])))\n",
    "AIC_like = np.round(AIC_like,3)\n",
    "Efficiency = np.log(N_params_Architope_expt) *(performance_Architope_exp['test']['MAE'])\n",
    "Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "# Build Table #\n",
    "#-------------#\n",
    "Architope_Model_Complexity_Expert = pd.DataFrame({'L-time': [Architope_partition_training_expt],\n",
    "                                                  'P-time':[Architope_partitioning_max_time_running_exp],\n",
    "                                                  'N_params_expt': [N_params_Architope_expt],\n",
    "                                                  'AIC-like': [AIC_like],\n",
    "                                                  'Eff': [Efficiency]})\n",
    "\n",
    "\n",
    "# Write Required Training Time(s)\n",
    "Architope_Model_Complexity_Expert.to_latex((results_tables_path+\"ArchitopeExpert_model_complexities.tex\"))\n",
    "\n",
    "#--------------======---------------#\n",
    "# Display Required Training Time(s) #\n",
    "#--------------======---------------#\n",
    "print(Architope_Model_Complexity_Expert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "#===============#\n",
      "# Model Summary #\n",
      "#===============#\n",
      " \n",
      "------------------------------------\n",
      "Model Performance: Expert Architope\n",
      "------------------------------------\n",
      "          train       test\n",
      "MAE    0.516594   0.500537\n",
      "MSE    0.593178   0.513258\n",
      "MAPE  29.976897  26.479500\n",
      " \n",
      "------------------------------------\n",
      "Model Complexity: Expert Architope\n",
      "------------------------------------\n",
      "      L-time     P-time  N_params_expt  AIC-like    Eff\n",
      "0  40.228501  16.548173           3404  6809.384  4.071\n",
      " \n",
      " \n",
      "😃😃 Have a wonderful day!! 😃😃\n"
     ]
    }
   ],
   "source": [
    "print(' ')\n",
    "print('#===============#')\n",
    "print('# Model Summary #')\n",
    "print('#===============#')\n",
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('Model Performance: Expert Architope')\n",
    "print('------------------------------------')\n",
    "print(performance_Architope_exp)\n",
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('Model Complexity: Expert Architope')\n",
    "print('------------------------------------')\n",
    "print(Architope_Model_Complexity_Expert)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('😃😃 Have a wonderful day!! 😃😃')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
