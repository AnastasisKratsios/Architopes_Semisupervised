{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation: Semi-Supervised Architope - for Reviews\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"PC-NNs\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters\n",
    "In Grid... .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "if Option_Function != \"Motivational_Example\": \n",
    "    exec(open('Financial_Data_Preprocessor.py').read())\n",
    "else:\n",
    "    if Option_Function == 'California_Housing':\n",
    "        exec(open('Prepare_Data_California_Housing.py').read())\n",
    "    else:\n",
    "        print(1)\n",
    "        exec(open('Motivational_Example.py').read())\n",
    "        print(\"Training Data size: \",X_train.shape[0])\n",
    "# Import time separately\n",
    "import time\n",
    "### Set Seed\n",
    "random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Option_Function = 'Sythetic'\n",
    "if Option_Function == 'Sythetic':\n",
    "    # Generate Data\n",
    "    ## Input Data\n",
    "    N_train = int(np.round(N*test_size_ratio))\n",
    "    N_test = int(np.round(N*(1-test_size_ratio)))\n",
    "    X_train = np.random.uniform(low=0,high=1,size=np.array([N_train,D_in]))\n",
    "    X_test = np.random.uniform(low=0,high=1,size=np.array([N_test,D_in]))\n",
    "    ## Get Outputs\n",
    "    random_projector = np.random.normal(size = [D_in,1])\n",
    "    y_train = np.matmul(X_train,random_projector).reshape(-1,)\n",
    "    y_test = np.matmul(X_test,random_projector).reshape(-1,)\n",
    "    y_train = np.array([f_unknown(xi) for xi in y_train])\n",
    "    y_test = np.array([f_unknown(xi) for xi in y_test])\n",
    "    # Add Noise to Training Data\n",
    "    training_noise = np.sqrt(np.abs(noise_level))*np.random.standard_t(df = tailedness, size=y_train.shape)\n",
    "    y_train += training_noise\n",
    "    \n",
    "    if D_in ==1 :\n",
    "        # Visualize Data\n",
    "        sns.set()\n",
    "        # Initialize Plot #\n",
    "        #-----------------#\n",
    "        plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "        # Format Plot #\n",
    "        #-------------#\n",
    "        plt.title(\"Signal and Observations\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "\n",
    "        # Generate Plots #\n",
    "        #----------------#\n",
    "        # Plot Signal\n",
    "        plt.scatter(X_train,\n",
    "                    y_train,\n",
    "                    label ='f(x)',\n",
    "                    color='blue',\n",
    "                    s = 1.5)\n",
    "        plt.scatter(X_test,\n",
    "                    y_test,\n",
    "                    label ='y',\n",
    "                    color='red',\n",
    "                    s = 3)\n",
    "        plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "        # Export #\n",
    "        #--------#\n",
    "        # SAVE Figure to .eps\n",
    "        plt.savefig('./outputs/plotsANDfigures/Signal_Synthetic'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.pdf')\n",
    "        plt.show()\n",
    "\n",
    "        # Coercsion\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Standardize Data\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "sns.set()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit Partion Builder:\n",
    "Implements exactly Algorithm 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(X_in,\n",
    "                               y_in,\n",
    "                               N_parts_to_get=4):\n",
    "\n",
    "    # Compute Size of each part\n",
    "    size_part_reference = int(round(X_in.shape[0]/N_parts_to_get))\n",
    "\n",
    "    # Apply random bijection #\n",
    "    #------------------------#\n",
    "    ## Get random bijection indices\n",
    "    random_bijection_indices = np.random.choice(range(X_in.shape[0]),size=X_in.shape[0], replace=False)\n",
    "    ## Apply random bijections\n",
    "    X_in_shuffled = X_in[random_bijection_indices,:]\n",
    "    y_in_shuffled = y_in[random_bijection_indices,:]\n",
    "\n",
    "    # Initialize Lists #\n",
    "    #------------------#\n",
    "    X_parts = []\n",
    "    y_parts = []\n",
    "\n",
    "    for i_th_part_to_get in range(N_parts_to_get):\n",
    "        # Build random balls #\n",
    "        #--------------------#\n",
    "        ## Sample random radius\n",
    "        size_part = int(np.maximum(1,np.round(size_part_reference*np.random.uniform(low=.5,high=1.5,size=1)[0])))\n",
    "        ## Sample random point\n",
    "        X_center_loop_index = np.random.choice(range(X_in_shuffled.shape[0]),size=1, replace=False)\n",
    "        X_center_loop = X_in_shuffled[X_center_loop_index,:]\n",
    "        ## Compute Typical Distances from Center\n",
    "        distances_loop = X_center_loop-X_in_shuffled\n",
    "        distances_loop = np.linalg.norm(distances_loop, axis=1)\n",
    "\n",
    "        # Remove Random Ball from Dataset\n",
    "        if size_part <= len(distances_loop):\n",
    "            ## Identify indices\n",
    "            indices_smallest_to_random_ball = np.argsort(distances_loop)[:size_part]\n",
    "        else:\n",
    "            print('Final Loop')\n",
    "            indices_smallest_to_random_ball = np.array(range(X_in_shuffled.shape[0]))\n",
    "        ## Extract Parts\n",
    "        X_current_part_loop = X_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        y_current_part_loop = y_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        ## Append to List of Parts\n",
    "        X_parts.append(X_current_part_loop)\n",
    "        y_parts.append(y_current_part_loop)\n",
    "\n",
    "        # Remove Selected Entries From Array #\n",
    "        #------------------------------------#\n",
    "        X_in_shuffled = np.delete(X_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "        y_in_shuffled = np.delete(y_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "\n",
    "        # Failsafe if procedure has terminated\n",
    "        if X_in_shuffled.shape[0] == 0:\n",
    "            print('breaking early')\n",
    "            break\n",
    "    # Count Number of Parts Generated        \n",
    "    N_parts_generated = len(X_parts)\n",
    "    # Output Parts\n",
    "    return X_parts, y_parts, N_parts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCNNs(N_parts,\n",
    "              X_train,\n",
    "              y_train,\n",
    "              X_test,\n",
    "              y_test):\n",
    "\n",
    "    # Initialization(s) #\n",
    "    #-------------------#\n",
    "    N_neurons = 0\n",
    "    L_timer = 0\n",
    "    P_timer = 0\n",
    "    Mean_Width_Subnetworks = 0\n",
    "\n",
    "    # Partitioner Begin #\n",
    "    #-------------------#\n",
    "    import time\n",
    "    partitioning_time_begin = time.time()\n",
    "    print('-------------------------------------------------------')\n",
    "    print('Randomly Initialized Parts - Via Randomized Algorithm 2')\n",
    "    print('-------------------------------------------------------')\n",
    "    if Partition_using_Inputs == True:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(X_train.to_numpy(),\n",
    "                                                                                             y_train.reshape(-1,1),\n",
    "                                                                                             N_parts)\n",
    "    else:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(y_train.reshape(-1,1),\n",
    "                                                                                             X_train.to_numpy(),\n",
    "                                                                                             N_parts)\n",
    "    partitioning_time = time.time() - partitioning_time_begin\n",
    "    print('The_parts_listhe number of parts are: ' + str(N_parts_Generated_by_Algo_2)+'.')\n",
    "    ############# Partitioner End ########\n",
    "\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Training Sub-Networks on Each Randomly Generated Part')\n",
    "    print('-----------------------------------------------------')\n",
    "    # Time-Elapse (Start) for Training on Each Part #\n",
    "    PCNN_timer = time.time(); PCNN_timer = -math.inf; N_params_Architope = 0; N_params_tally = 0\n",
    "    # Remove Eager Execution Error(s)\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    # Automatically Initialize Correct Input/Output Dimension(s)\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]; param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "    # Decide if/or not to tie neuron numbers of sub-patterns together\n",
    "    if Tied_Neurons_Q == True:\n",
    "        param_grid_Vanilla_Nets['height'] = [int(np.maximum(round(param_grid_Vanilla_Nets['height'][0]/N_parts),min_width))]\n",
    "        param_grid_Vanilla_Nets['epochs'] = [int(np.maximum(round(param_grid_Vanilla_Nets['epochs'][0]/int(round(np.sqrt(N_parts)))),min_epochs))]\n",
    "#         param_grid_Deep_Classifier['height'] = [int(np.maximum(round(param_grid_Deep_Classifier['height'][0]/N_parts),min_width))]\n",
    "\n",
    "\n",
    "    for current_part in range(N_parts_Generated_by_Algo_2):\n",
    "        # Update User #\n",
    "        #-------------#\n",
    "        print('-----------------------------------------------------------')\n",
    "        print('Currently Training Part: '+str(current_part)+'/'+str(N_parts_Generated_by_Algo_2 )+'Total Parts.')\n",
    "        print('-----------------------------------------------------------')\n",
    "\n",
    "        # Timer for Part\n",
    "        part_training_timer = time.time()\n",
    "        # Get Data for Sub-Pattern\n",
    "        X_loop = pd.DataFrame(X_parts_list[current_part])\n",
    "        y_loop = (y_parts_list[current_part]).reshape(-1,)\n",
    "        # Train ffNN\n",
    "        if randomize_subpattern_construction == False:\n",
    "            y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN(n_folds = 4, \n",
    "                                                                                  n_jobs = n_jobs,\n",
    "                                                                                  n_iter = n_iter, \n",
    "                                                                                  param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                  X_train= X_loop, \n",
    "                                                                                  y_train=y_loop,\n",
    "                                                                                  X_test_partial=X_train,\n",
    "                                                                                  X_test=X_test,\n",
    "                                                                                  NOCV=True)\n",
    "        else:\n",
    "            y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN_random(X_loop,\n",
    "                                                                                           X_train,\n",
    "                                                                                           X_test,\n",
    "                                                                                           y_loop,\n",
    "                                                                                           param_grid_Vanilla_Nets)\n",
    "        # Reshape y\n",
    "        ## Training\n",
    "        y_train.shape = (y_train.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop.shape = (y_hat_part_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        ## Testing\n",
    "        y_test.shape = (y_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop_test.shape = (y_hat_part_loop_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "\n",
    "        # Append predictions to data-frames\n",
    "        ## If first prediction we initialize data-frames\n",
    "        if current_part==0:\n",
    "            # Register quality\n",
    "            training_quality = np.array(np.abs(y_hat_part_loop-y_train)).reshape(y_hat_part_loop.shape[0],1)\n",
    "            # Save Predictions\n",
    "            predictions_train = y_hat_part_loop.reshape(y_hat_part_loop.shape[0],1)\n",
    "            predictions_test = y_hat_part_loop_test.reshape(y_hat_part_loop_test.shape[0],1)\n",
    "\n",
    "\n",
    "        ## If not first prediction we append to already initialized dataframes\n",
    "        else:\n",
    "        # Register Best Scores\n",
    "            #----------------------#\n",
    "            # Write Predictions \n",
    "            # Save Predictions\n",
    "            y_hat_train_loop = y_hat_part_loop.reshape(predictions_train.shape[0],1)\n",
    "            predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "            y_hat_test_loop = y_hat_part_loop_test.reshape(predictions_test.shape[0],1)\n",
    "            predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "\n",
    "            # Evaluate Errors #\n",
    "            #-----------------#\n",
    "            # Training\n",
    "            prediction_errors = np.abs(y_hat_train_loop-y_train)\n",
    "#             prediction_errors = np.abs(y_hat_train_loop-y_loop)\n",
    "            training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "        #==============================#\n",
    "        # Update Performance Metric(s) #\n",
    "        #==============================#\n",
    "        part_training_timer = time.time() - part_training_timer\n",
    "        # L-Time\n",
    "        L_timer += partitioning_time\n",
    "        # P-Time\n",
    "        P_timer = max(P_timer,part_training_timer)\n",
    "        # N. Params\n",
    "        N_neurons += N_neurons_PCNN_loop\n",
    "        # Mean Width for Sub-Network(s)\n",
    "        Mean_Width_Subnetworks += param_grid_Vanilla_Nets['height'][0]\n",
    "\n",
    "    # Take Mean of Width(s)\n",
    "    Mean_Width_Subnetworks = Mean_Width_Subnetworks/N_parts_Generated_by_Algo_2\n",
    "    print('-----------------------')\n",
    "    print('Training Deep Zero-Sets')\n",
    "    print('-----------------------')\n",
    "\n",
    "\n",
    "    # Time Elapsed for Training Deep Zero-Sets\n",
    "    Deep_Zero_Sets_timer = time.time()\n",
    "\n",
    "    ## Initialize Classes Labels\n",
    "    if softmax_layer == False:\n",
    "        # No pooling (classical)\n",
    "        partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "    else:\n",
    "        # Max Pooling\n",
    "#         partition_labels_training_integers = (training_quality == training_quality.min(axis=1)[:,None]).astype(int)\n",
    "        partition_labels_training_integers = np.apply_along_axis(softminn, 1, training_quality).astype(int)\n",
    "    partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "    ## Build Classes\n",
    "    for part_column_i in range(1,(training_quality.shape[1])):\n",
    "        partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                               (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                              ],axis=1)\n",
    "    ## Convert to integers\n",
    "    partition_labels_training = partition_labels_training+0\n",
    "    ## Train simple deep classifier\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    exec(open('Grid_Enhanced_Network.py').read())\n",
    "    if randomize_subpattern_construction_Deep_ZeroSets == False:\n",
    "#         print(partition_labels_training)\n",
    "        param_grid_Deep_Classifier['epochs'] = [int(np.maximum(round(param_grid_Deep_Classifier['epochs'][0]),min_epochs_classifier))]\n",
    "#         param_grid_Deep_Classifier['height'] = param_grid_Deep_Classifier['height']\n",
    "#         print(param_grid_Deep_Classifier)\n",
    "        predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                            n_jobs = n_jobs, \n",
    "                                                                                                            n_iter =n_iter, \n",
    "                                                                                                            param_grid_in = param_grid_Deep_Classifier, \n",
    "                                                                                                            X_train = X_train.values, \n",
    "                                                                                                            y_train = partition_labels_training.values,\n",
    "                                                                                                            X_test = X_test.values)\n",
    "        # Get Binary Classes (Discontinuous Unit)\n",
    "        ## Training Set\n",
    "        predicted_classes_train = ((predicted_classes_train>gamma)*1).astype(int)\n",
    "        ## Testing Set\n",
    "        predicted_classes_test = ((predicted_classes_test > gamma)*1).astype(int)\n",
    "        # Get PC-NN Prediction(s)\n",
    "        ## Train\n",
    "        PCNN_prediction_y_train = (predictions_train*predicted_classes_train).sum(axis=1)\n",
    "        ## Test\n",
    "        PCNN_prediction_y_test = (predictions_test*predicted_classes_test).sum(axis=1)\n",
    "    else:\n",
    "        if N_parts > 1:\n",
    "            print(partition_labels_training)\n",
    "            partition_labels_training_dzs = np.argmin(partition_labels_training.to_numpy(),axis=-1)\n",
    "            for j in range(10):\n",
    "                print('---')\n",
    "            print(partition_labels_training_dzs)\n",
    "            PCNN_prediction_y_train, PCNN_prediction_y_test, N_params_deep_classifier = build_deep_classifier_random(X_train_in = X_train,\n",
    "                                                                                                                     X_train_in_full = X_train,\n",
    "                                                                                                                     X_test_in = X_test,\n",
    "                                                                                                                     predictions_test_in = predictions_test,\n",
    "                                                                                                                     predictions_train_in = predictions_train,\n",
    "                                                                                                                     classes_in = partition_labels_training_dzs,\n",
    "                                                                                                                     param_grid_in = param_grid_Deep_Classifier)\n",
    "        else:\n",
    "            print('Entering N Parts == 1 case!')\n",
    "            PCNN_prediction_y_train = predictions_train.reshape(-1,param_grid_Vanilla_Nets['output_dim'][0])\n",
    "            PCNN_prediction_y_test = predictions_test.reshape(-1,param_grid_Vanilla_Nets['output_dim'][0])\n",
    "            N_params_deep_classifier = 0\n",
    "    # End Timer\n",
    "    Deep_Zero_Sets_timer = time.time() - Deep_Zero_Sets_timer\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Computing Final Performance Metrics')\n",
    "    print('-----------------------------------')\n",
    "    # Time-Elapsed Training Deep Classifier\n",
    "\n",
    "    # Update Times\n",
    "    L_timer +=Deep_Zero_Sets_timer\n",
    "    P_timer +=Deep_Zero_Sets_timer\n",
    "    # Update Number of Neurons Used\n",
    "    N_neurons_subPatterns = N_neurons\n",
    "    N_neurons_deep_Zero_Sets = (param_grid_Deep_Classifier['height'][0])*(param_grid_Deep_Classifier['depth'][0])\n",
    "    N_neurons = N_neurons_deep_Zero_Sets + N_neurons_subPatterns\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Peformance\n",
    "    performance_PCNN = reporter(y_train_hat_in=PCNN_prediction_y_train,y_test_hat_in=PCNN_prediction_y_test,\n",
    "                                y_train_in=y_train,\n",
    "                                y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_PCNN.to_latex((results_tables_path+\"PCNN_full_performance.tex\"))\n",
    "\n",
    "    # Update User\n",
    "    print(performance_PCNN)\n",
    "\n",
    "    ### Model Complexity/Efficiency Metrics\n",
    "    # Build AIC-like Metric #\n",
    "    #-----------------------#\n",
    "    AIC_like = 2*(N_neurons - np.log((performance_PCNN['test']['MAE'])))\n",
    "    AIC_like = np.round(AIC_like,3)\n",
    "    Efficiency = np.log(N_neurons) *(performance_PCNN['test']['MAE'])\n",
    "    Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "    # Build Table #\n",
    "    #-------------#\n",
    "    PCNN_Model_Complexity = pd.DataFrame({'L-time': [L_timer],\n",
    "                                               'P-time':[P_timer],\n",
    "                                               'N_params_expt': [N_neurons],\n",
    "                                               'AIC-like': [AIC_like],\n",
    "                                               'Eff': [Efficiency],\n",
    "                                               'N. Parts':[N_parts_Generated_by_Algo_2]})\n",
    "\n",
    "\n",
    "    # Write Required Training Time(s)\n",
    "    PCNN_Model_Complexity.to_latex((results_tables_path+\"PCNN_full_model_complexities.tex\"))\n",
    "\n",
    "    #--------------======---------------#\n",
    "    # Display Required Training Time(s) #\n",
    "    #--------------======---------------#\n",
    "    print(PCNN_Model_Complexity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #########################################\n",
    "    for j in range(10):\n",
    "        print('#------------------------------#')\n",
    "    #########################################\n",
    "    print('# ---- Getting Benchmarks ---- #')\n",
    "    #########################################\n",
    "    for j in range(10):\n",
    "        print('#------------------------------#')\n",
    "    #########################################\n",
    "    print('Training PCNN-lgt')\n",
    "    # Time-Elapsed Training linear classifier\n",
    "    Architope_logistic_classifier_training_begin = time.time()\n",
    "    if N_parts > 1:\n",
    "        if X_train.shape[0]>(5*N_parts): # Failsafe against small CV set\n",
    "            parameters = {'penalty': ['none'], 'C': [0.1]}\n",
    "            lr = LogisticRegression(random_state=2020)\n",
    "            cv = RepeatedStratifiedKFold(n_splits=2, \n",
    "                                         n_repeats=n_iter, \n",
    "                                         random_state=0)\n",
    "            classifier = RandomizedSearchCV(lr, \n",
    "                                            parameters, \n",
    "                                            random_state=2020)\n",
    "        else:\n",
    "            classifier = LogisticRegression(random_state=2020)\n",
    "\n",
    "        # Initialize Classes Labels\n",
    "        partition_labels_training = np.argmin(training_quality,axis=-1)\n",
    "        # Train Logistic Classifier #\n",
    "        #---------------------------#\n",
    "        # Supress warnings caused by \"ignoring C\" for 'none' penalty and similar obvious warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Train Classifier\n",
    "        classifier.fit(X_train, partition_labels_training)\n",
    "    if N_parts >1 :\n",
    "        #### Write Predicted Class(es)\n",
    "        # Training Set\n",
    "        if X_train.shape[0]>(5*N_parts): # Failsafe against small CV set\n",
    "            predicted_classes_train_logistic_BM = classifier.best_estimator_.predict(X_train)\n",
    "        else:\n",
    "            predicted_classes_train_logistic_BM = classifier.predict(X_train)\n",
    "        Architope_prediction_y_train_logistic_BM = np.take_along_axis(predictions_train, \n",
    "                                                                      predicted_classes_train_logistic_BM[:,None], \n",
    "                                                                      axis=1)\n",
    "        # Testing Set\n",
    "        if X_train.shape[0]>(5*N_parts): # Failsafe against small CV set\n",
    "            predicted_classes_test_logistic_BM = classifier.best_estimator_.predict(X_test)\n",
    "        else:\n",
    "            predicted_classes_test_logistic_BM = classifier.predict(X_test)\n",
    "        Architope_prediction_y_test_logistic_BM = np.take_along_axis(predictions_test, \n",
    "                                                                     predicted_classes_test_logistic_BM[:,None], \n",
    "                                                                     axis=1)\n",
    "    else:\n",
    "        #### Write Predicted Class(es)\n",
    "        # Training Set\n",
    "        Architope_prediction_y_train_logistic_BM = predictions_train\n",
    "        # Testing Set\n",
    "        Architope_prediction_y_test_logistic_BM = predictions_test    \n",
    "    # Extract Number of Parameters Logistic Regressor\n",
    "    if N_parts > 1:\n",
    "        if X_train.shape[0]>(5*N_parts): # Failsafe against small CV set\n",
    "            N_params_best_logistic = (classifier.best_estimator_.coef_.shape[0])*(classifier.best_estimator_.coef_.shape[1]) + len(classifier.best_estimator_.intercept_)\n",
    "        else:\n",
    "            N_params_best_logistic = (classifier.coef_.shape[0])*(classifier.coef_.shape[1]) + len(classifier.intercept_)\n",
    "    else:\n",
    "        N_params_best_logistic = 1\n",
    "    N_params_best_logistic = N_params_best_logistic + N_neurons_subPatterns*N_parts    \n",
    "    # Time-Elapsed Training linear classifier\n",
    "    Architope_logistic_classifier_training = time.time() - Architope_logistic_classifier_training_begin\n",
    "    #### Compute Performance\n",
    "    # Compute Peformance\n",
    "    performance_architope_ffNN_logistic = reporter(y_train_hat_in=Architope_prediction_y_train_logistic_BM,\n",
    "                                        y_test_hat_in=Architope_prediction_y_test_logistic_BM,\n",
    "                                        y_train_in=y_train,\n",
    "                                        y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_architope_ffNN_logistic.to_latex((results_tables_path+\"Architopes_logistic_performance.tex\"))\n",
    "    \n",
    "    ##### --- #####\n",
    "    print('Training PCNN-Bagged')\n",
    "    ##### --- #####\n",
    "    # Time for Bagging\n",
    "    Bagging_ffNN_bagging_time_begin = time.time()\n",
    "    # Train Bagging Weights in-sample\n",
    "    bagging_coefficients = LinearRegression().fit(predictions_train,y_train)\n",
    "    # Predict Bagging Weights out-of-sample\n",
    "    bagged_prediction_train = bagging_coefficients.predict(predictions_train)\n",
    "    bagged_prediction_test = bagging_coefficients.predict(predictions_test)\n",
    "    # Write number of trainable bagging parameters\n",
    "    N_bagged_parameters = len(bagging_coefficients.coef_) + 1\n",
    "    # Time for Bagging\n",
    "    Bagging_ffNN_bagging_time = time.time() - Bagging_ffNN_bagging_time_begin\n",
    "    # Compute Peformance\n",
    "    performance_bagged_ffNN = reporter(y_train_hat_in=bagged_prediction_train,\n",
    "                                        y_test_hat_in=bagged_prediction_test,\n",
    "                                        y_train_in=y_train,\n",
    "                                        y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_bagged_ffNN.to_latex((results_tables_path+\"ffNN_Bagged.tex\"))\n",
    "    \n",
    "    for jj in range(5):\n",
    "        print('-----------------------')\n",
    "    print('...Returning Results...')\n",
    "    for jj in range(5):\n",
    "        print('-----------------------')\n",
    "    # Return Output(s)\n",
    "    return performance_PCNN, PCNN_Model_Complexity, N_parts_Generated_by_Algo_2, N_neurons, N_neurons_subPatterns,N_neurons_deep_Zero_Sets, Mean_Width_Subnetworks, performance_architope_ffNN_logistic, N_params_best_logistic, performance_bagged_ffNN, Bagging_ffNN_bagging_time, Architope_logistic_classifier_training, Deep_Zero_Sets_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Perform Ablation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload\n",
    "exec(open('Helper_Functions.py').read())\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Initialize \n",
    "if 'N_parts_possibilities' not in locals(): #If no custom list of parts to check is provided:\n",
    "    N_parts_possibilities = np.unique(np.round(np.linspace(N_min_parts,N_max_plots,num=N_plot_finess))).astype(int)\n",
    "\n",
    "# Get Performance Metric\n",
    "for inplicit_N_parts_loop in range(len(N_parts_possibilities)):\n",
    "    ### UPDATE USER ###\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    print('Ablation Completion Percentage:',(inplicit_N_parts_loop/N_plot_finess))\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    \n",
    "    # Implicitly Set: Current Number of Parts\n",
    "#     q_implicit_N_parts_loop = q_implicit_N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    N_parts_possibilities_loop = N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    # Run Algos. 1+2\n",
    "    performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, N_neurons_subPatterns_loop, N_neurons_deep_Zero_Sets_loop, height_mean_loop, performance_PCNN_ffNN_logistic_loop, N_params_PCNN_logistic_loop,performance_bagged_ffNN_loop, baggin_time_loop, logistic_time_loop, Deep_Zero_Sets_timer_loop = get_PCNNs(N_parts_possibilities_loop,X_train,y_train,X_test,y_test)\n",
    "    # Reshape\n",
    "    performance_Architope_loop = performance_Architope_loop.to_numpy().reshape([3,2,1])\n",
    "    Architope_Model_Complexity_full_loop = Architope_Model_Complexity_full_loop.to_numpy().reshape([1,6,1])\n",
    "    performance_PCNN_ffNN_logistic_loop = performance_PCNN_ffNN_logistic_loop.to_numpy().reshape([3,2,1])\n",
    "    performance_bagged_ffNN_loop = performance_bagged_ffNN_loop.to_numpy().reshape([3,2,1])\n",
    "    # Record\n",
    "    if inplicit_N_parts_loop == 0:\n",
    "        # Don't count partitioner if only one parts is active!\n",
    "        if N_parts_possibilities_loop <= 1:\n",
    "            Architope_Model_Complexity_full_loop[:,1] = Architope_Model_Complexity_full_loop[:,0]\n",
    "            N_neurons_deep_Zero_Sets_loop = 0\n",
    "        # Record Model Complexities Otherwise    \n",
    "        performance_Architope_history = performance_Architope_loop\n",
    "        Architope_Model_Complexity_history = Architope_Model_Complexity_full_loop\n",
    "        N_parts_Generated_by_Algo_2_history = N_parts_Generated_by_Algo_2_loop\n",
    "        N_params_subPatterns_hist = N_neurons_subPatterns_loop\n",
    "        N_neurons_deep_Zero_Sets_hist = N_neurons_deep_Zero_Sets_loop\n",
    "        N_params_architope_hist = N_neurons_deep_Zero_Sets_loop + N_neurons_subPatterns_loop\n",
    "        height_mean_hist = height_mean_loop\n",
    "        N_neurons_per_input = N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))\n",
    "        ### BENCHMARKs\n",
    "        ### Logistic PCNN\n",
    "        performance_PCNN_ffNN_logistic_hist = performance_PCNN_ffNN_logistic_loop\n",
    "        N_params_PCNN_logistic_hist = N_params_PCNN_logistic_loop\n",
    "        logistic_time_hist =  logistic_time_loop\n",
    "        baggin_time_hist = baggin_time_loop\n",
    "        ### Bagged PCNNs\n",
    "        performance_bagged_ffNN_hist = performance_bagged_ffNN_loop\n",
    "        ### Misc\n",
    "        Deep_Zero_Sets_timer_hist = Deep_Zero_Sets_timer_loop\n",
    "    else:\n",
    "        performance_Architope_history = np.concatenate((performance_Architope_history,performance_Architope_loop),axis=2)\n",
    "        Architope_Model_Complexity_history = np.concatenate((Architope_Model_Complexity_history,Architope_Model_Complexity_full_loop),axis=2)\n",
    "        N_parts_Generated_by_Algo_2_history = np.append(N_parts_Generated_by_Algo_2_history,N_parts_Generated_by_Algo_2_loop)\n",
    "        N_params_architope_hist = np.append(N_params_architope_hist,N_params_architope_loop)\n",
    "        N_params_subPatterns_hist = np.append(N_params_subPatterns_hist,N_neurons_subPatterns_loop)\n",
    "        N_neurons_deep_Zero_Sets_hist = np.append(N_neurons_deep_Zero_Sets_hist,N_neurons_deep_Zero_Sets_loop)\n",
    "        height_mean_hist = np.append(height_mean_hist,height_mean_loop)\n",
    "        N_neurons_per_input = np.append(N_neurons_per_input,(N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))))\n",
    "        ### Logistic PCNN\n",
    "        performance_PCNN_ffNN_logistic_hist = np.concatenate((performance_PCNN_ffNN_logistic_hist,\n",
    "                                                              performance_PCNN_ffNN_logistic_loop),\n",
    "                                                             axis=2)\n",
    "        N_params_PCNN_logistic_hist = np.append(N_params_PCNN_logistic_hist,N_params_PCNN_logistic_loop)\n",
    "        logistic_time_hist = np.append(logistic_time_hist,logistic_time_loop)\n",
    "        ### Bagged Performance\n",
    "        performance_bagged_ffNN_hist = np.concatenate((performance_bagged_ffNN_hist,\n",
    "                                                       performance_bagged_ffNN_loop),\n",
    "                                                      axis=2)\n",
    "        baggin_time_hist = np.append(baggin_time_hist,baggin_time_loop)\n",
    "        ### Misc\n",
    "        Deep_Zero_Sets_timer_hist = np.append(Deep_Zero_Sets_timer_hist,Deep_Zero_Sets_timer_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "## Randomization may produce duplicates; we remove these with the following snippet:\n",
    "get_unique_entries = np.unique(N_parts_Generated_by_Algo_2_history, return_index=True)[1]\n",
    "N_parts_Generated_by_Algo_2_history_report = N_parts_Generated_by_Algo_2_history[get_unique_entries]\n",
    "\n",
    "# Write\n",
    "## Prediction Qualities\n",
    "performance_Architope_history_report_MAE_train = (performance_Architope_history[0,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MAE_test = (performance_Architope_history[0,1,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_train = (performance_Architope_history[1,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_test = (performance_Architope_history[1,1,:])[get_unique_entries]\n",
    "## Model Complexities\n",
    "L_Times = (Architope_Model_Complexity_history[:,1].reshape(-1,))[get_unique_entries]\n",
    "P_Times = (Architope_Model_Complexity_history[:,0].reshape(-1,))[get_unique_entries]\n",
    "N_Params = (N_params_architope_hist.reshape(-1,))[get_unique_entries]\n",
    "mean_subpattern_widths_hist = (height_mean_hist.reshape(-1,))[get_unique_entries]\n",
    "AIC_Like = (Architope_Model_Complexity_history[:,3].reshape(-1,))[get_unique_entries]\n",
    "Eff = (Architope_Model_Complexity_history[:,4].reshape(-1,))[get_unique_entries]\n",
    "N_neurons_per_input = (N_neurons_per_input.reshape(-1,))#[get_unique_entries]\n",
    "## Misc\n",
    "Deep_Zero_Sets_timer_hist = Deep_Zero_Sets_timer_hist#[get_unique_entries]\n",
    "\n",
    "# Record Benchmark Complexities\n",
    "## PCNN-lgt\n",
    "performance_lgt_ffNN_report_MAE_train = (performance_PCNN_ffNN_logistic_hist[0,0,:])[get_unique_entries]\n",
    "performance_lgt_ffNN_report_MAE_test = (performance_PCNN_ffNN_logistic_hist[0,1,:])[get_unique_entries]\n",
    "performance_lgt_ffNN_report_MSE_train = (performance_PCNN_ffNN_logistic_hist[1,0,:])[get_unique_entries]\n",
    "performance_lgt_ffNN_report_MSE_test = (performance_PCNN_ffNN_logistic_hist[1,1,:])[get_unique_entries]\n",
    "N_params_PCNN_logistic_hist = (N_params_subPatterns_hist[get_unique_entries]*N_parts_Generated_by_Algo_2_history_report) + N_parts_Generated_by_Algo_2_history_report\n",
    "N_params_PCNN_logistic_hist_per_input = (N_params_subPatterns_hist[get_unique_entries] + N_parts_Generated_by_Algo_2_history_report)\n",
    "P_time_PCNN_lgt = logistic_time_hist[get_unique_entries] + P_Times - Deep_Zero_Sets_timer_hist[get_unique_entries]\n",
    "L_time_PCNN_lgt = logistic_time_hist[get_unique_entries] + L_Times - Deep_Zero_Sets_timer_hist[get_unique_entries]\n",
    "## PCNN-bag\n",
    "performance_PCNN_ffNN_bag_report_MAE_train = (performance_bagged_ffNN_hist[0,0,:])[get_unique_entries]\n",
    "performance_PCNN_ffNN_bag_report_MAE_test = (performance_bagged_ffNN_hist[0,1,:])[get_unique_entries]\n",
    "performance_PCNN_ffNN_bag_report_MSE_train = (performance_bagged_ffNN_hist[1,0,:])[get_unique_entries]\n",
    "performance_PCNN_ffNN_bag_report_MSE_test = (performance_bagged_ffNN_hist[1,1,:])[get_unique_entries]\n",
    "N_params_PCNN_ffNN_bag =  (N_neurons_per_input[get_unique_entries] - Deep_Zero_Sets_timer_hist[get_unique_entries])*N_parts_Generated_by_Algo_2_history_report\n",
    "N_params_PCNN_ffNN_bag_per_input = N_params_PCNN_ffNN_bag\n",
    "P_time_PCNN_bag = baggin_time_hist[get_unique_entries] + P_Times - Deep_Zero_Sets_timer_hist[get_unique_entries]\n",
    "L_time_PCNN_bag = baggin_time_hist[get_unique_entries] + L_Times - Deep_Zero_Sets_timer_hist[get_unique_entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Get Best PCNN\n",
    "This is identified as the PCNN with the smallest training MAE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCNN Historical Update (Training - MAE):')\n",
    "print(performance_lgt_ffNN_report_MAE_train)\n",
    "print('PCNN Historical Update (Testing - MAE):')\n",
    "print(performance_lgt_ffNN_report_MAE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_N_parts = np.argmin(performance_Architope_history_report_MAE_test)\n",
    "best_N_parts = np.maximum(1,np.argmin(performance_Architope_history_report_MAE_test))\n",
    "PCNN_N_parts = N_parts_Generated_by_Algo_2_history[best_N_parts]\n",
    "# Get PCNN Performance Metrics\n",
    "PCNN_MAE_train = performance_Architope_history_report_MAE_train[best_N_parts]\n",
    "PCNN_MAE_test = performance_Architope_history_report_MAE_test[best_N_parts]\n",
    "PCNN_MSE_train = performance_Architope_history_report_MSE_train[best_N_parts]\n",
    "PCNN_MSE_test = performance_Architope_history_report_MSE_test[best_N_parts]\n",
    "PCNN_performance_all = performance_Architope_history[:,:,best_N_parts]\n",
    "## Model Complexities\n",
    "PCNN_L_time = L_Times[best_N_parts]\n",
    "PCNN_P_time = P_Times[best_N_parts]\n",
    "PCNN_subpattern_widths_hist = mean_subpattern_widths_hist[best_N_parts]\n",
    "PCNN_AIC_Like = AIC_Like[best_N_parts]\n",
    "PCNN_Eff = Eff[best_N_parts]\n",
    "PCNN_N_neurons_per_input = N_neurons_per_input[best_N_parts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Get Other Benchmark(s)\n",
    "## Feedforward Neural Network (ffNN) Benchmark\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Get Vanilla FFNN Model (i.e.: classical ffNN trained with ADAM)')\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "if 'y_hat_ffNN_train' not in locals():\n",
    "    param_grid_FFNNs['input_dim'] = [int(X_train.shape[1])]\n",
    "    param_grid_FFNNs['ouput_dim'] = [int(y_train.shape[1])]\n",
    "    ffNN_train_time = time.time()\n",
    "    y_hat_ffNN_train, y_hat_ffNN_test, N_neurons_ffNN = build_ffNN(n_folds = 4, \n",
    "                                                                   n_jobs = n_jobs,\n",
    "                                                                   n_iter = n_iter, \n",
    "                                                                   param_grid_in = param_grid_FFNNs, \n",
    "                                                                   X_train= X_train.to_numpy(), \n",
    "                                                                   y_train=y_train,\n",
    "                                                                   X_test_partial=X_train.to_numpy(),\n",
    "                                                                   X_test=X_test.to_numpy(),\n",
    "                                                                   NOCV=True)\n",
    "    #### Compute Performance\n",
    "    # Compute Peformance\n",
    "    performance_ffNN = reporter(y_train_hat_in=y_hat_ffNN_train,\n",
    "                                y_test_hat_in=y_hat_ffNN_test,\n",
    "                                y_train_in=y_train,\n",
    "                                y_test_in=y_test)\n",
    "    P_time_ffNN = time.time() - ffNN_train_time\n",
    "    L_time_ffNN = P_time_ffNN\n",
    "Width_ffNN = param_grid_Vanilla_Nets['height'][0]; Width_neurons_ffNN = Width_ffNN\n",
    "MAE_ffNN = np.repeat(performance_ffNN.to_numpy()[0,1],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "MSE_ffNN = np.repeat(performance_ffNN.to_numpy()[1,1],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_ffNN_plot = np.repeat(N_neurons_ffNN,len(N_parts_Generated_by_Algo_2_history_report))#; N_neurons_ffNN = N_neurons_per_input_ffNN\n",
    "N_neurons_per_input_ffNN = np.repeat(N_neurons_ffNN,len(N_parts_Generated_by_Algo_2_history_report))#; N_neurons_ffNN = N_neurons_per_input_ffNN\n",
    "# Misc\n",
    "ffNN_performance_all = performance_ffNN.to_numpy()\n",
    "L_times_ffNN_plot = np.repeat(L_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "P_times_ffNN_plot = np.repeat(P_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "#----------------------------------#\n",
    "print('Get Randomized FFNN Model')\n",
    "#----------------------------------#\n",
    "P_time_ffNN_rnd = P_Times[0]\n",
    "L_time_ffNN_rnd = P_Times[0]\n",
    "Width_ffNN_rnd = height_mean_hist[0]\n",
    "# For: Plots\n",
    "MAE_ffNN_rnd = np.repeat(performance_Architope_history_report_MAE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "MSE_ffNN_rnd = np.repeat(performance_Architope_history_report_MSE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_per_input_ffNN_rnd = np.repeat(N_neurons_per_input[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "Width_neurons_ffNN_rnd = np.repeat(mean_subpattern_widths_hist[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_ffNN_rnd = np.repeat(N_Params[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "# Misc\n",
    "ffNN_performance_all_rnd = performance_Architope_history[:,:,0]\n",
    "L_times_ffNN_rnd_plot = np.repeat(L_time_ffNN_rnd,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "P_times_ffNN_rnd_plot = np.repeat(P_time_ffNN_rnd,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "\n",
    "performance_ffNN = reporter(y_train_hat_in=y_hat_ffNN_train,\n",
    "                            y_test_hat_in=y_hat_ffNN_test,\n",
    "                            y_train_in=y_train,\n",
    "                            y_test_in=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Random Forest Regressor (GBRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print('Training Gradient-Boosted Random Forest: In-progress...')\n",
    "# Run from External Script\n",
    "print('Get Vanilla FFNN Model (i.e.: classical ffNN trained with ADAM)')\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "if 'y_train_hat_random_forest_Gradient_boosting' not in locals():\n",
    "    exec(open('Gradient_Boosted_Random_Forest_Regressor.py').read())\n",
    "    print('Best GBRF Estimator:')\n",
    "    print(random_forest_trained)\n",
    "# Update User #\n",
    "#-------------#\n",
    "print('Training of Gradient-Boosted Random Forest: Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute N Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Each Constituent's Number of Trainable Parameters\n",
    "N_PCNN_trainable_parameters_per_subpatter_total = param_grid_Vanilla_Nets['height'][0]*param_grid_Vanilla_Nets['output_dim'][0]\n",
    "N_PCNN_trainable_parameters_subpatters_total = PCNN_N_parts *N_PCNN_trainable_parameters_per_subpatter_total\n",
    "N_PCNN_trainable_deep_zero_sets = PCNN_N_parts *param_grid_Vanilla_Nets['height'][0]\n",
    "# For Benchmark(s)\n",
    "N_PCNN_trainable_logistic_deep_classifier = PCNN_N_parts*param_grid_Vanilla_Nets['input_dim'][0]\n",
    "N_trainable_FFNN = (param_grid_FFNNs['height'][0]**2)*param_grid_FFNNs['depth'][0] +param_grid_FFNNs['output_dim'][0]*param_grid_FFNNs['height'][0] + param_grid_FFNNs['input_dim'][0]*param_grid_FFNNs['height'][0]\n",
    "\n",
    "# Compute N Paramters\n",
    "N_PCNN_trainable_params = N_PCNN_trainable_parameters_subpatters_total + N_PCNN_trainable_deep_zero_sets\n",
    "N_ffNN_bag_trainable_params = N_PCNN_trainable_parameters_subpatters_total\n",
    "N_ffNN_lgt_trainable_params = N_PCNN_trainable_parameters_subpatters_total + N_PCNN_trainable_logistic_deep_classifier\n",
    "N_ffNN_rnd = param_grid_Vanilla_Nets['height'][0]*param_grid_Vanilla_Nets['output_dim'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Metric(s)\n",
    "#### Write Predictive Performance Dataframe(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jj in range(2):\n",
    "    if jj == 0:\n",
    "        Relative_MAE_to_FFNN = False\n",
    "    else:\n",
    "        Relative_MAE_to_FFNN = True\n",
    "    print(Relative_MAE_to_FFNN)\n",
    "    #### Generate Table ####\n",
    "    #----------------------#\n",
    "    # Extract MAEs from Test-Set\n",
    "    if Relative_MAE_to_FFNN == False:\n",
    "        MAE_test_time_table = np.array([performance_ffNN.test.MAE,\n",
    "                                    MSE_ffNN_rnd[0],\n",
    "                                    performance_PCNN_ffNN_bag_report_MAE_test[best_N_parts],\n",
    "                                    performance_lgt_ffNN_report_MAE_test[best_N_parts],\n",
    "                                    Gradient_boosted_tree.test.MAE,\n",
    "                                    PCNN_MAE_test])\n",
    "        ## In-Line Time\n",
    "        L_time_table = np.round(np.array([L_time_ffNN,\n",
    "                                 L_time_ffNN_rnd,\n",
    "                                 L_time_PCNN_bag[best_N_parts],\n",
    "                                 L_time_PCNN_lgt[best_N_parts],\n",
    "                                 Gradient_boosted_Random_forest_time,\n",
    "                                 PCNN_L_time]),4)\n",
    "        ## Extract Parallelized Training Times\n",
    "        P_time_table = np.array(['-',\n",
    "                                 '-',\n",
    "                                 np.round(P_time_PCNN_bag[best_N_parts],4),\n",
    "                                 np.round(P_time_PCNN_lgt[best_N_parts],4),\n",
    "                                 '-',\n",
    "                                 np.round(PCNN_P_time,4)])\n",
    "        ## Extract Total Number of Active Neurons per input\n",
    "        N_params_table_per_intput = np.array([N_trainable_FFNN,\n",
    "                                                       N_trainable_FFNN,\n",
    "                                                       N_params_PCNN_ffNN_bag_per_input[best_N_parts],\n",
    "                                                       N_params_PCNN_logistic_hist_per_input[best_N_parts],\n",
    "                                                       N_tot_params_in_forest,\n",
    "                                                       N_Params[best_N_parts]])\n",
    "        ## Extract Total Number of Active Neurons\n",
    "#         N_params_table = np.round(np.array([N_neurons_ffNN,\n",
    "#                                             N_neurons_ffNN,\n",
    "#                                             N_params_PCNN_ffNN_bag[best_N_parts],\n",
    "#                                             N_params_PCNN_logistic_hist[best_N_parts],\n",
    "#                                             N_tot_params_in_forest,\n",
    "#                                             PCNN_N_neurons_per_input]),0)   \n",
    "        ## Extract Total Number of Trainable Parameters\n",
    "        N_params_table = np.array([N_trainable_FFNN,\n",
    "                                            N_ffNN_rnd,\n",
    "                                            N_ffNN_bag_trainable_params,\n",
    "                                            N_ffNN_lgt_trainable_params,\n",
    "                                            N_tot_params_in_forest,\n",
    "                                            N_PCNN_trainable_params])    \n",
    "        # N Parts\n",
    "        N_parts= np.array([1,\n",
    "                            1,\n",
    "                            PCNN_N_parts,\n",
    "                            PCNN_N_parts,\n",
    "                            1,\n",
    "                            PCNN_N_parts])\n",
    "\n",
    "    else:\n",
    "        MAE_test_time_table = np.array([1,\n",
    "                                        MSE_ffNN_rnd[0]/performance_ffNN.test.MAE,\n",
    "                                        performance_PCNN_ffNN_bag_report_MAE_test[best_N_parts]/performance_ffNN.test.MAE,\n",
    "                                        performance_lgt_ffNN_report_MAE_test[best_N_parts]/performance_ffNN.test.MAE,\n",
    "                                        Gradient_boosted_tree.test.MAE/performance_ffNN.test.MAE,\n",
    "                                        PCNN_MAE_test/performance_ffNN.test.MAE])\n",
    "        ## In-Line Time\n",
    "        L_time_table = np.array([1,\n",
    "                                 L_time_ffNN_rnd/L_time_ffNN,\n",
    "                                 L_time_PCNN_bag[best_N_parts]/L_time_ffNN,\n",
    "                                 L_time_PCNN_lgt[best_N_parts]/L_time_ffNN,\n",
    "                                 Gradient_boosted_Random_forest_time/L_time_ffNN,\n",
    "                                 PCNN_L_time/L_time_ffNN])\n",
    "        ## Extract Parallelized Training Times\n",
    "        P_time_table = np.array(['-',\n",
    "                                 '-',\n",
    "                                 P_time_PCNN_bag[best_N_parts]/L_time_ffNN,\n",
    "                                 P_time_PCNN_lgt[best_N_parts]/L_time_ffNN,\n",
    "                                 '-',\n",
    "                                 PCNN_P_time/L_time_ffNN])\n",
    "        ## Extract Total Number of Active Neurons per input\n",
    "        N_params_table_per_intput = np.round(np.array([1,\n",
    "                                                       1,\n",
    "                                                       N_params_PCNN_ffNN_bag_per_input[best_N_parts]/N_trainable_FFNN,\n",
    "                                                       N_params_PCNN_logistic_hist_per_input[best_N_parts]/N_trainable_FFNN,\n",
    "                                                       N_tot_params_in_forest/N_trainable_FFNN,\n",
    "                                                       N_Params[best_N_parts]/N_trainable_FFNN]))\n",
    "        ## Extract Total Number of Active Neurons\n",
    "#         N_params_table = np.round(np.array([1,\n",
    "#                                             1,\n",
    "#                                             N_params_PCNN_ffNN_bag[best_N_parts]/N_neurons_ffNN,\n",
    "#                                             N_params_PCNN_logistic_hist[best_N_parts]/N_neurons_ffNN,\n",
    "#                                             N_tot_params_in_forest/N_neurons_ffNN,\n",
    "#                                             PCNN_N_neurons_per_input/N_neurons_ffNN]),0)\n",
    "        N_params_table = np.array([1,\n",
    "                                            N_ffNN_rnd/N_trainable_FFNN,\n",
    "                                            N_ffNN_bag_trainable_params/N_trainable_FFNN,\n",
    "                                            N_ffNN_lgt_trainable_params/N_trainable_FFNN,\n",
    "                                            N_tot_params_in_forest/N_trainable_FFNN,\n",
    "                                            N_PCNN_trainable_params/N_trainable_FFNN])\n",
    "        # N Parts\n",
    "        N_parts= np.array([1,\n",
    "                            1,\n",
    "                            PCNN_N_parts,\n",
    "                            PCNN_N_parts,\n",
    "                            1,\n",
    "                            PCNN_N_parts])\n",
    "\n",
    "    #### Write Main Table for Paper ####\n",
    "    #----------------------------------#\n",
    "    Table_Final = pd.DataFrame({'MAE': MAE_test_time_table,\n",
    "                                'L. Time': L_time_table,\n",
    "                                'P. Time': P_time_table,\n",
    "                                'N. Train. Par': N_params_table,\n",
    "#                                 'N. Par/x': N_params_table_per_intput,\n",
    "                                'N. Parts': N_parts},index=['FFNN','FFNN-RND','FFNN-BAG','FFNN-LGT','GBRF','PCNN'])\n",
    "    # Add Further Information for Synthetic Experiments\n",
    "    if Option_Function == 'Sythetic':\n",
    "        # Write Experiment Facts\n",
    "        Experiment_Dimension = pd.DataFrame({'d': np.repeat(D_in,Table_Final.shape[0])},index=Table_Final.index)\n",
    "        Variance_Experiment = pd.DataFrame({'$\\sigma$': np.repeat(noise_level,Table_Final.shape[0])},index=Table_Final.index)\n",
    "        N_Samples_Experiment = pd.DataFrame({'N': np.repeat(N,Table_Final.shape[0])},index=Table_Final.index)\n",
    "        Tailedness_Experiment = pd.DataFrame({'$\\nu$': np.repeat(tailedness,Table_Final.shape[0])},index=Table_Final.index)\n",
    "        Partitioning_Frequency_Experiment = pd.DataFrame({'r': np.repeat(frequency_or_self_paritioning,Table_Final.shape[0])},index=Table_Final.index)\n",
    "        \n",
    "\n",
    "        # Append To Final Table\n",
    "        Table_Final = Table_Final.join(Experiment_Dimension)\n",
    "        Table_Final = Table_Final.join(Variance_Experiment)\n",
    "        Table_Final = Table_Final.join(N_Samples_Experiment)\n",
    "        Table_Final = Table_Final.join(Tailedness_Experiment)\n",
    "        Table_Final = Table_Final.join(Partitioning_Frequency_Experiment)\n",
    "    \n",
    "    # Write to File\n",
    "    Table_Final.to_latex(('outputs/tables/'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'ReltoFFNN'+str(Relative_MAE_to_FFNN)+'Table.tex'),\n",
    "                         caption = 'Prediction and Complexity Metrics: '+str(Option_Function)+'Relative to FFNN'+str(Relative_MAE_to_FFNN),\n",
    "                         label='tab__'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q),\n",
    "                         float_format=\"{:0.2e}\".format)\n",
    "    #--------------======---------------#\n",
    "    print('Display Performance Metrics')\n",
    "    #--------------======---------------#\n",
    "    for j in range(3):\n",
    "        print('-----------------------------------')\n",
    "    print('Final Results of This Experiment: ')\n",
    "    print('Relative to FFNN: '+str(Relative_MAE_to_FFNN))\n",
    "    print(Table_Final)\n",
    "    for j in range(3):\n",
    "        print('-----------------------------------')\n",
    "\n",
    "# Reports Table for Live User\n",
    "Table_Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Plots\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.savefig('./outputs/plotsANDfigures/dump.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MSE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MSE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MSE_ffNN,\n",
    "         label = 'FFNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MSE_ffNN_rnd,\n",
    "         label = 'FFNNN-RND',\n",
    "         color='pink',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_lgt_ffNN_report_MSE_test,\n",
    "         label = 'ffNN-LGT',\n",
    "         color='red',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_PCNN_ffNN_bag_report_MSE_test,\n",
    "         label = 'ffNN-BAG',\n",
    "         color='orange',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "# Add Legend\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MSE_test___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MAE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MAE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MAE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MAE_ffNN,\n",
    "         label = 'FFNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MAE_ffNN_rnd,\n",
    "         label = 'FFNNN-RND',\n",
    "         color='pink',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_lgt_ffNN_report_MAE_test,\n",
    "         label = 'FFNN-LGT',\n",
    "         color='red',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "# plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "#          performance_PCNN_ffNN_bag_report_MAE_test,\n",
    "#          label = 'FFNN-BAG',\n",
    "#          color='orange',\n",
    "#          linewidth=2.5,\n",
    "#          linestyle = '-.', \n",
    "#          alpha = .75)\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MAE___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P. Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"P. Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"P. Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_times_ffNN_plot,\n",
    "         label = 'FFNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_times_ffNN_rnd_plot,\n",
    "         label = 'FFNNN-RND',\n",
    "         color='pink',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_time_PCNN_lgt,\n",
    "         label = 'FFNN-LGT',\n",
    "         color='red',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_time_PCNN_bag,\n",
    "         label = 'FFNN-BAG',\n",
    "         color='orange',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_L_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L. Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"L. Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"L. Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_times_ffNN_plot,\n",
    "         label = 'FFNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_times_ffNN_rnd_plot,\n",
    "         label = 'FFNNN-RND',\n",
    "         color='pink',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_time_PCNN_lgt,\n",
    "         label = 'FFNN-LGT',\n",
    "         color='red',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_time_PCNN_bag,\n",
    "         label = 'FFNN-BAG',\n",
    "         color='orange',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_P_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"N. Params\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"N. Params\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_Params,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN_plot,\n",
    "         label = 'FFNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN_rnd,\n",
    "         label = 'FFNNN-RND',\n",
    "         color='pink',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_logistic_hist,\n",
    "         label = 'FFNN-LGT',\n",
    "         color='red',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_ffNN_bag,\n",
    "         label = 'FFNN-BAG',\n",
    "         color='orange',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_N_Params___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Active Neurons Per Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Active Neurons per. Input\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Active Neurons per. Input\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_per_input,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN_plot,\n",
    "         label = 'FFNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN_rnd,\n",
    "         label = 'FFNNN-RND',\n",
    "         color='pink',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_logistic_hist_per_input,\n",
    "         label = 'FFNN-LGT',\n",
    "         color='red',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_ffNN_bag_per_input,\n",
    "         label = 'FFNN-BAG',\n",
    "         color='orange',\n",
    "         linewidth=2.5,\n",
    "         linestyle = '-.', \n",
    "         alpha = .75)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Active_Neurons_per_input___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Widths for Sub-Pattern Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set()\n",
    "# # Initialize Plot #\n",
    "# #-----------------#\n",
    "# plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# # Format Plot #\n",
    "# #-------------#\n",
    "# plt.title(\"Mean Subpattern Widths\")\n",
    "# plt.xlabel(\"N. Parts\")\n",
    "# plt.ylabel(\"Mean Subpattern Widths\")\n",
    "\n",
    "# # Generate Plots #\n",
    "# #----------------#\n",
    "# # Plot Signal\n",
    "# # plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "# #          mean_subpattern_widths_hist,\n",
    "# #          label = 'ffNN-lgt',\n",
    "# #          color='red',\n",
    "# #          linewidth=2.5,\n",
    "# #          linestyle = '-.', \n",
    "# #          alpha = .75)\n",
    "# # plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "# #          mean_subpattern_widths_hist,\n",
    "# #          label = 'ffNN-bag',\n",
    "# #          color='orange',\n",
    "# #          linewidth=2.5,\n",
    "# #          linestyle = '-.', \n",
    "# #          alpha = .75)\n",
    "# plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "#          mean_subpattern_widths_hist,\n",
    "#          label = 'PCNN',\n",
    "#          color='seagreen',\n",
    "#          linewidth=2.5)\n",
    "# plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "#          mean_subpattern_widths_hist,\n",
    "#          label = 'ffNN',\n",
    "#          color='darkmagenta',\n",
    "#          linewidth=2.5)\n",
    "# plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# # Export #\n",
    "# #--------#\n",
    "# # SAVE Figure to .eps\n",
    "# plt.savefig('./outputs/plotsANDfigures/Ablation_Mean_Widths___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.pdf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# MAE Progression (PCNN only)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MAE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MAE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MAE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MAE_ffNN,\n",
    "         label = 'FFNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MAE___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' Done!!! ')\n",
    "print('Best Number of Part(s): ', N_parts_Generated_by_Algo_2_history[best_N_parts])\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print('Prediction Metric(s)')\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "#--------------======---------------#\n",
    "print('Display Performance Metrics')\n",
    "#--------------======---------------#\n",
    "print(Table_Final.round(3))\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print('Prediction Metric(s)')\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print(' Have a great day!!  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
