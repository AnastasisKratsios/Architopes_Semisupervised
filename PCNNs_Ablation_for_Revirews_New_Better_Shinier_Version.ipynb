{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation: Semi-Supervised Architope - for Reviews\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"PC-NNs\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 1-(1/48)\n",
    "min_width = 200\n",
    "# Ablation Finess\n",
    "N_plot_finess = 2\n",
    "# min_parts_threshold = .001; max_parts_threshold = 0.9\n",
    "N_min_parts = 1; N_max_plots = 2\n",
    "Tied_Neurons_Q = True\n",
    "# Partition with Inputs (determine parts with domain) or outputs (determine parts with image)\n",
    "Partition_using_Inputs = True\n",
    "# Cuttoff Level\n",
    "gamma = .5\n",
    "# Softmax Layer instead of sigmoid\n",
    "softmax_layer = False #<- Just out of curiosity...but it doesn't perform many better IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------#\n",
    "# Only For Motivational Example Only #\n",
    "#------------------------------------#\n",
    "## Hyperparameters\n",
    "percentage_in_row = .25\n",
    "N = 10**4\n",
    "\n",
    "def f_1(x):\n",
    "    return x\n",
    "def f_2(x):\n",
    "    return np.exp(-x)\n",
    "x_0 = 0\n",
    "x_end = 1\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "1\n",
      "Training Data size:  208\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "if Option_Function != \"Motivational_Example\": \n",
    "    exec(open('Financial_Data_Preprocessor.py').read())\n",
    "else:\n",
    "    print(1)\n",
    "    exec(open('Motivational_Example.py').read())\n",
    "    print(\"Training Data size: \",X_train.shape[0])\n",
    "# exec(open('Prepare_Data_California_Housing.py').read())\n",
    "# Import time separately\n",
    "import time\n",
    "\n",
    "# TEMP\n",
    "# import pickle_compat\n",
    "# pickle_compat.patch()\n",
    "# param_grid_Vanilla_Nets['input_dim']=X_train.shape[1]\n",
    "sns.set()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit Partion Builder:\n",
    "Implements exactly Algorithm 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(X_in,\n",
    "                               y_in,\n",
    "                               N_parts_to_get=4):\n",
    "\n",
    "    # Compute Size of each part\n",
    "    size_part_reference = int(round(X_in.shape[0]/N_parts_to_get))\n",
    "\n",
    "    # Apply random bijection #\n",
    "    #------------------------#\n",
    "    ## Get random bijection indices\n",
    "    random_bijection_indices = np.random.choice(range(X_in.shape[0]),size=X_in.shape[0], replace=False)\n",
    "    ## Apply random bijections\n",
    "    X_in_shuffled = X_in[random_bijection_indices,:]\n",
    "    y_in_shuffled = y_in[random_bijection_indices,:]\n",
    "\n",
    "    # Initialize Lists #\n",
    "    #------------------#\n",
    "    X_parts = []\n",
    "    y_parts = []\n",
    "\n",
    "    for i_th_part_to_get in range(N_parts_to_get):\n",
    "        # Build random balls #\n",
    "        #--------------------#\n",
    "        ## Sample random radius\n",
    "        size_part = int(np.maximum(1,np.round(size_part_reference*np.random.uniform(low=.5,high=1.5,size=1)[0])))\n",
    "        ## Sample random point\n",
    "        X_center_loop_index = np.random.choice(range(X_in_shuffled.shape[0]),size=1, replace=False)\n",
    "        X_center_loop = X_in_shuffled[X_center_loop_index,:]\n",
    "        ## Compute Typical Distances from Center\n",
    "        distances_loop = X_center_loop-X_in_shuffled\n",
    "        distances_loop = np.linalg.norm(distances_loop, axis=1)\n",
    "\n",
    "        # Remove Random Ball from Dataset\n",
    "        if size_part <= len(distances_loop):\n",
    "            ## Identify indices\n",
    "            indices_smallest_to_random_ball = np.argsort(distances_loop)[:size_part]\n",
    "        else:\n",
    "            print('Final Loop')\n",
    "            indices_smallest_to_random_ball = np.array(range(X_in_shuffled.shape[0]))\n",
    "        ## Extract Parts\n",
    "        X_current_part_loop = X_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        y_current_part_loop = y_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        ## Append to List of Parts\n",
    "        X_parts.append(X_current_part_loop)\n",
    "        y_parts.append(y_current_part_loop)\n",
    "\n",
    "        # Remove Selected Entries From Array #\n",
    "        #------------------------------------#\n",
    "        X_in_shuffled = np.delete(X_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "        y_in_shuffled = np.delete(y_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "\n",
    "        # Failsafe if procedure has terminated\n",
    "        if X_in_shuffled.shape[0] == 0:\n",
    "            print('breaking early')\n",
    "            break\n",
    "    # Count Number of Parts Generated        \n",
    "    N_parts_generated = len(X_parts)\n",
    "    # Output Parts\n",
    "    return X_parts, y_parts, N_parts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCNNs(N_parts,X_train,y_train,X_test,y_test):\n",
    "\n",
    "    # Initialization(s) #\n",
    "    #-------------------#\n",
    "    N_neurons = 0\n",
    "    L_timer = 0\n",
    "    P_timer = 0\n",
    "    Mean_Width_Subnetworks = 0\n",
    "\n",
    "    # Partitioner Begin #\n",
    "    #-------------------#\n",
    "    import time\n",
    "    partitioning_time_begin = time.time()\n",
    "    print('-------------------------------------------------------')\n",
    "    print('Randomly Initialized Parts - Via Randomized Algorithm 2')\n",
    "    print('-------------------------------------------------------')\n",
    "    if Partition_using_Inputs == True:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(X_train.to_numpy(),\n",
    "                                                                                             y_train.reshape(-1,1),\n",
    "                                                                                             N_parts)\n",
    "    else:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(y_train.reshape(-1,1),\n",
    "                                                                                             X_train.to_numpy(),\n",
    "                                                                                             N_parts)\n",
    "    partitioning_time = time.time() - partitioning_time_begin\n",
    "    print('The_parts_listhe number of parts are: ' + str(N_parts_Generated_by_Algo_2)+'.')\n",
    "    ############# Partitioner End ########\n",
    "\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Training Sub-Networks on Each Randomly Generated Part')\n",
    "    print('-----------------------------------------------------')\n",
    "    # Time-Elapse (Start) for Training on Each Part #\n",
    "    PCNN_timer = time.time(); PCNN_timer = -math.inf; N_params_Architope = 0; N_params_tally = 0\n",
    "    # Remove Eager Execution Error(s)\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    # Automatically Initialize Correct Input/Output Dimension(s)\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]; param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "    # Decide if/or not to tie neuron numbers of sub-patterns together\n",
    "    if Tied_Neurons_Q == True:\n",
    "        param_grid_Vanilla_Nets['height'] = [int(np.maximum(round(param_grid_Vanilla_Nets['height'][0]/N_parts),min_width))]\n",
    "        param_grid_Vanilla_Nets['epochs'] = [int(np.maximum(round(param_grid_Vanilla_Nets['epochs'][0]/int(round(np.sqrt(N_parts)))),min_width))]\n",
    "#         param_grid_Deep_Classifier['height'] = [int(np.maximum(round(param_grid_Deep_Classifier['height'][0]/N_parts),min_width))]\n",
    "\n",
    "    for current_part in range(N_parts_Generated_by_Algo_2):\n",
    "        # Update User #\n",
    "        #-------------#\n",
    "        print('-----------------------------------------------------------')\n",
    "        print('Currently Training Part: '+str(current_part)+'/'+str(N_parts_Generated_by_Algo_2 )+'Total Parts.')\n",
    "        print('-----------------------------------------------------------')\n",
    "\n",
    "        # Timer for Part\n",
    "        part_training_timer = time.time()\n",
    "        # Get Data for Sub-Pattern\n",
    "        X_loop = pd.DataFrame(X_parts_list[current_part])\n",
    "        y_loop = (y_parts_list[current_part]).reshape(-1,)\n",
    "        # Train ffNN\n",
    "        y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN(n_folds = 4, \n",
    "                                                                              n_jobs = n_jobs,\n",
    "                                                                              n_iter = n_iter, \n",
    "                                                                              param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                              X_train= X_loop, \n",
    "                                                                              y_train=y_loop,\n",
    "                                                                              X_test_partial=X_train,\n",
    "                                                                              X_test=X_test,\n",
    "                                                                              NOCV=True)\n",
    "        # Reshape y\n",
    "        ## Training\n",
    "        y_train.shape = (y_train.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop.shape = (y_hat_part_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        ## Testing\n",
    "        y_test.shape = (y_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop_test.shape = (y_hat_part_loop_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "\n",
    "        # Append predictions to data-frames\n",
    "        ## If first prediction we initialize data-frames\n",
    "        if current_part==0:\n",
    "            # Register quality\n",
    "            training_quality = np.array(np.abs(y_hat_part_loop-y_train)).reshape(y_hat_part_loop.shape[0],1)\n",
    "\n",
    "            # Save Predictions\n",
    "            predictions_train = y_hat_part_loop.reshape(y_hat_part_loop.shape[0],1)\n",
    "            predictions_test = y_hat_part_loop_test.reshape(y_hat_part_loop_test.shape[0],1)\n",
    "\n",
    "\n",
    "        ## If not first prediction we append to already initialized dataframes\n",
    "        else:\n",
    "        # Register Best Scores\n",
    "            #----------------------#\n",
    "            # Write Predictions \n",
    "            # Save Predictions\n",
    "            y_hat_train_loop = y_hat_part_loop.reshape(predictions_train.shape[0],1)\n",
    "            predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "            y_hat_test_loop = y_hat_part_loop_test.reshape(predictions_test.shape[0],1)\n",
    "            predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "\n",
    "            # Evaluate Errors #\n",
    "            #-----------------#\n",
    "            # Training\n",
    "            prediction_errors = np.abs(y_hat_train_loop-y_train)\n",
    "            training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "\n",
    "        #==============================#\n",
    "        # Update Performance Metric(s) #\n",
    "        #==============================#\n",
    "        part_training_timer = time.time() - part_training_timer\n",
    "        # L-Time\n",
    "        L_timer += partitioning_time\n",
    "        # P-Time\n",
    "        P_timer = max(P_timer,part_training_timer)\n",
    "        # N. Params\n",
    "        N_neurons += N_neurons_PCNN_loop\n",
    "        # Mean Width for Sub-Network(s)\n",
    "        Mean_Width_Subnetworks += param_grid_Vanilla_Nets['height'][0]\n",
    "\n",
    "    # Take Mean of Width(s)\n",
    "    Mean_Width_Subnetworks = Mean_Width_Subnetworks/N_parts_Generated_by_Algo_2\n",
    "    print('-----------------------')\n",
    "    print('Training Deep Zero-Sets')\n",
    "    print('-----------------------')\n",
    "\n",
    "\n",
    "    # Time Elapsed for Training Deep Zero-Sets\n",
    "    Deep_Zero_Sets_timer = time.time()\n",
    "\n",
    "    ## Initialize Classes Labels\n",
    "    if softmax_layer == False:\n",
    "        # No pooling (classical)\n",
    "        partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "    else:\n",
    "        # Max Pooling\n",
    "#         partition_labels_training_integers = (training_quality == training_quality.min(axis=1)[:,None]).astype(int)\n",
    "        partition_labels_training_integers = np.apply_along_axis(softminn, 1, training_quality).astype(int)\n",
    "    partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "    ## Build Classes\n",
    "    for part_column_i in range(1,(training_quality.shape[1])):\n",
    "        partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                               (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                              ],axis=1)\n",
    "    ## Convert to integers\n",
    "    partition_labels_training = partition_labels_training+0\n",
    "    ## Train simple deep classifier\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter =n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train.values, \n",
    "                                                                                                        y_train = partition_labels_training.values,\n",
    "                                                                                                        X_test = X_test.values)\n",
    "    # Get Binary Classes (Discontinuous Unit)\n",
    "    ## Training Set\n",
    "    predicted_classes_train = ((predicted_classes_train>gamma)*1).astype(int)\n",
    "    ## Testing Set\n",
    "    predicted_classes_test = ((predicted_classes_test > gamma)*1).astype(int)\n",
    "    # Get PC-NN Prediction(s)\n",
    "    ## Train\n",
    "    PCNN_prediction_y_train = (predictions_train*predicted_classes_train).sum(axis=1)\n",
    "    ## Test\n",
    "    PCNN_prediction_y_test = (predictions_test*predicted_classes_test).sum(axis=1)\n",
    "\n",
    "    # End Timer\n",
    "    Deep_Zero_Sets_timer = time.time() - Deep_Zero_Sets_timer\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Computing Final Performance Metrics')\n",
    "    print('-----------------------------------')\n",
    "    # Time-Elapsed Training Deep Classifier\n",
    "\n",
    "    # Update Times\n",
    "    L_timer +=Deep_Zero_Sets_timer\n",
    "    P_timer +=Deep_Zero_Sets_timer\n",
    "    # Update Number of Neurons Used\n",
    "    N_neurons_subPatterns = N_neurons\n",
    "    N_neurons_deep_Zero_Sets = (param_grid_Deep_Classifier['height'][0])*(param_grid_Deep_Classifier['depth'][0])\n",
    "    N_neurons = N_neurons_deep_Zero_Sets + N_neurons_subPatterns\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Peformance\n",
    "    performance_PCNN = reporter(y_train_hat_in=PCNN_prediction_y_train,y_test_hat_in=PCNN_prediction_y_test,\n",
    "                                y_train_in=y_train,\n",
    "                                y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_PCNN.to_latex((results_tables_path+\"PCNN_full_performance.tex\"))\n",
    "\n",
    "    # Update User\n",
    "    print(performance_PCNN)\n",
    "\n",
    "    ### Model Complexity/Efficiency Metrics\n",
    "    # Build AIC-like Metric #\n",
    "    #-----------------------#\n",
    "    AIC_like = 2*(N_neurons - np.log((performance_PCNN['test']['MAE'])))\n",
    "    AIC_like = np.round(AIC_like,3)\n",
    "    Efficiency = np.log(N_neurons) *(performance_PCNN['test']['MAE'])\n",
    "    Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "    # Build Table #\n",
    "    #-------------#\n",
    "    PCNN_Model_Complexity = pd.DataFrame({'L-time': [L_timer],\n",
    "                                               'P-time':[P_timer],\n",
    "                                               'N_params_expt': [N_neurons],\n",
    "                                               'AIC-like': [AIC_like],\n",
    "                                               'Eff': [Efficiency],\n",
    "                                               'N. Parts':[N_parts_Generated_by_Algo_2]})\n",
    "\n",
    "\n",
    "    # Write Required Training Time(s)\n",
    "    PCNN_Model_Complexity.to_latex((results_tables_path+\"PCNN_full_model_complexities.tex\"))\n",
    "\n",
    "    #--------------======---------------#\n",
    "    # Display Required Training Time(s) #\n",
    "    #--------------======---------------#\n",
    "    print(PCNN_Model_Complexity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #########################################\n",
    "    for j in range(10):\n",
    "        print('#------------------------------#')\n",
    "    #########################################\n",
    "    print('# ---- Getting Benchmarks ---- #')\n",
    "    #########################################\n",
    "    for j in range(10):\n",
    "        print('#------------------------------#')\n",
    "    #########################################\n",
    "    # Time-Elapsed Training linear classifier\n",
    "    Architope_logistic_classifier_training_begin = time.time()\n",
    "    \n",
    "    if N_parts > 1:\n",
    "        parameters = {'penalty': ['none'], 'C': [0.1]}\n",
    "        lr = LogisticRegression(random_state=2020)\n",
    "        cv = RepeatedStratifiedKFold(n_splits=CV_folds, \n",
    "                                     n_repeats=n_iter, random_state=0)\n",
    "        classifier = RandomizedSearchCV(lr, \n",
    "                                        parameters, \n",
    "                                        random_state=2020)\n",
    "\n",
    "        # Initialize Classes Labels\n",
    "        partition_labels_training = np.argmin(training_quality,axis=-1)\n",
    "\n",
    "        # Train logistic Classifier\n",
    "        print(\"Training classifier and generating partition!\")\n",
    "\n",
    "        # Train Logistic Classifier #\n",
    "        #---------------------------#\n",
    "        # Supress warnings caused by \"ignoring C\" for 'none' penalty and similar obvious warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Train Classifier\n",
    "        classifier.fit(X_train, partition_labels_training)\n",
    "    if N_parts >1 :\n",
    "        #### Write Predicted Class(es)\n",
    "        # Training Set\n",
    "        predicted_classes_train_logistic_BM = classifier.best_estimator_.predict(X_train)\n",
    "        Architope_prediction_y_train_logistic_BM = np.take_along_axis(predictions_train, predicted_classes_train_logistic_BM[:,None], axis=1)\n",
    "\n",
    "        # Testing Set\n",
    "        predicted_classes_test_logistic_BM = classifier.best_estimator_.predict(X_test)\n",
    "        Architope_prediction_y_test_logistic_BM = np.take_along_axis(predictions_test, \n",
    "                                                                     predicted_classes_test_logistic_BM[:,None], \n",
    "                                                                     axis=1)\n",
    "    else:\n",
    "        #### Write Predicted Class(es)\n",
    "        # Training Set\n",
    "        Architope_prediction_y_train_logistic_BM = predictions_train\n",
    "\n",
    "        # Testing Set\n",
    "        Architope_prediction_y_test_logistic_BM = predictions_test\n",
    "        \n",
    "    # Extract Number of Parameters Logistic Regressor\n",
    "    if N_parts > 1:\n",
    "        N_params_best_logistic = (classifier.best_estimator_.coef_.shape[0])*(classifier.best_estimator_.coef_.shape[1]) + len(classifier.best_estimator_.intercept_)\n",
    "    else:\n",
    "        N_params_best_logistic = 1\n",
    "        \n",
    "    # Time-Elapsed Training linear classifier\n",
    "    Architope_logistic_classifier_training = time.time() - Architope_logistic_classifier_training_begin\n",
    "\n",
    "    #### Compute Performance\n",
    "    # Compute Peformance\n",
    "    performance_architope_ffNN_logistic = reporter(y_train_hat_in=Architope_prediction_y_train_logistic_BM,\n",
    "                                        y_test_hat_in=Architope_prediction_y_test_logistic_BM,\n",
    "                                        y_train_in=y_train,\n",
    "                                        y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_architope_ffNN_logistic.to_latex((results_tables_path+\"Architopes_logistic_performance.tex\"))\n",
    "\n",
    "    \n",
    "    # Return Output(s)\n",
    "    return performance_PCNN, PCNN_Model_Complexity, N_parts_Generated_by_Algo_2, N_neurons, N_neurons_subPatterns,N_neurons_deep_Zero_Sets, Mean_Width_Subnetworks, performance_architope_ffNN_logistic, N_params_best_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Perform Ablation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Ablation Completion Percentage: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "-------------------------------------------------------\n",
      "Randomly Initialized Parts - Via Randomized Algorithm 2\n",
      "-------------------------------------------------------\n",
      "Final Loop\n",
      "breaking early\n",
      "The_parts_listhe number of parts are: 1.\n",
      "-----------------------------------------------------\n",
      "Training Sub-Networks on Each Randomly Generated Part\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 0/1Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Train on 208 samples\n",
      "Epoch 1/200\n",
      "208/208 [==============================] - 0s 1ms/sample - loss: 0.5790 - mse: 0.3790 - mae: 0.5790 - mape: 106.1126\n",
      "Epoch 2/200\n",
      "208/208 [==============================] - 0s 71us/sample - loss: 0.5783 - mse: 0.3782 - mae: 0.5783 - mape: 105.8534\n",
      "Epoch 3/200\n",
      "208/208 [==============================] - 0s 74us/sample - loss: 0.5776 - mse: 0.3774 - mae: 0.5776 - mape: 105.5715\n",
      "Epoch 4/200\n",
      "208/208 [==============================] - 0s 80us/sample - loss: 0.5768 - mse: 0.3765 - mae: 0.5768 - mape: 105.2931\n",
      "Epoch 5/200\n",
      "208/208 [==============================] - 0s 74us/sample - loss: 0.5761 - mse: 0.3757 - mae: 0.5761 - mape: 105.0485\n",
      "Epoch 6/200\n",
      "208/208 [==============================] - 0s 86us/sample - loss: 0.5754 - mse: 0.3748 - mae: 0.5754 - mape: 104.7742\n",
      "Epoch 7/200\n",
      "208/208 [==============================] - 0s 92us/sample - loss: 0.5746 - mse: 0.3740 - mae: 0.5746 - mape: 104.5243\n",
      "Epoch 8/200\n",
      "208/208 [==============================] - 0s 69us/sample - loss: 0.5739 - mse: 0.3732 - mae: 0.5739 - mape: 104.2669\n",
      "Epoch 9/200\n",
      "208/208 [==============================] - 0s 74us/sample - loss: 0.5732 - mse: 0.3723 - mae: 0.5732 - mape: 103.9817\n",
      "Epoch 10/200\n",
      "208/208 [==============================] - 0s 81us/sample - loss: 0.5724 - mse: 0.3715 - mae: 0.5724 - mape: 103.6984\n",
      "Epoch 11/200\n",
      "208/208 [==============================] - 0s 81us/sample - loss: 0.5717 - mse: 0.3707 - mae: 0.5717 - mape: 103.4527\n",
      "Epoch 12/200\n",
      "208/208 [==============================] - 0s 75us/sample - loss: 0.5709 - mse: 0.3698 - mae: 0.5709 - mape: 103.1822\n",
      "Epoch 13/200\n",
      "208/208 [==============================] - 0s 70us/sample - loss: 0.5702 - mse: 0.3690 - mae: 0.5702 - mape: 102.9211\n",
      "Epoch 14/200\n",
      "208/208 [==============================] - 0s 62us/sample - loss: 0.5695 - mse: 0.3682 - mae: 0.5695 - mape: 102.6515\n",
      "Epoch 15/200\n",
      "208/208 [==============================] - 0s 94us/sample - loss: 0.5687 - mse: 0.3674 - mae: 0.5687 - mape: 102.3896\n",
      "Epoch 16/200\n",
      "208/208 [==============================] - 0s 91us/sample - loss: 0.5680 - mse: 0.3665 - mae: 0.5680 - mape: 102.1061\n",
      "Epoch 17/200\n",
      "208/208 [==============================] - 0s 84us/sample - loss: 0.5673 - mse: 0.3657 - mae: 0.5673 - mape: 101.8502\n",
      "Epoch 18/200\n",
      "208/208 [==============================] - 0s 83us/sample - loss: 0.5665 - mse: 0.3649 - mae: 0.5665 - mape: 101.5871\n",
      "Epoch 19/200\n",
      "208/208 [==============================] - 0s 87us/sample - loss: 0.5658 - mse: 0.3641 - mae: 0.5658 - mape: 101.3121\n",
      "Epoch 20/200\n",
      "208/208 [==============================] - 0s 77us/sample - loss: 0.5650 - mse: 0.3632 - mae: 0.5650 - mape: 101.0702\n",
      "Epoch 21/200\n",
      "208/208 [==============================] - 0s 65us/sample - loss: 0.5643 - mse: 0.3624 - mae: 0.5643 - mape: 100.7817\n",
      "Epoch 22/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.5636 - mse: 0.3616 - mae: 0.5636 - mape: 100.5233\n",
      "Epoch 23/200\n",
      "208/208 [==============================] - 0s 76us/sample - loss: 0.5628 - mse: 0.3608 - mae: 0.5628 - mape: 100.2669\n",
      "Epoch 24/200\n",
      "208/208 [==============================] - 0s 75us/sample - loss: 0.5621 - mse: 0.3600 - mae: 0.5621 - mape: 99.9744\n",
      "Epoch 25/200\n",
      "208/208 [==============================] - 0s 100us/sample - loss: 0.5614 - mse: 0.3591 - mae: 0.5614 - mape: 99.7054\n",
      "Epoch 26/200\n",
      "208/208 [==============================] - 0s 118us/sample - loss: 0.5606 - mse: 0.3583 - mae: 0.5606 - mape: 99.4370\n",
      "Epoch 27/200\n",
      "208/208 [==============================] - 0s 118us/sample - loss: 0.5599 - mse: 0.3575 - mae: 0.5599 - mape: 99.1828\n",
      "Epoch 28/200\n",
      "208/208 [==============================] - 0s 87us/sample - loss: 0.5591 - mse: 0.3567 - mae: 0.5591 - mape: 98.9013\n",
      "Epoch 29/200\n",
      "208/208 [==============================] - 0s 78us/sample - loss: 0.5584 - mse: 0.3559 - mae: 0.5584 - mape: 98.6503\n",
      "Epoch 30/200\n",
      "208/208 [==============================] - 0s 79us/sample - loss: 0.5576 - mse: 0.3551 - mae: 0.5576 - mape: 98.3925\n",
      "Epoch 31/200\n",
      "208/208 [==============================] - 0s 69us/sample - loss: 0.5569 - mse: 0.3542 - mae: 0.5569 - mape: 98.0816\n",
      "Epoch 32/200\n",
      "208/208 [==============================] - 0s 61us/sample - loss: 0.5562 - mse: 0.3534 - mae: 0.5562 - mape: 97.8461\n",
      "Epoch 33/200\n",
      "208/208 [==============================] - 0s 88us/sample - loss: 0.5554 - mse: 0.3526 - mae: 0.5554 - mape: 97.5623\n",
      "Epoch 34/200\n",
      "208/208 [==============================] - 0s 98us/sample - loss: 0.5547 - mse: 0.3518 - mae: 0.5547 - mape: 97.3088\n",
      "Epoch 35/200\n",
      "208/208 [==============================] - 0s 86us/sample - loss: 0.5539 - mse: 0.3510 - mae: 0.5539 - mape: 97.0413\n",
      "Epoch 36/200\n",
      "208/208 [==============================] - 0s 77us/sample - loss: 0.5532 - mse: 0.3502 - mae: 0.5532 - mape: 96.7373\n",
      "Epoch 37/200\n",
      "208/208 [==============================] - 0s 74us/sample - loss: 0.5524 - mse: 0.3494 - mae: 0.5524 - mape: 96.4961\n",
      "Epoch 38/200\n",
      "208/208 [==============================] - 0s 90us/sample - loss: 0.5517 - mse: 0.3486 - mae: 0.5517 - mape: 96.2172\n",
      "Epoch 39/200\n",
      "208/208 [==============================] - 0s 78us/sample - loss: 0.5509 - mse: 0.3477 - mae: 0.5509 - mape: 95.9688\n",
      "Epoch 40/200\n",
      "208/208 [==============================] - 0s 84us/sample - loss: 0.5502 - mse: 0.3469 - mae: 0.5502 - mape: 95.7433\n",
      "Epoch 41/200\n",
      "208/208 [==============================] - 0s 114us/sample - loss: 0.5495 - mse: 0.3461 - mae: 0.5495 - mape: 95.5713\n",
      "Epoch 42/200\n",
      "208/208 [==============================] - 0s 87us/sample - loss: 0.5487 - mse: 0.3453 - mae: 0.5487 - mape: 95.3848\n",
      "Epoch 43/200\n",
      "208/208 [==============================] - 0s 77us/sample - loss: 0.5480 - mse: 0.3445 - mae: 0.5480 - mape: 95.2021\n",
      "Epoch 44/200\n",
      "208/208 [==============================] - 0s 86us/sample - loss: 0.5473 - mse: 0.3437 - mae: 0.5473 - mape: 94.9978\n",
      "Epoch 45/200\n",
      "208/208 [==============================] - 0s 89us/sample - loss: 0.5465 - mse: 0.3429 - mae: 0.5465 - mape: 94.8312\n",
      "Epoch 46/200\n",
      "208/208 [==============================] - 0s 78us/sample - loss: 0.5458 - mse: 0.3421 - mae: 0.5458 - mape: 94.6283\n",
      "Epoch 47/200\n",
      "208/208 [==============================] - 0s 82us/sample - loss: 0.5450 - mse: 0.3413 - mae: 0.5450 - mape: 94.4794\n",
      "Epoch 48/200\n",
      "208/208 [==============================] - 0s 82us/sample - loss: 0.5443 - mse: 0.3405 - mae: 0.5443 - mape: 94.2678\n",
      "Epoch 49/200\n",
      "208/208 [==============================] - 0s 88us/sample - loss: 0.5436 - mse: 0.3397 - mae: 0.5436 - mape: 94.0782\n",
      "Epoch 50/200\n",
      "208/208 [==============================] - 0s 91us/sample - loss: 0.5428 - mse: 0.3389 - mae: 0.5428 - mape: 93.8738\n",
      "Epoch 51/200\n",
      "208/208 [==============================] - 0s 88us/sample - loss: 0.5421 - mse: 0.3381 - mae: 0.5421 - mape: 93.7022\n",
      "Epoch 52/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.5413 - mse: 0.3373 - mae: 0.5413 - mape: 93.4876\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 0s 65us/sample - loss: 0.5406 - mse: 0.3365 - mae: 0.5406 - mape: 93.3410\n",
      "Epoch 54/200\n",
      "208/208 [==============================] - 0s 75us/sample - loss: 0.5398 - mse: 0.3357 - mae: 0.5398 - mape: 93.1314\n",
      "Epoch 55/200\n",
      "208/208 [==============================] - 0s 65us/sample - loss: 0.5391 - mse: 0.3349 - mae: 0.5391 - mape: 92.9549\n",
      "Epoch 56/200\n",
      "208/208 [==============================] - 0s 55us/sample - loss: 0.5383 - mse: 0.3341 - mae: 0.5383 - mape: 92.7808\n",
      "Epoch 57/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.5376 - mse: 0.3333 - mae: 0.5376 - mape: 92.5981\n",
      "Epoch 58/200\n",
      "208/208 [==============================] - 0s 65us/sample - loss: 0.5369 - mse: 0.3325 - mae: 0.5369 - mape: 92.4250\n",
      "Epoch 59/200\n",
      "208/208 [==============================] - 0s 77us/sample - loss: 0.5361 - mse: 0.3317 - mae: 0.5361 - mape: 92.2996\n",
      "Epoch 60/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.5354 - mse: 0.3309 - mae: 0.5354 - mape: 92.1173\n",
      "Epoch 61/200\n",
      "208/208 [==============================] - 0s 62us/sample - loss: 0.5346 - mse: 0.3302 - mae: 0.5346 - mape: 92.0004\n",
      "Epoch 62/200\n",
      "208/208 [==============================] - 0s 61us/sample - loss: 0.5339 - mse: 0.3294 - mae: 0.5339 - mape: 91.8617\n",
      "Epoch 63/200\n",
      "208/208 [==============================] - 0s 66us/sample - loss: 0.5332 - mse: 0.3286 - mae: 0.5332 - mape: 91.7773\n",
      "Epoch 64/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.5325 - mse: 0.3278 - mae: 0.5325 - mape: 91.7118\n",
      "Epoch 65/200\n",
      "208/208 [==============================] - 0s 73us/sample - loss: 0.5317 - mse: 0.3270 - mae: 0.5317 - mape: 91.6439\n",
      "Epoch 66/200\n",
      "208/208 [==============================] - 0s 57us/sample - loss: 0.5310 - mse: 0.3263 - mae: 0.5310 - mape: 91.5937\n",
      "Epoch 67/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.5303 - mse: 0.3255 - mae: 0.5303 - mape: 91.5639\n",
      "Epoch 68/200\n",
      "208/208 [==============================] - 0s 66us/sample - loss: 0.5296 - mse: 0.3247 - mae: 0.5296 - mape: 91.5048\n",
      "Epoch 69/200\n",
      "208/208 [==============================] - 0s 70us/sample - loss: 0.5289 - mse: 0.3240 - mae: 0.5289 - mape: 91.4492\n",
      "Epoch 70/200\n",
      "208/208 [==============================] - 0s 79us/sample - loss: 0.5282 - mse: 0.3232 - mae: 0.5282 - mape: 91.4292\n",
      "Epoch 71/200\n",
      "208/208 [==============================] - 0s 65us/sample - loss: 0.5275 - mse: 0.3224 - mae: 0.5275 - mape: 91.3513\n",
      "Epoch 72/200\n",
      "208/208 [==============================] - 0s 61us/sample - loss: 0.5268 - mse: 0.3217 - mae: 0.5268 - mape: 91.3324\n",
      "Epoch 73/200\n",
      "208/208 [==============================] - 0s 69us/sample - loss: 0.5261 - mse: 0.3209 - mae: 0.5261 - mape: 91.2694\n",
      "Epoch 74/200\n",
      "208/208 [==============================] - 0s 70us/sample - loss: 0.5254 - mse: 0.3201 - mae: 0.5254 - mape: 91.2286\n",
      "Epoch 75/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.5247 - mse: 0.3194 - mae: 0.5247 - mape: 91.1750\n",
      "Epoch 76/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.5240 - mse: 0.3186 - mae: 0.5240 - mape: 91.1659\n",
      "Epoch 77/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.5232 - mse: 0.3178 - mae: 0.5232 - mape: 91.0743\n",
      "Epoch 78/200\n",
      "208/208 [==============================] - 0s 60us/sample - loss: 0.5225 - mse: 0.3171 - mae: 0.5225 - mape: 91.0278\n",
      "Epoch 79/200\n",
      "208/208 [==============================] - 0s 66us/sample - loss: 0.5218 - mse: 0.3163 - mae: 0.5218 - mape: 91.0095\n",
      "Epoch 80/200\n",
      "208/208 [==============================] - 0s 77us/sample - loss: 0.5211 - mse: 0.3155 - mae: 0.5211 - mape: 90.9217\n",
      "Epoch 81/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.5204 - mse: 0.3148 - mae: 0.5204 - mape: 90.9197\n",
      "Epoch 82/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.5197 - mse: 0.3140 - mae: 0.5197 - mape: 90.8513\n",
      "Epoch 83/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.5189 - mse: 0.3132 - mae: 0.5189 - mape: 90.8082\n",
      "Epoch 84/200\n",
      "208/208 [==============================] - 0s 68us/sample - loss: 0.5182 - mse: 0.3125 - mae: 0.5182 - mape: 90.7735\n",
      "Epoch 85/200\n",
      "208/208 [==============================] - 0s 56us/sample - loss: 0.5175 - mse: 0.3117 - mae: 0.5175 - mape: 90.7488\n",
      "Epoch 86/200\n",
      "208/208 [==============================] - 0s 58us/sample - loss: 0.5168 - mse: 0.3109 - mae: 0.5168 - mape: 90.6544\n",
      "Epoch 87/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.5160 - mse: 0.3102 - mae: 0.5160 - mape: 90.6158\n",
      "Epoch 88/200\n",
      "208/208 [==============================] - 0s 66us/sample - loss: 0.5153 - mse: 0.3094 - mae: 0.5153 - mape: 90.5680\n",
      "Epoch 89/200\n",
      "208/208 [==============================] - 0s 70us/sample - loss: 0.5146 - mse: 0.3086 - mae: 0.5146 - mape: 90.5355\n",
      "Epoch 90/200\n",
      "208/208 [==============================] - 0s 81us/sample - loss: 0.5139 - mse: 0.3079 - mae: 0.5139 - mape: 90.4874\n",
      "Epoch 91/200\n",
      "208/208 [==============================] - 0s 66us/sample - loss: 0.5131 - mse: 0.3071 - mae: 0.5131 - mape: 90.4567\n",
      "Epoch 92/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.5124 - mse: 0.3064 - mae: 0.5124 - mape: 90.3968\n",
      "Epoch 93/200\n",
      "208/208 [==============================] - 0s 69us/sample - loss: 0.5117 - mse: 0.3056 - mae: 0.5117 - mape: 90.3454\n",
      "Epoch 94/200\n",
      "208/208 [==============================] - 0s 76us/sample - loss: 0.5110 - mse: 0.3048 - mae: 0.5110 - mape: 90.3162\n",
      "Epoch 95/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.5102 - mse: 0.3041 - mae: 0.5102 - mape: 90.2491\n",
      "Epoch 96/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.5095 - mse: 0.3033 - mae: 0.5095 - mape: 90.1791\n",
      "Epoch 97/200\n",
      "208/208 [==============================] - 0s 61us/sample - loss: 0.5088 - mse: 0.3026 - mae: 0.5088 - mape: 90.1623\n",
      "Epoch 98/200\n",
      "208/208 [==============================] - 0s 69us/sample - loss: 0.5080 - mse: 0.3018 - mae: 0.5080 - mape: 90.0909\n",
      "Epoch 99/200\n",
      "208/208 [==============================] - 0s 82us/sample - loss: 0.5073 - mse: 0.3010 - mae: 0.5073 - mape: 90.1136\n",
      "Epoch 100/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.5066 - mse: 0.3003 - mae: 0.5066 - mape: 90.0739\n",
      "Epoch 101/200\n",
      "208/208 [==============================] - 0s 65us/sample - loss: 0.5059 - mse: 0.2995 - mae: 0.5059 - mape: 90.0560\n",
      "Epoch 102/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.5051 - mse: 0.2988 - mae: 0.5051 - mape: 89.9687\n",
      "Epoch 103/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.5044 - mse: 0.2980 - mae: 0.5044 - mape: 89.9525\n",
      "Epoch 104/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.5037 - mse: 0.2972 - mae: 0.5037 - mape: 89.9002\n",
      "Epoch 105/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.5029 - mse: 0.2965 - mae: 0.5029 - mape: 89.8756\n",
      "Epoch 106/200\n",
      "208/208 [==============================] - 0s 61us/sample - loss: 0.5022 - mse: 0.2957 - mae: 0.5022 - mape: 89.8609\n",
      "Epoch 107/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.5015 - mse: 0.2950 - mae: 0.5015 - mape: 89.8221\n",
      "Epoch 108/200\n",
      "208/208 [==============================] - 0s 81us/sample - loss: 0.5007 - mse: 0.2942 - mae: 0.5007 - mape: 89.7832\n",
      "Epoch 109/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.5000 - mse: 0.2934 - mae: 0.5000 - mape: 89.7844\n",
      "Epoch 110/200\n",
      "208/208 [==============================] - 0s 65us/sample - loss: 0.4992 - mse: 0.2927 - mae: 0.4992 - mape: 89.7154\n",
      "Epoch 111/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.4985 - mse: 0.2919 - mae: 0.4985 - mape: 89.7329\n",
      "Epoch 112/200\n",
      "208/208 [==============================] - 0s 68us/sample - loss: 0.4978 - mse: 0.2912 - mae: 0.4978 - mape: 89.7109\n",
      "Epoch 113/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.4971 - mse: 0.2904 - mae: 0.4971 - mape: 89.6485\n",
      "Epoch 114/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.4963 - mse: 0.2897 - mae: 0.4963 - mape: 89.6313\n",
      "Epoch 115/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.4956 - mse: 0.2889 - mae: 0.4956 - mape: 89.6228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/200\n",
      "208/208 [==============================] - 0s 76us/sample - loss: 0.4949 - mse: 0.2882 - mae: 0.4949 - mape: 89.6139\n",
      "Epoch 117/200\n",
      "208/208 [==============================] - 0s 70us/sample - loss: 0.4942 - mse: 0.2874 - mae: 0.4942 - mape: 89.6156\n",
      "Epoch 118/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.4934 - mse: 0.2867 - mae: 0.4934 - mape: 89.5922\n",
      "Epoch 119/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.4927 - mse: 0.2859 - mae: 0.4927 - mape: 89.5992\n",
      "Epoch 120/200\n",
      "208/208 [==============================] - 0s 75us/sample - loss: 0.4920 - mse: 0.2852 - mae: 0.4920 - mape: 89.5727\n",
      "Epoch 121/200\n",
      "208/208 [==============================] - 0s 69us/sample - loss: 0.4912 - mse: 0.2844 - mae: 0.4912 - mape: 89.5550\n",
      "Epoch 122/200\n",
      "208/208 [==============================] - 0s 74us/sample - loss: 0.4905 - mse: 0.2837 - mae: 0.4905 - mape: 89.5337\n",
      "Epoch 123/200\n",
      "208/208 [==============================] - 0s 71us/sample - loss: 0.4898 - mse: 0.2829 - mae: 0.4898 - mape: 89.5236\n",
      "Epoch 124/200\n",
      "208/208 [==============================] - 0s 85us/sample - loss: 0.4890 - mse: 0.2822 - mae: 0.4890 - mape: 89.4912\n",
      "Epoch 125/200\n",
      "208/208 [==============================] - 0s 74us/sample - loss: 0.4883 - mse: 0.2814 - mae: 0.4883 - mape: 89.4923\n",
      "Epoch 126/200\n",
      "208/208 [==============================] - 0s 68us/sample - loss: 0.4876 - mse: 0.2807 - mae: 0.4876 - mape: 89.5048\n",
      "Epoch 127/200\n",
      "208/208 [==============================] - 0s 74us/sample - loss: 0.4868 - mse: 0.2800 - mae: 0.4868 - mape: 89.5170\n",
      "Epoch 128/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.4861 - mse: 0.2792 - mae: 0.4861 - mape: 89.4864\n",
      "Epoch 129/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.4853 - mse: 0.2785 - mae: 0.4853 - mape: 89.4617\n",
      "Epoch 130/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.4846 - mse: 0.2777 - mae: 0.4846 - mape: 89.4475\n",
      "Epoch 131/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.4839 - mse: 0.2770 - mae: 0.4839 - mape: 89.4608\n",
      "Epoch 132/200\n",
      "208/208 [==============================] - 0s 62us/sample - loss: 0.4831 - mse: 0.2762 - mae: 0.4831 - mape: 89.4237\n",
      "Epoch 133/200\n",
      "208/208 [==============================] - 0s 62us/sample - loss: 0.4824 - mse: 0.2755 - mae: 0.4824 - mape: 89.4226\n",
      "Epoch 134/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.4816 - mse: 0.2747 - mae: 0.4816 - mape: 89.4058\n",
      "Epoch 135/200\n",
      "208/208 [==============================] - 0s 71us/sample - loss: 0.4809 - mse: 0.2740 - mae: 0.4809 - mape: 89.3734\n",
      "Epoch 136/200\n",
      "208/208 [==============================] - ETA: 0s - loss: 0.4828 - mse: 0.2791 - mae: 0.4828 - mape: 96.059 - 0s 71us/sample - loss: 0.4801 - mse: 0.2732 - mae: 0.4801 - mape: 89.3738\n",
      "Epoch 137/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.4794 - mse: 0.2725 - mae: 0.4794 - mape: 89.3820\n",
      "Epoch 138/200\n",
      "208/208 [==============================] - 0s 66us/sample - loss: 0.4786 - mse: 0.2717 - mae: 0.4786 - mape: 89.3578\n",
      "Epoch 139/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.4779 - mse: 0.2710 - mae: 0.4779 - mape: 89.3595\n",
      "Epoch 140/200\n",
      "208/208 [==============================] - 0s 71us/sample - loss: 0.4771 - mse: 0.2703 - mae: 0.4771 - mape: 89.3439\n",
      "Epoch 141/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.4764 - mse: 0.2695 - mae: 0.4764 - mape: 89.2684\n",
      "Epoch 142/200\n",
      "208/208 [==============================] - 0s 77us/sample - loss: 0.4756 - mse: 0.2688 - mae: 0.4756 - mape: 89.2829\n",
      "Epoch 143/200\n",
      "208/208 [==============================] - 0s 85us/sample - loss: 0.4749 - mse: 0.2680 - mae: 0.4749 - mape: 89.2751\n",
      "Epoch 144/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.4741 - mse: 0.2673 - mae: 0.4741 - mape: 89.2581\n",
      "Epoch 145/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.4734 - mse: 0.2665 - mae: 0.4734 - mape: 89.2426\n",
      "Epoch 146/200\n",
      "208/208 [==============================] - 0s 70us/sample - loss: 0.4726 - mse: 0.2658 - mae: 0.4726 - mape: 89.2896\n",
      "Epoch 147/200\n",
      "208/208 [==============================] - 0s 69us/sample - loss: 0.4718 - mse: 0.2650 - mae: 0.4718 - mape: 89.2313\n",
      "Epoch 148/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.4711 - mse: 0.2643 - mae: 0.4711 - mape: 89.2123\n",
      "Epoch 149/200\n",
      "208/208 [==============================] - 0s 56us/sample - loss: 0.4703 - mse: 0.2636 - mae: 0.4703 - mape: 89.2055\n",
      "Epoch 150/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.4696 - mse: 0.2628 - mae: 0.4696 - mape: 89.1920\n",
      "Epoch 151/200\n",
      "208/208 [==============================] - 0s 62us/sample - loss: 0.4688 - mse: 0.2621 - mae: 0.4688 - mape: 89.2115\n",
      "Epoch 152/200\n",
      "208/208 [==============================] - 0s 61us/sample - loss: 0.4680 - mse: 0.2613 - mae: 0.4680 - mape: 89.1903\n",
      "Epoch 153/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.4672 - mse: 0.2606 - mae: 0.4672 - mape: 89.1417\n",
      "Epoch 154/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.4665 - mse: 0.2598 - mae: 0.4665 - mape: 89.1661\n",
      "Epoch 155/200\n",
      "208/208 [==============================] - 0s 72us/sample - loss: 0.4657 - mse: 0.2591 - mae: 0.4657 - mape: 89.1200\n",
      "Epoch 156/200\n",
      "208/208 [==============================] - 0s 65us/sample - loss: 0.4649 - mse: 0.2583 - mae: 0.4649 - mape: 89.1116\n",
      "Epoch 157/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.4641 - mse: 0.2576 - mae: 0.4641 - mape: 89.1043\n",
      "Epoch 158/200\n",
      "208/208 [==============================] - 0s 69us/sample - loss: 0.4634 - mse: 0.2568 - mae: 0.4634 - mape: 89.0854\n",
      "Epoch 159/200\n",
      "208/208 [==============================] - 0s 61us/sample - loss: 0.4626 - mse: 0.2561 - mae: 0.4626 - mape: 89.0780\n",
      "Epoch 160/200\n",
      "208/208 [==============================] - 0s 58us/sample - loss: 0.4618 - mse: 0.2554 - mae: 0.4618 - mape: 89.0735\n",
      "Epoch 161/200\n",
      "208/208 [==============================] - 0s 56us/sample - loss: 0.4610 - mse: 0.2546 - mae: 0.4610 - mape: 89.0500\n",
      "Epoch 162/200\n",
      "208/208 [==============================] - 0s 58us/sample - loss: 0.4602 - mse: 0.2539 - mae: 0.4602 - mape: 89.0724\n",
      "Epoch 163/200\n",
      "208/208 [==============================] - 0s 62us/sample - loss: 0.4595 - mse: 0.2531 - mae: 0.4595 - mape: 89.0814\n",
      "Epoch 164/200\n",
      "208/208 [==============================] - 0s 68us/sample - loss: 0.4587 - mse: 0.2524 - mae: 0.4587 - mape: 89.0691\n",
      "Epoch 165/200\n",
      "208/208 [==============================] - 0s 75us/sample - loss: 0.4579 - mse: 0.2516 - mae: 0.4579 - mape: 89.0487\n",
      "Epoch 166/200\n",
      "208/208 [==============================] - 0s 85us/sample - loss: 0.4571 - mse: 0.2509 - mae: 0.4571 - mape: 89.0153\n",
      "Epoch 167/200\n",
      "208/208 [==============================] - 0s 68us/sample - loss: 0.4563 - mse: 0.2502 - mae: 0.4563 - mape: 88.9891\n",
      "Epoch 168/200\n",
      "208/208 [==============================] - 0s 65us/sample - loss: 0.4556 - mse: 0.2494 - mae: 0.4556 - mape: 88.9999\n",
      "Epoch 169/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.4548 - mse: 0.2487 - mae: 0.4548 - mape: 88.9461\n",
      "Epoch 170/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.4540 - mse: 0.2480 - mae: 0.4540 - mape: 88.9839\n",
      "Epoch 171/200\n",
      "208/208 [==============================] - 0s 59us/sample - loss: 0.4532 - mse: 0.2472 - mae: 0.4532 - mape: 88.9749\n",
      "Epoch 172/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.4524 - mse: 0.2465 - mae: 0.4524 - mape: 88.9394\n",
      "Epoch 173/200\n",
      "208/208 [==============================] - 0s 80us/sample - loss: 0.4516 - mse: 0.2457 - mae: 0.4516 - mape: 88.9367\n",
      "Epoch 174/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.4508 - mse: 0.2450 - mae: 0.4508 - mape: 88.9180\n",
      "Epoch 175/200\n",
      "208/208 [==============================] - 0s 62us/sample - loss: 0.4500 - mse: 0.2443 - mae: 0.4500 - mape: 88.9105\n",
      "Epoch 176/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.4492 - mse: 0.2435 - mae: 0.4492 - mape: 88.9175\n",
      "Epoch 177/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.4484 - mse: 0.2428 - mae: 0.4484 - mape: 88.8730\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 0s 61us/sample - loss: 0.4476 - mse: 0.2421 - mae: 0.4476 - mape: 88.8994\n",
      "Epoch 179/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.4468 - mse: 0.2413 - mae: 0.4468 - mape: 88.8626\n",
      "Epoch 180/200\n",
      "208/208 [==============================] - 0s 75us/sample - loss: 0.4460 - mse: 0.2406 - mae: 0.4460 - mape: 88.8468\n",
      "Epoch 181/200\n",
      "208/208 [==============================] - 0s 68us/sample - loss: 0.4452 - mse: 0.2399 - mae: 0.4452 - mape: 88.8477\n",
      "Epoch 182/200\n",
      "208/208 [==============================] - 0s 67us/sample - loss: 0.4444 - mse: 0.2391 - mae: 0.4444 - mape: 88.8155\n",
      "Epoch 183/200\n",
      "208/208 [==============================] - 0s 68us/sample - loss: 0.4436 - mse: 0.2384 - mae: 0.4436 - mape: 88.8140\n",
      "Epoch 184/200\n",
      "208/208 [==============================] - 0s 70us/sample - loss: 0.4428 - mse: 0.2376 - mae: 0.4428 - mape: 88.8029\n",
      "Epoch 185/200\n",
      "208/208 [==============================] - 0s 62us/sample - loss: 0.4420 - mse: 0.2369 - mae: 0.4420 - mape: 88.7879\n",
      "Epoch 186/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.4412 - mse: 0.2362 - mae: 0.4412 - mape: 88.7579\n",
      "Epoch 187/200\n",
      "208/208 [==============================] - 0s 68us/sample - loss: 0.4404 - mse: 0.2354 - mae: 0.4404 - mape: 88.7594\n",
      "Epoch 188/200\n",
      "208/208 [==============================] - 0s 78us/sample - loss: 0.4396 - mse: 0.2347 - mae: 0.4396 - mape: 88.7501\n",
      "Epoch 189/200\n",
      "208/208 [==============================] - 0s 69us/sample - loss: 0.4387 - mse: 0.2340 - mae: 0.4387 - mape: 88.7341\n",
      "Epoch 190/200\n",
      "208/208 [==============================] - 0s 65us/sample - loss: 0.4379 - mse: 0.2332 - mae: 0.4379 - mape: 88.7186\n",
      "Epoch 191/200\n",
      "208/208 [==============================] - 0s 70us/sample - loss: 0.4371 - mse: 0.2325 - mae: 0.4371 - mape: 88.7352\n",
      "Epoch 192/200\n",
      "208/208 [==============================] - 0s 71us/sample - loss: 0.4363 - mse: 0.2318 - mae: 0.4363 - mape: 88.6822\n",
      "Epoch 193/200\n",
      "208/208 [==============================] - 0s 63us/sample - loss: 0.4355 - mse: 0.2310 - mae: 0.4355 - mape: 88.7009\n",
      "Epoch 194/200\n",
      "208/208 [==============================] - 0s 58us/sample - loss: 0.4347 - mse: 0.2303 - mae: 0.4347 - mape: 88.6657\n",
      "Epoch 195/200\n",
      "208/208 [==============================] - 0s 64us/sample - loss: 0.4338 - mse: 0.2296 - mae: 0.4338 - mape: 88.6525\n",
      "Epoch 196/200\n",
      "208/208 [==============================] - 0s 61us/sample - loss: 0.4330 - mse: 0.2289 - mae: 0.4330 - mape: 88.6314\n",
      "Epoch 197/200\n",
      "208/208 [==============================] - 0s 57us/sample - loss: 0.4322 - mse: 0.2281 - mae: 0.4322 - mape: 88.6524\n",
      "Epoch 198/200\n",
      "208/208 [==============================] - 0s 62us/sample - loss: 0.4314 - mse: 0.2274 - mae: 0.4314 - mape: 88.6115\n",
      "Epoch 199/200\n",
      "208/208 [==============================] - 0s 82us/sample - loss: 0.4306 - mse: 0.2267 - mae: 0.4306 - mape: 88.6325\n",
      "Epoch 200/200\n",
      "208/208 [==============================] - 0s 66us/sample - loss: 0.4297 - mse: 0.2259 - mae: 0.4297 - mape: 88.5807\n",
      "-----------------------\n",
      "Training Deep Zero-Sets\n",
      "-----------------------\n",
      "Train on 208 samples\n",
      "208/208 [==============================] - 0s 1ms/sample - loss: 0.6596 - accuracy: 1.0000\n",
      "-----------------------------------\n",
      "Computing Final Performance Metrics\n",
      "-----------------------------------\n",
      "           train        test\n",
      "MAE     0.429223    0.427831\n",
      "MSE     0.225504    0.228054\n",
      "MAPE  321.829684  320.379808\n",
      "     L-time    P-time  N_params_expt  AIC-like    Eff  N. Parts\n",
      "0  0.770885  5.190553            240   481.698  2.345         1\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "# ---- Getting Benchmarks ---- #\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "Training classifier and generating partition!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a8f7aded3872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mN_parts_possibilities_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_parts_possibilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minplicit_N_parts_loop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Run Algos. 1+2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mperformance_Architope_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArchitope_Model_Complexity_full_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_parts_Generated_by_Algo_2_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_params_architope_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_neurons_subPatterns_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_neurons_deep_Zero_Sets_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_mean_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance_architope_ffNN_logistic_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_params_best_logistic_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_PCNNs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_parts_possibilities_loop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#     performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, height_mean_loop = Ablate_PCNNs(q_implicit_N_parts_loop,data_y,X_train,X_test,y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ffb8c3bd5439>\u001b[0m in \u001b[0;36mget_PCNNs\u001b[0;34m(N_parts, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Train Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_labels_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m#### Write Predicted Class(es)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bpcnn/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bpcnn/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bpcnn/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m   1375\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "# Initialize \n",
    "# q_implicit_N_parts_possibilities = np.linspace(min_parts_threshold,max_parts_threshold,N_plot_finess)\n",
    "N_parts_possibilities = np.unique(np.round(np.linspace(N_min_parts,N_max_plots,num=N_plot_finess))).astype(int)\n",
    "# Custom: N_parts_possibilities = np.array([1,2,3,4,5,8]); N_plot_finess = len(N_parts_possibilities)\n",
    "\n",
    "# Get Performance Metric\n",
    "for inplicit_N_parts_loop in range(len(N_parts_possibilities)):\n",
    "    ### UPDATE USER ###\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    print('Ablation Completion Percentage:',(inplicit_N_parts_loop/N_plot_finess))\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    \n",
    "    # Implicitly Set: Current Number of Parts\n",
    "#     q_implicit_N_parts_loop = q_implicit_N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    N_parts_possibilities_loop = N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    # Run Algos. 1+2\n",
    "    performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, N_neurons_subPatterns_loop, N_neurons_deep_Zero_Sets_loop, height_mean_loop, performance_architope_ffNN_logistic_loop, N_params_best_logistic_loop = get_PCNNs(N_parts_possibilities_loop,X_train,y_train,X_test,y_test)\n",
    "#     performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, height_mean_loop = Ablate_PCNNs(q_implicit_N_parts_loop,data_y,X_train,X_test,y_test)\n",
    "    # Reshape\n",
    "    performance_Architope_loop = performance_Architope_loop.to_numpy().reshape([3,2,1])\n",
    "    Architope_Model_Complexity_full_loop = Architope_Model_Complexity_full_loop.to_numpy().reshape([1,6,1])\n",
    "\n",
    "    # Record\n",
    "    if inplicit_N_parts_loop == 0:\n",
    "        # Don't count partitioner if only one parts is active!\n",
    "        if N_parts_possibilities_loop <= 1:\n",
    "            Architope_Model_Complexity_full_loop[:,1] = Architope_Model_Complexity_full_loop[:,0]\n",
    "            N_neurons_deep_Zero_Sets_loop = 0\n",
    "        # Record Model Complexities Otherwise    \n",
    "        performance_Architope_history = performance_Architope_loop\n",
    "        Architope_Model_Complexity_history = Architope_Model_Complexity_full_loop\n",
    "        N_parts_Generated_by_Algo_2_history = N_parts_Generated_by_Algo_2_loop\n",
    "        N_params_subPatterns_hist = N_neurons_subPatterns_loop\n",
    "        N_neurons_deep_Zero_Sets_hist = N_neurons_deep_Zero_Sets_loop\n",
    "        N_params_architope_hist = N_neurons_deep_Zero_Sets_loop + N_neurons_subPatterns_loop\n",
    "        height_mean_hist = height_mean_loop\n",
    "        N_neurons_per_input = N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))\n",
    "    else:\n",
    "        performance_Architope_history = np.concatenate((performance_Architope_history,performance_Architope_loop),axis=2)\n",
    "        Architope_Model_Complexity_history = np.concatenate((Architope_Model_Complexity_history,Architope_Model_Complexity_full_loop),axis=2)\n",
    "        N_parts_Generated_by_Algo_2_history = np.append(N_parts_Generated_by_Algo_2_history,N_parts_Generated_by_Algo_2_loop)\n",
    "        N_params_architope_hist = np.append(N_params_architope_hist,N_params_architope_loop)\n",
    "        N_params_subPatterns_hist = np.append(N_params_subPatterns_hist,N_neurons_subPatterns_loop)\n",
    "        N_neurons_deep_Zero_Sets_hist = np.append(N_neurons_deep_Zero_Sets_hist,N_neurons_deep_Zero_Sets_loop)\n",
    "        height_mean_hist = np.append(height_mean_hist,height_mean_loop)\n",
    "        N_neurons_per_input = np.append(N_neurons_per_input,(N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))))\n",
    "\n",
    "# Cleanup\n",
    "## Randomization may produce duplicates; we remove these with the following snippet:\n",
    "get_unique_entries = np.unique(N_parts_Generated_by_Algo_2_history, return_index=True)[1]\n",
    "N_parts_Generated_by_Algo_2_history_report = N_parts_Generated_by_Algo_2_history[get_unique_entries]\n",
    "\n",
    "# Write\n",
    "## Prediction Qualities\n",
    "performance_Architope_history_report_MAE_train = (performance_Architope_history[0,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MAE_test = (performance_Architope_history[0,1,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_train = (performance_Architope_history[1,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_test = (performance_Architope_history[1,1,:])[get_unique_entries]\n",
    "## Model Complexities\n",
    "L_Times = (Architope_Model_Complexity_history[:,1].reshape(-1,))[get_unique_entries]\n",
    "P_Times = (Architope_Model_Complexity_history[:,0].reshape(-1,))[get_unique_entries]\n",
    "N_Params = (N_params_architope_hist.reshape(-1,))[get_unique_entries]\n",
    "mean_subpattern_widths_hist = (height_mean_hist.reshape(-1,))[get_unique_entries]\n",
    "AIC_Like = (Architope_Model_Complexity_history[:,3].reshape(-1,))[get_unique_entries]\n",
    "Eff = (Architope_Model_Complexity_history[:,4].reshape(-1,))[get_unique_entries]\n",
    "N_neurons_per_input = (N_neurons_per_input.reshape(-1,))[get_unique_entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Feedforward Neural Network (ffNN) Benchmark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Model complexities for ffNNs\n",
    "P_time_ffNN = P_Times[0]\n",
    "L_time_ffNN = P_Times[0]\n",
    "Width_ffNN = height_mean_hist[0]\n",
    "# For: Plots\n",
    "MAE_ffNN = np.repeat(performance_Architope_history_report_MAE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "MSE_ffNN = np.repeat(performance_Architope_history_report_MSE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "L_times_ffNN_plot = np.repeat(L_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "P_times_ffNN_plot = np.repeat(P_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_per_input_ffNN = np.repeat(N_neurons_per_input[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "Width_neurons_ffNN = np.repeat(mean_subpattern_widths_hist[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_ffNN = np.repeat(N_Params[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "# Record in Table\n",
    "ffNN_Model_Complexity = pd.DataFrame({'L-time': [L_time_ffNN],\n",
    "                                               'P-time':[P_time_ffNN],\n",
    "                                               'N_params_expt': [N_neurons_ffNN],\n",
    "                                               'AIC-like': [0],\n",
    "                                               'Eff': [0],\n",
    "                                               'N. Parts':[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Plots\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MSE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MSE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MSE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "# Add Legend\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MSE_test___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MAE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MAE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MAE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MAE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MAE___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"L-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"L-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_L_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"P-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"P-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_P_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"N. Params\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"N. Params\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_Params,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_N_Params___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Active Neurons Per Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Active Neurons per. Input\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Active Neurons per. Input\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_per_input,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Active_Neurons_per_input___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Widths for Sub-Pattern Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Mean Subpattern Widths\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Mean Subpattern Widths\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         Width_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Mean_Widths___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCNN with Logistic-Classifier Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_architope_ffNN_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
