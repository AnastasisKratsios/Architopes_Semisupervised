{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation: Semi-Supervised Architope - for Reviews\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"PC-NNs\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 0.9\n",
    "min_width = 50\n",
    "# Ablation Finess\n",
    "N_plot_finess = 5\n",
    "# min_parts_threshold = .001; max_parts_threshold = 0.9\n",
    "N_min_parts = 1; N_max_plots = 4\n",
    "Tied_Neurons_Q = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------#\n",
    "# Only For Motivational Example Only #\n",
    "#------------------------------------#\n",
    "## Hyperparameters\n",
    "percentage_in_row = .25\n",
    "N = 10**4\n",
    "\n",
    "def f_1(x):\n",
    "    return x\n",
    "def f_2(x):\n",
    "    return x**2\n",
    "x_0 = 0\n",
    "x_end = 1\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "#================================================#\n",
      " Training Datasize: 539 and test datasize: 60.  \n",
      "#================================================#\n",
      "#================================================#\n",
      " Training Datasize: 191 and test datasize: 21.  \n",
      "#================================================#\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "if Option_Function != \"Motivational_Example\": \n",
    "    exec(open('Financial_Data_Preprocessor.py').read())\n",
    "else:\n",
    "    print(1)\n",
    "    exec(open('Motivational_Example.py').read())\n",
    "    print(\"Training Data size: \",X_train.shape[0])\n",
    "# Import time separately\n",
    "import time\n",
    "\n",
    "# TEMP\n",
    "# import pickle_compat\n",
    "# pickle_compat.patch()\n",
    "# param_grid_Vanilla_Nets['input_dim']=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit Partion Builder:\n",
    "Implements exactly Algorithm 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(X_in,\n",
    "                               y_in,\n",
    "                               N_parts_to_get=4):\n",
    "\n",
    "    # Compute Size of each part\n",
    "    size_part_reference = int(round(X_in.shape[0]/N_parts_to_get))\n",
    "\n",
    "    # Apply random bijection #\n",
    "    #------------------------#\n",
    "    ## Get random bijection indices\n",
    "    random_bijection_indices = np.random.choice(range(X_in.shape[0]),size=X_in.shape[0], replace=False)\n",
    "    ## Apply random bijections\n",
    "    X_in_shuffled = X_in[random_bijection_indices,:]\n",
    "    y_in_shuffled = y_in[random_bijection_indices,:]\n",
    "\n",
    "    # Initialize Lists #\n",
    "    #------------------#\n",
    "    X_parts = []\n",
    "    y_parts = []\n",
    "\n",
    "    for i_th_part_to_get in range(N_parts_to_get):\n",
    "        # Build random balls #\n",
    "        #--------------------#\n",
    "        ## Sample random radius\n",
    "        size_part = int(np.maximum(1,np.round(size_part_reference*np.random.uniform(low=.5,high=1.5,size=1)[0])))\n",
    "        ## Sample random point\n",
    "        X_center_loop_index = np.random.choice(range(X_in_shuffled.shape[0]),size=1, replace=False)\n",
    "        X_center_loop = X_in_shuffled[X_center_loop_index,:]\n",
    "        ## Compute Typical Distances from Center\n",
    "        distances_loop = X_center_loop-X_in_shuffled\n",
    "        distances_loop = np.linalg.norm(distances_loop, axis=1)\n",
    "\n",
    "        # Remove Random Ball from Dataset\n",
    "        if size_part <= len(distances_loop):\n",
    "            ## Identify indices\n",
    "            indices_smallest_to_random_ball = np.argsort(distances_loop)[:size_part]\n",
    "        else:\n",
    "            print('Final Loop')\n",
    "            indices_smallest_to_random_ball = np.array(range(X_in_shuffled.shape[0]))\n",
    "        ## Extract Parts\n",
    "        X_current_part_loop = X_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        y_current_part_loop = y_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        ## Append to List of Parts\n",
    "        X_parts.append(X_current_part_loop)\n",
    "        y_parts.append(y_current_part_loop)\n",
    "\n",
    "        # Remove Selected Entries From Array #\n",
    "        #------------------------------------#\n",
    "        X_in_shuffled = np.delete(X_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "        y_in_shuffled = np.delete(y_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "\n",
    "        # Failsafe if procedure has terminated\n",
    "        if X_in_shuffled.shape[0] == 0:\n",
    "            print('breaking early')\n",
    "            break\n",
    "    # Count Number of Parts Generated        \n",
    "    N_parts_generated = len(X_parts)\n",
    "    # Output Parts\n",
    "    return X_parts, y_parts, N_parts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCNNs(N_parts,X_train,y_train,X_test,y_test):\n",
    "\n",
    "    # Initialization(s) #\n",
    "    #-------------------#\n",
    "    N_neurons = 0\n",
    "    L_timer = 0\n",
    "    P_timer = 0\n",
    "    Mean_Width_Subnetworks = 0\n",
    "\n",
    "    # Partitioner Begin #\n",
    "    #-------------------#\n",
    "    import time\n",
    "    partitioning_time_begin = time.time()\n",
    "    print('-------------------------------------------------------')\n",
    "    print('Randomly Initialized Parts - Via Randomized Algorithm 2')\n",
    "    print('-------------------------------------------------------')\n",
    "    X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(X_train.to_numpy(),\n",
    "                                                                                         y_train.reshape(-1,1),\n",
    "                                                                                         N_parts)\n",
    "    partitioning_time = time.time() - partitioning_time_begin\n",
    "    print('The_parts_listhe number of parts are: ' + str(N_parts_Generated_by_Algo_2)+'.')\n",
    "    ############# Partitioner End ########\n",
    "\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Training Sub-Networks on Each Randomly Generated Part')\n",
    "    print('-----------------------------------------------------')\n",
    "    # Time-Elapse (Start) for Training on Each Part #\n",
    "    PCNN_timer = time.time(); PCNN_timer = -math.inf; N_params_Architope = 0; N_params_tally = 0\n",
    "    # Remove Eager Execution Error(s)\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    # Automatically Initialize Correct Input/Output Dimension(s)\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]; param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "    # Decide if/or not to tie neuron numbers of sub-patterns together\n",
    "    if Tied_Neurons_Q == True:\n",
    "        param_grid_Vanilla_Nets['height'] = [int(np.maximum(round(param_grid_Vanilla_Nets['height'][0]/N_parts),min_width))]\n",
    "        param_grid_Deep_Classifier['height'] = [int(np.maximum(round(param_grid_Deep_Classifier['height'][0]/N_parts),min_width))]\n",
    "\n",
    "    for current_part in range(N_parts_Generated_by_Algo_2):\n",
    "        # Update User #\n",
    "        #-------------#\n",
    "        print('-----------------------------------------------------------')\n",
    "        print('Currently Training Part: '+str(current_part)+'/'+str(N_parts_Generated_by_Algo_2 )+'Total Parts.')\n",
    "        print('-----------------------------------------------------------')\n",
    "\n",
    "        # Timer for Part\n",
    "        part_training_timer = time.time()\n",
    "        # Get Data for Sub-Pattern\n",
    "        X_loop = pd.DataFrame(X_parts_list[current_part])\n",
    "        y_loop = (y_parts_list[current_part]).reshape(-1,)\n",
    "        # Train ffNN\n",
    "        y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN(n_folds = 4, \n",
    "                                                                              n_jobs = n_jobs,\n",
    "                                                                              n_iter = n_iter, \n",
    "                                                                              param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                              X_train= X_loop, \n",
    "                                                                              y_train=y_loop,\n",
    "                                                                              X_test_partial=X_train,\n",
    "                                                                              X_test=X_test,\n",
    "                                                                              NOCV=True)\n",
    "        # Reshape y\n",
    "        ## Training\n",
    "        y_train.shape = (y_train.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop.shape = (y_hat_part_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        ## Testing\n",
    "        y_test.shape = (y_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop_test.shape = (y_hat_part_loop_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "\n",
    "        # Append predictions to data-frames\n",
    "        ## If first prediction we initialize data-frames\n",
    "        if current_part==0:\n",
    "            # Register quality\n",
    "            training_quality = np.array(np.abs(y_hat_part_loop-y_train)).reshape(y_hat_part_loop.shape[0],1)\n",
    "\n",
    "            # Save Predictions\n",
    "            predictions_train = y_hat_part_loop.reshape(y_hat_part_loop.shape[0],1)\n",
    "            predictions_test = y_hat_part_loop_test.reshape(y_hat_part_loop_test.shape[0],1)\n",
    "\n",
    "\n",
    "        ## If not first prediction we append to already initialized dataframes\n",
    "        else:\n",
    "        # Register Best Scores\n",
    "            #----------------------#\n",
    "            # Write Predictions \n",
    "            # Save Predictions\n",
    "            y_hat_train_loop = y_hat_part_loop.reshape(predictions_train.shape[0],1)\n",
    "            predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "            y_hat_test_loop = y_hat_part_loop_test.reshape(predictions_test.shape[0],1)\n",
    "            predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "\n",
    "            # Evaluate Errors #\n",
    "            #-----------------#\n",
    "            # Training\n",
    "            prediction_errors = np.abs(y_hat_train_loop-y_train)\n",
    "            training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "\n",
    "        #==============================#\n",
    "        # Update Performance Metric(s) #\n",
    "        #==============================#\n",
    "        part_training_timer = time.time() - part_training_timer\n",
    "        # L-Time\n",
    "        L_timer += partitioning_time\n",
    "        # P-Time\n",
    "        P_timer = max(P_timer,part_training_timer)\n",
    "        # N. Params\n",
    "        N_neurons += N_neurons_PCNN_loop\n",
    "        # Mean Width for Sub-Network(s)\n",
    "        Mean_Width_Subnetworks += param_grid_Vanilla_Nets['height'][0]\n",
    "\n",
    "    # Take Mean of Width(s)\n",
    "    Mean_Width_Subnetworks = Mean_Width_Subnetworks/N_parts_Generated_by_Algo_2\n",
    "    print('-----------------------')\n",
    "    print('Training Deep Zero-Sets')\n",
    "    print('-----------------------')\n",
    "\n",
    "\n",
    "    # Time Elapsed for Training Deep Zero-Sets\n",
    "    Deep_Zero_Sets_timer = time.time()\n",
    "\n",
    "    ## Initialize Classes Labels\n",
    "    partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "    partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "    ## Build Classes\n",
    "    for part_column_i in range(1,(training_quality.shape[1])):\n",
    "        partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                               (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                              ],axis=1)\n",
    "    ## Convert to integers\n",
    "    partition_labels_training = partition_labels_training+0\n",
    "    ## Train simple deep classifier\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter =n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train.values, \n",
    "                                                                                                        y_train = partition_labels_training.values,\n",
    "                                                                                                        X_test = X_test.values)\n",
    "    # Get Binary Classes (Discontinuous Unit)\n",
    "    ## Training Set\n",
    "    predicted_classes_train = ((predicted_classes_train>.5)*1).astype(int)\n",
    "    ## Testing Set\n",
    "    predicted_classes_test = ((predicted_classes_test > .5)*1).astype(int)\n",
    "    # Get PC-NN Prediction(s)\n",
    "    ## Train\n",
    "    PCNN_prediction_y_train = (predictions_train*predicted_classes_train).sum(axis=1)\n",
    "    ## Test\n",
    "    PCNN_prediction_y_test = (predictions_test*predicted_classes_test).sum(axis=1)\n",
    "\n",
    "    # End Timer\n",
    "    Deep_Zero_Sets_timer = time.time() - Deep_Zero_Sets_timer\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Computing Final Performance Metrics')\n",
    "    print('-----------------------------------')\n",
    "    # Time-Elapsed Training Deep Classifier\n",
    "\n",
    "    # Update Times\n",
    "    L_timer +=Deep_Zero_Sets_timer\n",
    "    P_timer +=Deep_Zero_Sets_timer\n",
    "    # Update Number of Neurons Used\n",
    "    N_neurons_subPatterns = N_neurons\n",
    "    N_neurons_deep_Zero_Sets = (param_grid_Deep_Classifier['height'][0])*(param_grid_Deep_Classifier['depth'][0])\n",
    "    N_neurons = N_neurons_deep_Zero_Sets + N_neurons_subPatterns\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Peformance\n",
    "    performance_PCNN = reporter(y_train_hat_in=PCNN_prediction_y_train,y_test_hat_in=PCNN_prediction_y_test,\n",
    "                                y_train_in=y_train,\n",
    "                                y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_PCNN.to_latex((results_tables_path+\"PCNN_full_performance.tex\"))\n",
    "\n",
    "    # Update User\n",
    "    print(performance_PCNN)\n",
    "\n",
    "    ### Model Complexity/Efficiency Metrics\n",
    "    # Build AIC-like Metric #\n",
    "    #-----------------------#\n",
    "    AIC_like = 2*(N_neurons - np.log((performance_PCNN['test']['MAE'])))\n",
    "    AIC_like = np.round(AIC_like,3)\n",
    "    Efficiency = np.log(N_neurons) *(performance_PCNN['test']['MAE'])\n",
    "    Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "    # Build Table #\n",
    "    #-------------#\n",
    "    PCNN_Model_Complexity = pd.DataFrame({'L-time': [L_timer],\n",
    "                                               'P-time':[P_timer],\n",
    "                                               'N_params_expt': [N_neurons],\n",
    "                                               'AIC-like': [AIC_like],\n",
    "                                               'Eff': [Efficiency],\n",
    "                                               'N. Parts':[N_parts_Generated_by_Algo_2]})\n",
    "\n",
    "\n",
    "    # Write Required Training Time(s)\n",
    "    PCNN_Model_Complexity.to_latex((results_tables_path+\"PCNN_full_model_complexities.tex\"))\n",
    "\n",
    "    #--------------======---------------#\n",
    "    # Display Required Training Time(s) #\n",
    "    #--------------======---------------#\n",
    "    print(PCNN_Model_Complexity)\n",
    "    \n",
    "    \n",
    "    # Return Output(s)\n",
    "    return performance_PCNN, PCNN_Model_Complexity, N_parts_Generated_by_Algo_2, N_neurons, N_neurons_subPatterns,N_neurons_deep_Zero_Sets, Mean_Width_Subnetworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Perform Ablation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Ablation Completion Percentage: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "-------------------------------------------------------\n",
      "Randomly Initialized Parts - Via Randomized Algorithm 2\n",
      "-------------------------------------------------------\n",
      "The_parts_listhe number of parts are: 1.\n",
      "-----------------------------------------------------\n",
      "Training Sub-Networks on Each Randomly Generated Part\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 0/1Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Train on 174 samples\n",
      "Epoch 1/75\n",
      "174/174 [==============================] - 0s 3ms/sample - loss: 395.1528 - mse: 408044.6875 - mae: 395.1528 - mape: 101.5475\n",
      "Epoch 2/75\n",
      "174/174 [==============================] - 0s 151us/sample - loss: 395.1475 - mse: 408037.1562 - mae: 395.1475 - mape: 101.5435\n",
      "Epoch 3/75\n",
      "174/174 [==============================] - 0s 153us/sample - loss: 395.1432 - mse: 408031.2500 - mae: 395.1432 - mape: 101.5424\n",
      "Epoch 4/75\n",
      "174/174 [==============================] - 0s 141us/sample - loss: 395.1381 - mse: 408024.6875 - mae: 395.1381 - mape: 101.5411\n",
      "Epoch 5/75\n",
      "174/174 [==============================] - 0s 151us/sample - loss: 395.1330 - mse: 408018.0312 - mae: 395.1330 - mape: 101.5363\n",
      "Epoch 6/75\n",
      "174/174 [==============================] - 0s 124us/sample - loss: 395.1283 - mse: 408011.2500 - mae: 395.1283 - mape: 101.5346\n",
      "Epoch 7/75\n",
      "174/174 [==============================] - 0s 131us/sample - loss: 395.1232 - mse: 408003.7812 - mae: 395.1232 - mape: 101.5354\n",
      "Epoch 8/75\n",
      "174/174 [==============================] - 0s 520us/sample - loss: 395.1182 - mse: 407997.5312 - mae: 395.1181 - mape: 101.5332\n",
      "Epoch 9/75\n",
      "174/174 [==============================] - 0s 161us/sample - loss: 395.1129 - mse: 407989.8750 - mae: 395.1129 - mape: 101.5338\n",
      "Epoch 10/75\n",
      "174/174 [==============================] - 0s 592us/sample - loss: 395.1076 - mse: 407982.5625 - mae: 395.1076 - mape: 101.5300\n",
      "Epoch 11/75\n",
      "174/174 [==============================] - 0s 160us/sample - loss: 395.1027 - mse: 407976.0938 - mae: 395.1027 - mape: 101.5285\n",
      "Epoch 12/75\n",
      "174/174 [==============================] - 0s 353us/sample - loss: 395.0978 - mse: 407969.2500 - mae: 395.0977 - mape: 101.5277\n",
      "Epoch 13/75\n",
      "174/174 [==============================] - 0s 215us/sample - loss: 395.0925 - mse: 407961.7812 - mae: 395.0925 - mape: 101.5301\n",
      "Epoch 14/75\n",
      "174/174 [==============================] - 0s 1ms/sample - loss: 395.0869 - mse: 407954.5625 - mae: 395.0869 - mape: 101.5274\n",
      "Epoch 15/75\n",
      "174/174 [==============================] - 0s 176us/sample - loss: 395.0822 - mse: 407947.7812 - mae: 395.0822 - mape: 101.5253\n",
      "Epoch 16/75\n",
      "174/174 [==============================] - 0s 496us/sample - loss: 395.0769 - mse: 407940.3125 - mae: 395.0769 - mape: 101.5222\n",
      "Epoch 17/75\n",
      "174/174 [==============================] - 0s 208us/sample - loss: 395.0716 - mse: 407933.3438 - mae: 395.0716 - mape: 101.5191\n",
      "Epoch 18/75\n",
      "174/174 [==============================] - 0s 438us/sample - loss: 395.0661 - mse: 407925.9688 - mae: 395.0661 - mape: 101.5184\n",
      "Epoch 19/75\n",
      "174/174 [==============================] - 0s 150us/sample - loss: 395.0611 - mse: 407918.6562 - mae: 395.0611 - mape: 101.5169\n",
      "Epoch 20/75\n",
      "174/174 [==============================] - 0s 430us/sample - loss: 395.0558 - mse: 407911.4062 - mae: 395.0558 - mape: 101.5148\n",
      "Epoch 21/75\n",
      "174/174 [==============================] - 0s 147us/sample - loss: 395.0504 - mse: 407904.0312 - mae: 395.0504 - mape: 101.5119\n",
      "Epoch 22/75\n",
      "174/174 [==============================] - 0s 116us/sample - loss: 395.0452 - mse: 407896.8125 - mae: 395.0452 - mape: 101.5106\n",
      "Epoch 23/75\n",
      "174/174 [==============================] - 0s 445us/sample - loss: 395.0403 - mse: 407889.7812 - mae: 395.0403 - mape: 101.5127\n",
      "Epoch 24/75\n",
      "174/174 [==============================] - 0s 110us/sample - loss: 395.0352 - mse: 407882.9062 - mae: 395.0352 - mape: 101.5111\n",
      "Epoch 25/75\n",
      "174/174 [==============================] - 0s 121us/sample - loss: 395.0298 - mse: 407875.5312 - mae: 395.0298 - mape: 101.5091\n",
      "Epoch 26/75\n",
      "174/174 [==============================] - 0s 364us/sample - loss: 395.0250 - mse: 407869.0000 - mae: 395.0250 - mape: 101.5088\n",
      "Epoch 27/75\n",
      "174/174 [==============================] - 0s 116us/sample - loss: 395.0196 - mse: 407861.2500 - mae: 395.0196 - mape: 101.5098\n",
      "Epoch 28/75\n",
      "174/174 [==============================] - 0s 124us/sample - loss: 395.0142 - mse: 407854.4062 - mae: 395.0142 - mape: 101.5068\n",
      "Epoch 29/75\n",
      "174/174 [==============================] - 0s 351us/sample - loss: 395.0093 - mse: 407847.8750 - mae: 395.0093 - mape: 101.5080\n",
      "Epoch 30/75\n",
      "174/174 [==============================] - 0s 189us/sample - loss: 395.0038 - mse: 407840.8750 - mae: 395.0038 - mape: 101.5095\n",
      "Epoch 31/75\n",
      "174/174 [==============================] - 0s 150us/sample - loss: 394.9985 - mse: 407833.6562 - mae: 394.9985 - mape: 101.5072\n",
      "Epoch 32/75\n",
      "174/174 [==============================] - 0s 319us/sample - loss: 394.9930 - mse: 407825.9375 - mae: 394.9930 - mape: 101.5093\n",
      "Epoch 33/75\n",
      "174/174 [==============================] - 0s 112us/sample - loss: 394.9875 - mse: 407818.1250 - mae: 394.9875 - mape: 101.5074\n",
      "Epoch 34/75\n",
      "174/174 [==============================] - 0s 149us/sample - loss: 394.9822 - mse: 407810.8438 - mae: 394.9822 - mape: 101.5057\n",
      "Epoch 35/75\n",
      "174/174 [==============================] - 0s 157us/sample - loss: 394.9770 - mse: 407803.6875 - mae: 394.9771 - mape: 101.5072\n",
      "Epoch 36/75\n",
      "174/174 [==============================] - 0s 367us/sample - loss: 394.9712 - mse: 407795.8125 - mae: 394.9712 - mape: 101.5041\n",
      "Epoch 37/75\n",
      "174/174 [==============================] - 0s 189us/sample - loss: 394.9667 - mse: 407789.7812 - mae: 394.9667 - mape: 101.5059\n",
      "Epoch 38/75\n",
      "174/174 [==============================] - 0s 141us/sample - loss: 394.9603 - mse: 407781.1562 - mae: 394.9603 - mape: 101.5042\n",
      "Epoch 39/75\n",
      "174/174 [==============================] - 0s 509us/sample - loss: 394.9549 - mse: 407773.8750 - mae: 394.9548 - mape: 101.4981\n",
      "Epoch 40/75\n",
      "174/174 [==============================] - 0s 129us/sample - loss: 394.9490 - mse: 407765.5938 - mae: 394.9489 - mape: 101.4979\n",
      "Epoch 41/75\n",
      "174/174 [==============================] - 0s 397us/sample - loss: 394.9432 - mse: 407757.5938 - mae: 394.9431 - mape: 101.4950\n",
      "Epoch 42/75\n",
      "174/174 [==============================] - 0s 159us/sample - loss: 394.9374 - mse: 407749.3438 - mae: 394.9374 - mape: 101.4975\n",
      "Epoch 43/75\n",
      "174/174 [==============================] - 0s 132us/sample - loss: 394.9314 - mse: 407741.3438 - mae: 394.9314 - mape: 101.4961\n",
      "Epoch 44/75\n",
      "174/174 [==============================] - 0s 399us/sample - loss: 394.9256 - mse: 407733.6875 - mae: 394.9257 - mape: 101.4960\n",
      "Epoch 45/75\n",
      "174/174 [==============================] - 0s 156us/sample - loss: 394.9197 - mse: 407725.5938 - mae: 394.9197 - mape: 101.4932\n",
      "Epoch 46/75\n",
      "174/174 [==============================] - 0s 135us/sample - loss: 394.9138 - mse: 407717.7812 - mae: 394.9138 - mape: 101.4916\n",
      "Epoch 47/75\n",
      "174/174 [==============================] - 0s 301us/sample - loss: 394.9077 - mse: 407709.3438 - mae: 394.9078 - mape: 101.4968\n",
      "Epoch 48/75\n",
      "174/174 [==============================] - 0s 119us/sample - loss: 394.9015 - mse: 407701.5625 - mae: 394.9015 - mape: 101.4945\n",
      "Epoch 49/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174/174 [==============================] - 0s 126us/sample - loss: 394.8964 - mse: 407694.0625 - mae: 394.8964 - mape: 101.4944\n",
      "Epoch 50/75\n",
      "174/174 [==============================] - 0s 106us/sample - loss: 394.8897 - mse: 407685.2812 - mae: 394.8897 - mape: 101.4921\n",
      "Epoch 51/75\n",
      "174/174 [==============================] - 0s 955us/sample - loss: 394.8845 - mse: 407678.3125 - mae: 394.8845 - mape: 101.4920\n",
      "Epoch 52/75\n",
      "174/174 [==============================] - 0s 125us/sample - loss: 394.8786 - mse: 407669.6875 - mae: 394.8786 - mape: 101.4897\n",
      "Epoch 53/75\n",
      "174/174 [==============================] - 0s 127us/sample - loss: 394.8729 - mse: 407661.7812 - mae: 394.8729 - mape: 101.4853\n",
      "Epoch 54/75\n",
      "174/174 [==============================] - 0s 112us/sample - loss: 394.8669 - mse: 407654.2500 - mae: 394.8669 - mape: 101.4853\n",
      "Epoch 55/75\n",
      "174/174 [==============================] - 0s 354us/sample - loss: 394.8612 - mse: 407645.7500 - mae: 394.8612 - mape: 101.4819\n",
      "Epoch 56/75\n",
      "174/174 [==============================] - 0s 112us/sample - loss: 394.8555 - mse: 407638.1250 - mae: 394.8555 - mape: 101.4821\n",
      "Epoch 57/75\n",
      "174/174 [==============================] - 0s 143us/sample - loss: 394.8497 - mse: 407630.0312 - mae: 394.8497 - mape: 101.4795\n",
      "Epoch 58/75\n",
      "174/174 [==============================] - 0s 234us/sample - loss: 394.8440 - mse: 407622.7188 - mae: 394.8440 - mape: 101.4780\n",
      "Epoch 59/75\n",
      "174/174 [==============================] - 0s 259us/sample - loss: 394.8376 - mse: 407614.1250 - mae: 394.8376 - mape: 101.4777\n",
      "Epoch 60/75\n",
      "174/174 [==============================] - 0s 150us/sample - loss: 394.8318 - mse: 407605.7812 - mae: 394.8318 - mape: 101.4762\n",
      "Epoch 61/75\n",
      "174/174 [==============================] - 0s 124us/sample - loss: 394.8254 - mse: 407596.8125 - mae: 394.8254 - mape: 101.4757\n",
      "Epoch 62/75\n",
      "174/174 [==============================] - 0s 510us/sample - loss: 394.8191 - mse: 407588.9062 - mae: 394.8192 - mape: 101.4768\n",
      "Epoch 63/75\n",
      "174/174 [==============================] - 0s 127us/sample - loss: 394.8130 - mse: 407580.2812 - mae: 394.8130 - mape: 101.4733\n",
      "Epoch 64/75\n",
      "174/174 [==============================] - 0s 461us/sample - loss: 394.8064 - mse: 407571.1875 - mae: 394.8064 - mape: 101.4797\n",
      "Epoch 65/75\n",
      "174/174 [==============================] - 0s 217us/sample - loss: 394.7999 - mse: 407563.2188 - mae: 394.7999 - mape: 101.4762\n",
      "Epoch 66/75\n",
      "174/174 [==============================] - 0s 397us/sample - loss: 394.7938 - mse: 407554.1562 - mae: 394.7938 - mape: 101.4780\n",
      "Epoch 67/75\n",
      "174/174 [==============================] - 0s 138us/sample - loss: 394.7876 - mse: 407546.6562 - mae: 394.7876 - mape: 101.4755\n",
      "Epoch 68/75\n",
      "174/174 [==============================] - 0s 125us/sample - loss: 394.7807 - mse: 407537.5312 - mae: 394.7807 - mape: 101.4747\n",
      "Epoch 69/75\n",
      "174/174 [==============================] - 0s 264us/sample - loss: 394.7745 - mse: 407528.1250 - mae: 394.7745 - mape: 101.4754\n",
      "Epoch 70/75\n",
      "174/174 [==============================] - 0s 132us/sample - loss: 394.7685 - mse: 407520.6875 - mae: 394.7685 - mape: 101.4784\n",
      "Epoch 71/75\n",
      "174/174 [==============================] - 0s 155us/sample - loss: 394.7615 - mse: 407511.7188 - mae: 394.7615 - mape: 101.4782\n",
      "Epoch 72/75\n",
      "174/174 [==============================] - 0s 152us/sample - loss: 394.7551 - mse: 407502.6562 - mae: 394.7551 - mape: 101.4816\n",
      "Epoch 73/75\n",
      "174/174 [==============================] - 0s 308us/sample - loss: 394.7491 - mse: 407494.8438 - mae: 394.7491 - mape: 101.4829\n",
      "Epoch 74/75\n",
      "174/174 [==============================] - 0s 111us/sample - loss: 394.7417 - mse: 407484.4062 - mae: 394.7417 - mape: 101.4835\n",
      "Epoch 75/75\n",
      "174/174 [==============================] - 0s 155us/sample - loss: 394.7352 - mse: 407476.6562 - mae: 394.7352 - mape: 101.4870\n",
      "-----------------------\n",
      "Training Deep Zero-Sets\n",
      "-----------------------\n",
      "Train on 191 samples\n",
      "191/191 [==============================] - 0s 2ms/sample - loss: 2.0191 - accuracy: 0.7749\n",
      "-----------------------------------\n",
      "Computing Final Performance Metrics\n",
      "-----------------------------------\n",
      "             train          test\n",
      "MAE   4.345536e+02    119.793450\n",
      "MSE   4.726779e+05  22063.149453\n",
      "MAPE           inf  32501.519221\n",
      "     L-time    P-time  N_params_expt  AIC-like      Eff  N. Parts\n",
      "0  1.825262  7.592606            200   390.428  634.704         1\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Ablation Completion Percentage: 0.2\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "-------------------------------------------------------\n",
      "Randomly Initialized Parts - Via Randomized Algorithm 2\n",
      "-------------------------------------------------------\n",
      "Final Loop\n",
      "breaking early\n",
      "The_parts_listhe number of parts are: 2.\n",
      "-----------------------------------------------------\n",
      "Training Sub-Networks on Each Randomly Generated Part\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 0/2Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Train on 139 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kratsi0000/opt/anaconda3/envs/bpcnn/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "139/139 [==============================] - 0s 3ms/sample - loss: 327.8125 - mse: 330239.8438 - mae: 327.8125 - mape: 100.4553\n",
      "Epoch 2/75\n",
      "139/139 [==============================] - 0s 137us/sample - loss: 327.8095 - mse: 330236.2812 - mae: 327.8095 - mape: 100.4570\n",
      "Epoch 3/75\n",
      "139/139 [==============================] - 0s 246us/sample - loss: 327.8064 - mse: 330232.4688 - mae: 327.8064 - mape: 100.4623\n",
      "Epoch 4/75\n",
      "139/139 [==============================] - 0s 146us/sample - loss: 327.8037 - mse: 330228.9688 - mae: 327.8037 - mape: 100.4680\n",
      "Epoch 5/75\n",
      "139/139 [==============================] - 0s 134us/sample - loss: 327.8008 - mse: 330225.5000 - mae: 327.8008 - mape: 100.4739\n",
      "Epoch 6/75\n",
      "139/139 [==============================] - 0s 127us/sample - loss: 327.7980 - mse: 330222.1250 - mae: 327.7980 - mape: 100.4746\n",
      "Epoch 7/75\n",
      "139/139 [==============================] - 0s 455us/sample - loss: 327.7954 - mse: 330219.0000 - mae: 327.7954 - mape: 100.4792\n",
      "Epoch 8/75\n",
      "139/139 [==============================] - 0s 125us/sample - loss: 327.7927 - mse: 330215.7188 - mae: 327.7927 - mape: 100.4844\n",
      "Epoch 9/75\n",
      "139/139 [==============================] - 0s 129us/sample - loss: 327.7901 - mse: 330212.3750 - mae: 327.7901 - mape: 100.4894\n",
      "Epoch 10/75\n",
      "139/139 [==============================] - 0s 142us/sample - loss: 327.7871 - mse: 330208.8125 - mae: 327.7871 - mape: 100.4932\n",
      "Epoch 11/75\n",
      "139/139 [==============================] - 1s 4ms/sample - loss: 327.7846 - mse: 330205.9062 - mae: 327.7846 - mape: 100.4986\n",
      "Epoch 12/75\n",
      "139/139 [==============================] - 0s 276us/sample - loss: 327.7818 - mse: 330202.4062 - mae: 327.7818 - mape: 100.5055\n",
      "Epoch 13/75\n",
      "139/139 [==============================] - 0s 3ms/sample - loss: 327.7792 - mse: 330199.2812 - mae: 327.7792 - mape: 100.5063\n",
      "Epoch 14/75\n",
      "139/139 [==============================] - 0s 697us/sample - loss: 327.7768 - mse: 330196.3125 - mae: 327.7768 - mape: 100.5106\n",
      "Epoch 15/75\n",
      "139/139 [==============================] - 0s 3ms/sample - loss: 327.7740 - mse: 330193.0938 - mae: 327.7740 - mape: 100.5113\n",
      "Epoch 16/75\n",
      "139/139 [==============================] - 0s 397us/sample - loss: 327.7715 - mse: 330190.2188 - mae: 327.7715 - mape: 100.5159\n",
      "Epoch 17/75\n",
      "139/139 [==============================] - 1s 4ms/sample - loss: 327.7687 - mse: 330186.7188 - mae: 327.7687 - mape: 100.5224\n",
      "Epoch 18/75\n",
      "139/139 [==============================] - 0s 2ms/sample - loss: 327.7661 - mse: 330183.4688 - mae: 327.7661 - mape: 100.5310\n",
      "Epoch 19/75\n",
      "139/139 [==============================] - 0s 209us/sample - loss: 327.7634 - mse: 330180.0938 - mae: 327.7633 - mape: 100.5359\n",
      "Epoch 20/75\n",
      "139/139 [==============================] - 0s 2ms/sample - loss: 327.7608 - mse: 330177.0625 - mae: 327.7608 - mape: 100.5407\n",
      "Epoch 21/75\n",
      "139/139 [==============================] - 0s 2ms/sample - loss: 327.7580 - mse: 330173.6562 - mae: 327.7580 - mape: 100.5449\n",
      "Epoch 22/75\n",
      "139/139 [==============================] - 1s 5ms/sample - loss: 327.7553 - mse: 330170.3750 - mae: 327.7553 - mape: 100.5537\n",
      "Epoch 23/75\n",
      "139/139 [==============================] - 1s 5ms/sample - loss: 327.7526 - mse: 330167.1562 - mae: 327.7526 - mape: 100.5558\n",
      "Epoch 24/75\n",
      "139/139 [==============================] - 1s 4ms/sample - loss: 327.7499 - mse: 330164.0312 - mae: 327.7499 - mape: 100.5634\n",
      "Epoch 25/75\n",
      "139/139 [==============================] - 0s 1ms/sample - loss: 327.7474 - mse: 330160.9062 - mae: 327.7474 - mape: 100.5709\n",
      "Epoch 26/75\n",
      "139/139 [==============================] - 1s 4ms/sample - loss: 327.7447 - mse: 330157.6562 - mae: 327.7447 - mape: 100.5785\n",
      "Epoch 27/75\n",
      "139/139 [==============================] - 0s 2ms/sample - loss: 327.7420 - mse: 330154.3125 - mae: 327.7420 - mape: 100.5844\n",
      "Epoch 28/75\n",
      "139/139 [==============================] - 0s 207us/sample - loss: 327.7393 - mse: 330151.2188 - mae: 327.7393 - mape: 100.5882\n",
      "Epoch 29/75\n",
      "139/139 [==============================] - 1s 5ms/sample - loss: 327.7364 - mse: 330147.4688 - mae: 327.7365 - mape: 100.5919\n",
      "Epoch 30/75\n",
      "139/139 [==============================] - 0s 2ms/sample - loss: 327.7340 - mse: 330144.8750 - mae: 327.7339 - mape: 100.5965\n",
      "Epoch 31/75\n",
      "139/139 [==============================] - 0s 224us/sample - loss: 327.7308 - mse: 330141.0625 - mae: 327.7308 - mape: 100.6032\n",
      "Epoch 32/75\n",
      "139/139 [==============================] - 0s 169us/sample - loss: 327.7282 - mse: 330137.7500 - mae: 327.7282 - mape: 100.6101\n",
      "Epoch 33/75\n",
      "139/139 [==============================] - 0s 1ms/sample - loss: 327.7256 - mse: 330134.5938 - mae: 327.7256 - mape: 100.6215\n",
      "Epoch 34/75\n",
      "139/139 [==============================] - 0s 295us/sample - loss: 327.7227 - mse: 330131.1250 - mae: 327.7227 - mape: 100.6257\n",
      "Epoch 35/75\n",
      "139/139 [==============================] - 0s 152us/sample - loss: 327.7201 - mse: 330127.8750 - mae: 327.7200 - mape: 100.6343\n",
      "Epoch 36/75\n",
      "139/139 [==============================] - 0s 3ms/sample - loss: 327.7172 - mse: 330124.2500 - mae: 327.7172 - mape: 100.6352\n",
      "Epoch 37/75\n",
      "139/139 [==============================] - 0s 2ms/sample - loss: 327.7144 - mse: 330121.2812 - mae: 327.7144 - mape: 100.6408\n",
      "Epoch 38/75\n",
      "139/139 [==============================] - 0s 479us/sample - loss: 327.7119 - mse: 330118.3750 - mae: 327.7119 - mape: 100.6495\n",
      "Epoch 39/75\n",
      "139/139 [==============================] - 0s 142us/sample - loss: 327.7091 - mse: 330114.6562 - mae: 327.7091 - mape: 100.6581\n",
      "Epoch 40/75\n",
      "139/139 [==============================] - 0s 137us/sample - loss: 327.7061 - mse: 330111.4688 - mae: 327.7061 - mape: 100.6679\n",
      "Epoch 41/75\n",
      "139/139 [==============================] - 0s 3ms/sample - loss: 327.7034 - mse: 330107.9375 - mae: 327.7034 - mape: 100.6705\n",
      "Epoch 42/75\n",
      "139/139 [==============================] - 0s 3ms/sample - loss: 327.7006 - mse: 330104.9688 - mae: 327.7006 - mape: 100.6786\n",
      "Epoch 43/75\n",
      "139/139 [==============================] - 0s 2ms/sample - loss: 327.6978 - mse: 330101.3438 - mae: 327.6978 - mape: 100.6831\n",
      "Epoch 44/75\n",
      "139/139 [==============================] - 0s 132us/sample - loss: 327.6953 - mse: 330098.2812 - mae: 327.6953 - mape: 100.6881\n",
      "Epoch 45/75\n",
      "139/139 [==============================] - 1s 5ms/sample - loss: 327.6923 - mse: 330094.6250 - mae: 327.6923 - mape: 100.6923\n",
      "Epoch 46/75\n",
      "139/139 [==============================] - 0s 1ms/sample - loss: 327.6897 - mse: 330091.5312 - mae: 327.6897 - mape: 100.7020\n",
      "Epoch 47/75\n",
      "139/139 [==============================] - 0s 3ms/sample - loss: 327.6867 - mse: 330087.8438 - mae: 327.6867 - mape: 100.7084\n",
      "Epoch 48/75\n",
      "139/139 [==============================] - 1s 5ms/sample - loss: 327.6839 - mse: 330084.4375 - mae: 327.6839 - mape: 100.7141\n"
     ]
    }
   ],
   "source": [
    "# Initialize \n",
    "# q_implicit_N_parts_possibilities = np.linspace(min_parts_threshold,max_parts_threshold,N_plot_finess)\n",
    "N_parts_possibilities = np.unique(np.round(np.linspace(N_min_parts,N_max_plots,num=N_plot_finess))).astype(int)\n",
    "\n",
    "# Get Performance Metric\n",
    "for inplicit_N_parts_loop in range(len(N_parts_possibilities)):\n",
    "    ### UPDATE USER ###\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    print('Ablation Completion Percentage:',(inplicit_N_parts_loop/N_plot_finess))\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    \n",
    "    # Implicitly Set: Current Number of Parts\n",
    "#     q_implicit_N_parts_loop = q_implicit_N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    N_parts_possibilities_loop = N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    # Run Algos. 1+2\n",
    "    performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, N_neurons_subPatterns_loop, N_neurons_deep_Zero_Sets_loop, height_mean_loop = get_PCNNs(N_parts_possibilities_loop,X_train,y_train,X_test,y_test)\n",
    "#     performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, height_mean_loop = Ablate_PCNNs(q_implicit_N_parts_loop,data_y,X_train,X_test,y_test)\n",
    "    # Reshape\n",
    "    performance_Architope_loop = performance_Architope_loop.to_numpy().reshape([3,2,1])\n",
    "    Architope_Model_Complexity_full_loop = Architope_Model_Complexity_full_loop.to_numpy().reshape([1,6,1])\n",
    "\n",
    "    # Record\n",
    "    if inplicit_N_parts_loop == 0:\n",
    "        # Don't count partitioner if only one parts is active!\n",
    "        if N_parts_possibilities_loop <= 1:\n",
    "            Architope_Model_Complexity_full_loop[:,1] = Architope_Model_Complexity_full_loop[:,0]\n",
    "            N_neurons_deep_Zero_Sets_loop = 0\n",
    "        # Record Model Complexities Otherwise    \n",
    "        performance_Architope_history = performance_Architope_loop\n",
    "        Architope_Model_Complexity_history = Architope_Model_Complexity_full_loop\n",
    "        N_parts_Generated_by_Algo_2_history = N_parts_Generated_by_Algo_2_loop\n",
    "        N_params_subPatterns_hist = N_neurons_subPatterns_loop\n",
    "        N_neurons_deep_Zero_Sets_hist = N_neurons_deep_Zero_Sets_loop\n",
    "        N_params_architope_hist = N_neurons_deep_Zero_Sets_loop + N_neurons_subPatterns_loop\n",
    "        height_mean_hist = height_mean_loop\n",
    "        N_neurons_per_input = N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))\n",
    "    else:\n",
    "        performance_Architope_history = np.concatenate((performance_Architope_history,performance_Architope_loop),axis=2)\n",
    "        Architope_Model_Complexity_history = np.concatenate((Architope_Model_Complexity_history,Architope_Model_Complexity_full_loop),axis=2)\n",
    "        N_parts_Generated_by_Algo_2_history = np.append(N_parts_Generated_by_Algo_2_history,N_parts_Generated_by_Algo_2_loop)\n",
    "        N_params_architope_hist = np.append(N_params_architope_hist,N_params_architope_loop)\n",
    "        N_params_subPatterns_hist = np.append(N_params_subPatterns_hist,N_neurons_subPatterns_loop)\n",
    "        N_neurons_deep_Zero_Sets_hist = np.append(N_neurons_deep_Zero_Sets_hist,N_neurons_deep_Zero_Sets_loop)\n",
    "        height_mean_hist = np.append(height_mean_hist,height_mean_loop)\n",
    "        N_neurons_per_input = np.append(N_neurons_per_input,(N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))))\n",
    "\n",
    "# Cleanup\n",
    "## Randomization may produce duplicates; we remove these with the following snippet:\n",
    "get_unique_entries = np.unique(N_parts_Generated_by_Algo_2_history, return_index=True)[1]\n",
    "N_parts_Generated_by_Algo_2_history_report = N_parts_Generated_by_Algo_2_history[get_unique_entries]\n",
    "\n",
    "# Write\n",
    "## Prediction Qualities\n",
    "performance_Architope_history_report_MAE_train = (performance_Architope_history[0,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MAE_test = (performance_Architope_history[0,1,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_train = (performance_Architope_history[1,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_test = (performance_Architope_history[1,1,:])[get_unique_entries]\n",
    "## Model Complexities\n",
    "L_Times = (Architope_Model_Complexity_history[:,1].reshape(-1,))[get_unique_entries]\n",
    "P_Times = (Architope_Model_Complexity_history[:,0].reshape(-1,))[get_unique_entries]\n",
    "N_Params = (N_params_architope_hist.reshape(-1,))[get_unique_entries]\n",
    "mean_subpattern_widths_hist = (height_mean_hist.reshape(-1,))[get_unique_entries]\n",
    "AIC_Like = (Architope_Model_Complexity_history[:,3].reshape(-1,))[get_unique_entries]\n",
    "Eff = (Architope_Model_Complexity_history[:,4].reshape(-1,))[get_unique_entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Feedforward Neural Network (ffNN) Benchmark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Model complexities for ffNNs\n",
    "P_time_ffNN = P_Times[0]\n",
    "L_time_ffNN = P_Times[0]\n",
    "Width_ffNN = height_mean_hist[0]\n",
    "# For: Plots\n",
    "MAE_ffNN = np.repeat(performance_Architope_history_report_MAE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "MSE_ffNN = np.repeat(performance_Architope_history_report_MSE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "L_times_ffNN_plot = np.repeat(L_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "P_times_ffNN_plot = np.repeat(P_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_per_input_ffNN = np.repeat(N_neurons_per_input[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "Width_neurons_ffNN = np.repeat(mean_subpattern_widths_hist[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "N_neurons_ffNN = np.repeat(N_Params[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "# Record in Table\n",
    "ffNN_Model_Complexity = pd.DataFrame({'L-time': [L_time_ffNN],\n",
    "                                               'P-time':[P_time_ffNN],\n",
    "                                               'N_params_expt': [N_neurons_ffNN],\n",
    "                                               'AIC-like': [0],\n",
    "                                               'Eff': [0],\n",
    "                                               'N. Parts':[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Plots\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MSE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MSE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen')\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MSE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta')\n",
    "# Add Legend\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MSE_test___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MAE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MAE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MAE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen')\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MAE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta')\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MAE___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"L-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"L-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen')\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_L_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"P-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"P-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen')\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_P_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"N. Params\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"N. Params\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_Params,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen')\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_N_Params___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Active Neurons Per Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Active Neurons per. Input\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Active Neurons per. Input\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_per_input,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen')\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Active_Neurons_per_input___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Widths for Sub-Pattern Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Mean Subpattern Widths\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Mean Subpattern Widths\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen')\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         Width_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Mean_Widths___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
