{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation: Semi-Supervised Architope - for Reviews\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"PC-NNs\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 1-(1/24)\n",
    "min_width = 500\n",
    "min_epochs = 15; min_epochs_classifier = 150\n",
    "# Ablation Finess\n",
    "N_plot_finess = 3\n",
    "# min_parts_threshold = .001; max_parts_threshold = 0.9\n",
    "N_min_parts = 2; N_max_plots = 20\n",
    "Tied_Neurons_Q = True\n",
    "randomize_subpattern_construction = True\n",
    "# Partition with Inputs (determine parts with domain) or outputs (determine parts with image)\n",
    "Partition_using_Inputs = True\n",
    "# Cuttoff Level\n",
    "gamma = .5\n",
    "# Softmax Layer instead of sigmoid\n",
    "softmax_layer = False #<- Just out of curiosity...but it doesn't perform many better IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------#\n",
    "# Only For Motivational Example Only #\n",
    "#------------------------------------#\n",
    "## Hyperparameters\n",
    "percentage_in_row = .25\n",
    "N = 10**4\n",
    "\n",
    "def f_1(x):\n",
    "    return 1 + np.sin(10*x)\n",
    "def f_2(x):\n",
    "    return -2 -np.exp(x)\n",
    "x_0 = 0\n",
    "x_end = 1\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "1\n",
      "Training Data size:  416\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "if Option_Function != \"Motivational_Example\": \n",
    "    exec(open('Financial_Data_Preprocessor.py').read())\n",
    "else:\n",
    "    print(1)\n",
    "    exec(open('Motivational_Example.py').read())\n",
    "    print(\"Training Data size: \",X_train.shape[0])\n",
    "# exec(open('Prepare_Data_California_Housing.py').read())\n",
    "# Import time separately\n",
    "import time\n",
    "\n",
    "# TEMP\n",
    "# import pickle_compat\n",
    "# pickle_compat.patch()\n",
    "# param_grid_Vanilla_Nets['input_dim']=X_train.shape[1]\n",
    "sns.set()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit Partion Builder:\n",
    "Implements exactly Algorithm 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(X_in,\n",
    "                               y_in,\n",
    "                               N_parts_to_get=4):\n",
    "\n",
    "    # Compute Size of each part\n",
    "    size_part_reference = int(round(X_in.shape[0]/N_parts_to_get))\n",
    "\n",
    "    # Apply random bijection #\n",
    "    #------------------------#\n",
    "    ## Get random bijection indices\n",
    "    random_bijection_indices = np.random.choice(range(X_in.shape[0]),size=X_in.shape[0], replace=False)\n",
    "    ## Apply random bijections\n",
    "    X_in_shuffled = X_in[random_bijection_indices,:]\n",
    "    y_in_shuffled = y_in[random_bijection_indices,:]\n",
    "\n",
    "    # Initialize Lists #\n",
    "    #------------------#\n",
    "    X_parts = []\n",
    "    y_parts = []\n",
    "\n",
    "    for i_th_part_to_get in range(N_parts_to_get):\n",
    "        # Build random balls #\n",
    "        #--------------------#\n",
    "        ## Sample random radius\n",
    "        size_part = int(np.maximum(1,np.round(size_part_reference*np.random.uniform(low=.5,high=1.5,size=1)[0])))\n",
    "        ## Sample random point\n",
    "        X_center_loop_index = np.random.choice(range(X_in_shuffled.shape[0]),size=1, replace=False)\n",
    "        X_center_loop = X_in_shuffled[X_center_loop_index,:]\n",
    "        ## Compute Typical Distances from Center\n",
    "        distances_loop = X_center_loop-X_in_shuffled\n",
    "        distances_loop = np.linalg.norm(distances_loop, axis=1)\n",
    "\n",
    "        # Remove Random Ball from Dataset\n",
    "        if size_part <= len(distances_loop):\n",
    "            ## Identify indices\n",
    "            indices_smallest_to_random_ball = np.argsort(distances_loop)[:size_part]\n",
    "        else:\n",
    "            print('Final Loop')\n",
    "            indices_smallest_to_random_ball = np.array(range(X_in_shuffled.shape[0]))\n",
    "        ## Extract Parts\n",
    "        X_current_part_loop = X_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        y_current_part_loop = y_in_shuffled[indices_smallest_to_random_ball,:]\n",
    "        ## Append to List of Parts\n",
    "        X_parts.append(X_current_part_loop)\n",
    "        y_parts.append(y_current_part_loop)\n",
    "\n",
    "        # Remove Selected Entries From Array #\n",
    "        #------------------------------------#\n",
    "        X_in_shuffled = np.delete(X_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "        y_in_shuffled = np.delete(y_in_shuffled,indices_smallest_to_random_ball,axis=0)\n",
    "\n",
    "        # Failsafe if procedure has terminated\n",
    "        if X_in_shuffled.shape[0] == 0:\n",
    "            print('breaking early')\n",
    "            break\n",
    "    # Count Number of Parts Generated        \n",
    "    N_parts_generated = len(X_parts)\n",
    "    # Output Parts\n",
    "    return X_parts, y_parts, N_parts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCNNs(N_parts,\n",
    "              X_train,\n",
    "              y_train,\n",
    "              X_test,\n",
    "              y_test):\n",
    "\n",
    "    # Initialization(s) #\n",
    "    #-------------------#\n",
    "    N_neurons = 0\n",
    "    L_timer = 0\n",
    "    P_timer = 0\n",
    "    Mean_Width_Subnetworks = 0\n",
    "\n",
    "    # Partitioner Begin #\n",
    "    #-------------------#\n",
    "    import time\n",
    "    partitioning_time_begin = time.time()\n",
    "    print('-------------------------------------------------------')\n",
    "    print('Randomly Initialized Parts - Via Randomized Algorithm 2')\n",
    "    print('-------------------------------------------------------')\n",
    "    if Partition_using_Inputs == True:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(X_train.to_numpy(),\n",
    "                                                                                             y_train.reshape(-1,1),\n",
    "                                                                                             N_parts)\n",
    "    else:\n",
    "        X_parts_list, y_parts_list, N_parts_Generated_by_Algo_2 = Random_Lipschitz_Partioner(y_train.reshape(-1,1),\n",
    "                                                                                             X_train.to_numpy(),\n",
    "                                                                                             N_parts)\n",
    "    partitioning_time = time.time() - partitioning_time_begin\n",
    "    print('The_parts_listhe number of parts are: ' + str(N_parts_Generated_by_Algo_2)+'.')\n",
    "    ############# Partitioner End ########\n",
    "\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Training Sub-Networks on Each Randomly Generated Part')\n",
    "    print('-----------------------------------------------------')\n",
    "    # Time-Elapse (Start) for Training on Each Part #\n",
    "    PCNN_timer = time.time(); PCNN_timer = -math.inf; N_params_Architope = 0; N_params_tally = 0\n",
    "    # Remove Eager Execution Error(s)\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    # Automatically Initialize Correct Input/Output Dimension(s)\n",
    "    param_grid_Vanilla_Nets['input_dim'] = [X_train.shape[1]]; param_grid_Vanilla_Nets['output_dim'] = [1]\n",
    "    param_grid_Deep_Classifier['input_dim'] = [X_train.shape[1]]\n",
    "    # Decide if/or not to tie neuron numbers of sub-patterns together\n",
    "    if Tied_Neurons_Q == True:\n",
    "        param_grid_Vanilla_Nets['height'] = [int(np.maximum(round(param_grid_Vanilla_Nets['height'][0]/N_parts),min_width))]\n",
    "        param_grid_Vanilla_Nets['epochs'] = [int(np.maximum(round(param_grid_Vanilla_Nets['epochs'][0]/int(round(np.sqrt(N_parts)))),min_epochs))]\n",
    "#         param_grid_Deep_Classifier['height'] = [int(np.maximum(round(param_grid_Deep_Classifier['height'][0]/N_parts),min_width))]\n",
    "\n",
    "\n",
    "    for current_part in range(N_parts_Generated_by_Algo_2):\n",
    "        # Update User #\n",
    "        #-------------#\n",
    "        print('-----------------------------------------------------------')\n",
    "        print('Currently Training Part: '+str(current_part)+'/'+str(N_parts_Generated_by_Algo_2 )+'Total Parts.')\n",
    "        print('-----------------------------------------------------------')\n",
    "\n",
    "        # Timer for Part\n",
    "        part_training_timer = time.time()\n",
    "        # Get Data for Sub-Pattern\n",
    "        X_loop = pd.DataFrame(X_parts_list[current_part])\n",
    "        y_loop = (y_parts_list[current_part]).reshape(-1,)\n",
    "        # Train ffNN\n",
    "        if randomize_subpattern_construction == False:\n",
    "            y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN(n_folds = 4, \n",
    "                                                                                  n_jobs = n_jobs,\n",
    "                                                                                  n_iter = n_iter, \n",
    "                                                                                  param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                                  X_train= X_loop, \n",
    "                                                                                  y_train=y_loop,\n",
    "                                                                                  X_test_partial=X_train,\n",
    "                                                                                  X_test=X_test,\n",
    "                                                                                  NOCV=True)\n",
    "        else:\n",
    "            y_hat_part_loop, y_hat_part_loop_test, N_neurons_PCNN_loop = build_ffNN_random(X_loop,\n",
    "                                                                                           X_train,\n",
    "                                                                                           X_test,\n",
    "                                                                                           y_loop,\n",
    "                                                                                           param_grid_Vanilla_Nets)\n",
    "        # Reshape y\n",
    "        ## Training\n",
    "        y_train.shape = (y_train.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop.shape = (y_hat_part_loop.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        ## Testing\n",
    "        y_test.shape = (y_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "        y_hat_part_loop_test.shape = (y_hat_part_loop_test.shape[0], param_grid_Vanilla_Nets['output_dim'][0])\n",
    "\n",
    "        # Append predictions to data-frames\n",
    "        ## If first prediction we initialize data-frames\n",
    "        if current_part==0:\n",
    "            # Register quality\n",
    "            training_quality = np.array(np.abs(y_hat_part_loop-y_train)).reshape(y_hat_part_loop.shape[0],1)\n",
    "            # Save Predictions\n",
    "            predictions_train = y_hat_part_loop.reshape(y_hat_part_loop.shape[0],1)\n",
    "            predictions_test = y_hat_part_loop_test.reshape(y_hat_part_loop_test.shape[0],1)\n",
    "\n",
    "\n",
    "        ## If not first prediction we append to already initialized dataframes\n",
    "        else:\n",
    "        # Register Best Scores\n",
    "            #----------------------#\n",
    "            # Write Predictions \n",
    "            # Save Predictions\n",
    "            y_hat_train_loop = y_hat_part_loop.reshape(predictions_train.shape[0],1)\n",
    "            predictions_train = np.append(predictions_train,y_hat_train_loop,axis=1)\n",
    "            y_hat_test_loop = y_hat_part_loop_test.reshape(predictions_test.shape[0],1)\n",
    "            predictions_test = np.append(predictions_test,y_hat_test_loop,axis=1)\n",
    "\n",
    "            # Evaluate Errors #\n",
    "            #-----------------#\n",
    "            # Training\n",
    "            prediction_errors = np.abs(y_hat_train_loop-y_train)\n",
    "#             prediction_errors = np.abs(y_hat_train_loop-y_loop)\n",
    "            training_quality = np.append(training_quality,prediction_errors.reshape(training_quality.shape[0],1),axis=1)\n",
    "        #==============================#\n",
    "        # Update Performance Metric(s) #\n",
    "        #==============================#\n",
    "        part_training_timer = time.time() - part_training_timer\n",
    "        # L-Time\n",
    "        L_timer += partitioning_time\n",
    "        # P-Time\n",
    "        P_timer = max(P_timer,part_training_timer)\n",
    "        # N. Params\n",
    "        N_neurons += N_neurons_PCNN_loop\n",
    "        # Mean Width for Sub-Network(s)\n",
    "        Mean_Width_Subnetworks += param_grid_Vanilla_Nets['height'][0]\n",
    "\n",
    "    # Take Mean of Width(s)\n",
    "    Mean_Width_Subnetworks = Mean_Width_Subnetworks/N_parts_Generated_by_Algo_2\n",
    "    print('-----------------------')\n",
    "    print('Training Deep Zero-Sets')\n",
    "    print('-----------------------')\n",
    "\n",
    "\n",
    "    # Time Elapsed for Training Deep Zero-Sets\n",
    "    Deep_Zero_Sets_timer = time.time()\n",
    "\n",
    "    ## Initialize Classes Labels\n",
    "    if softmax_layer == False:\n",
    "        # No pooling (classical)\n",
    "        partition_labels_training_integers = np.argmin(training_quality,axis=-1)\n",
    "    else:\n",
    "        # Max Pooling\n",
    "#         partition_labels_training_integers = (training_quality == training_quality.min(axis=1)[:,None]).astype(int)\n",
    "        partition_labels_training_integers = np.apply_along_axis(softminn, 1, training_quality).astype(int)\n",
    "    partition_labels_training = pd.DataFrame(pd.DataFrame(partition_labels_training_integers) == 0)\n",
    "    ## Build Classes\n",
    "    for part_column_i in range(1,(training_quality.shape[1])):\n",
    "        partition_labels_training = pd.concat([partition_labels_training,\n",
    "                                               (pd.DataFrame(partition_labels_training_integers) == part_column_i)\n",
    "                                              ],axis=1)\n",
    "    ## Convert to integers\n",
    "    partition_labels_training = partition_labels_training+0\n",
    "    ## Train simple deep classifier\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    exec(open('Grid_Enhanced_Network.py').read())\n",
    "    if randomize_subpattern_construction == False:\n",
    "#         print(partition_labels_training)\n",
    "        param_grid_Deep_Classifier['epochs'] = [int(np.maximum(round(param_grid_Deep_Classifier['epochs'][0]),min_epochs_classifier))]\n",
    "#         print(param_grid_Deep_Classifier)\n",
    "        predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                            n_jobs = n_jobs, \n",
    "                                                                                                            n_iter =n_iter, \n",
    "                                                                                                            param_grid_in = param_grid_Deep_Classifier, \n",
    "                                                                                                            X_train = X_train.values, \n",
    "                                                                                                            y_train = partition_labels_training.values,\n",
    "                                                                                                            X_test = X_test.values)\n",
    "        # Get Binary Classes (Discontinuous Unit)\n",
    "        ## Training Set\n",
    "        predicted_classes_train = ((predicted_classes_train>gamma)*1).astype(int)\n",
    "        ## Testing Set\n",
    "        predicted_classes_test = ((predicted_classes_test > gamma)*1).astype(int)\n",
    "        # Get PC-NN Prediction(s)\n",
    "        ## Train\n",
    "        PCNN_prediction_y_train = (predictions_train*predicted_classes_train).sum(axis=1)\n",
    "        ## Test\n",
    "        PCNN_prediction_y_test = (predictions_test*predicted_classes_test).sum(axis=1)\n",
    "    else:\n",
    "        if N_parts > 1:\n",
    "            print(partition_labels_training)\n",
    "            partition_labels_training_dzs = np.argmin(partition_labels_training.to_numpy(),axis=-1)\n",
    "            for j in range(10):\n",
    "                print('---')\n",
    "            print(partition_labels_training_dzs)\n",
    "            PCNN_prediction_y_train, PCNN_prediction_y_test, N_params_deep_classifier = build_deep_classifier_random(X_train_in = X_train,\n",
    "                                                                                                                     X_train_in_full = X_train,\n",
    "                                                                                                                     X_test_in = X_test,\n",
    "                                                                                                                     predictions_test_in = predictions_test,\n",
    "                                                                                                                     predictions_train_in = predictions_train,\n",
    "                                                                                                                     classes_in = partition_labels_training_dzs,\n",
    "                                                                                                                     param_grid_in = param_grid_Deep_Classifier)\n",
    "        else:\n",
    "            print('Entering N Parts == 1 case!')\n",
    "            PCNN_prediction_y_train = predictions_train.reshape(-1,param_grid_Vanilla_Nets['output_dim'][0])\n",
    "            PCNN_prediction_y_test = predictions_test.reshape(-1,param_grid_Vanilla_Nets['output_dim'][0])\n",
    "            N_params_deep_classifier = 0\n",
    "    # End Timer\n",
    "    Deep_Zero_Sets_timer = time.time() - Deep_Zero_Sets_timer\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Computing Final Performance Metrics')\n",
    "    print('-----------------------------------')\n",
    "    # Time-Elapsed Training Deep Classifier\n",
    "\n",
    "    # Update Times\n",
    "    L_timer +=Deep_Zero_Sets_timer\n",
    "    P_timer +=Deep_Zero_Sets_timer\n",
    "    # Update Number of Neurons Used\n",
    "    N_neurons_subPatterns = N_neurons\n",
    "    N_neurons_deep_Zero_Sets = (param_grid_Deep_Classifier['height'][0])*(param_grid_Deep_Classifier['depth'][0])\n",
    "    N_neurons = N_neurons_deep_Zero_Sets + N_neurons_subPatterns\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Peformance\n",
    "    performance_PCNN = reporter(y_train_hat_in=PCNN_prediction_y_train,y_test_hat_in=PCNN_prediction_y_test,\n",
    "                                y_train_in=y_train,\n",
    "                                y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_PCNN.to_latex((results_tables_path+\"PCNN_full_performance.tex\"))\n",
    "\n",
    "    # Update User\n",
    "    print(performance_PCNN)\n",
    "\n",
    "    ### Model Complexity/Efficiency Metrics\n",
    "    # Build AIC-like Metric #\n",
    "    #-----------------------#\n",
    "    AIC_like = 2*(N_neurons - np.log((performance_PCNN['test']['MAE'])))\n",
    "    AIC_like = np.round(AIC_like,3)\n",
    "    Efficiency = np.log(N_neurons) *(performance_PCNN['test']['MAE'])\n",
    "    Efficiency = np.round(Efficiency,3)\n",
    "\n",
    "\n",
    "    # Build Table #\n",
    "    #-------------#\n",
    "    PCNN_Model_Complexity = pd.DataFrame({'L-time': [L_timer],\n",
    "                                               'P-time':[P_timer],\n",
    "                                               'N_params_expt': [N_neurons],\n",
    "                                               'AIC-like': [AIC_like],\n",
    "                                               'Eff': [Efficiency],\n",
    "                                               'N. Parts':[N_parts_Generated_by_Algo_2]})\n",
    "\n",
    "\n",
    "    # Write Required Training Time(s)\n",
    "    PCNN_Model_Complexity.to_latex((results_tables_path+\"PCNN_full_model_complexities.tex\"))\n",
    "\n",
    "    #--------------======---------------#\n",
    "    # Display Required Training Time(s) #\n",
    "    #--------------======---------------#\n",
    "    print(PCNN_Model_Complexity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #########################################\n",
    "    for j in range(10):\n",
    "        print('#------------------------------#')\n",
    "    #########################################\n",
    "    print('# ---- Getting Benchmarks ---- #')\n",
    "    #########################################\n",
    "    for j in range(10):\n",
    "        print('#------------------------------#')\n",
    "    #########################################\n",
    "    print('Training PCNN-lgt')\n",
    "    # Time-Elapsed Training linear classifier\n",
    "    Architope_logistic_classifier_training_begin = time.time()\n",
    "    if N_parts > 1:\n",
    "        parameters = {'penalty': ['none'], 'C': [0.1]}\n",
    "        lr = LogisticRegression(random_state=2020)\n",
    "        cv = RepeatedStratifiedKFold(n_splits=CV_folds, \n",
    "                                     n_repeats=n_iter, random_state=0)\n",
    "        classifier = RandomizedSearchCV(lr, \n",
    "                                        parameters, \n",
    "                                        random_state=2020)\n",
    "\n",
    "        # Initialize Classes Labels\n",
    "        partition_labels_training = np.argmin(training_quality,axis=-1)\n",
    "        # Train Logistic Classifier #\n",
    "        #---------------------------#\n",
    "        # Supress warnings caused by \"ignoring C\" for 'none' penalty and similar obvious warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Train Classifier\n",
    "        classifier.fit(X_train, partition_labels_training)\n",
    "    if N_parts >1 :\n",
    "        #### Write Predicted Class(es)\n",
    "        # Training Set\n",
    "        predicted_classes_train_logistic_BM = classifier.best_estimator_.predict(X_train)\n",
    "        Architope_prediction_y_train_logistic_BM = np.take_along_axis(predictions_train, predicted_classes_train_logistic_BM[:,None], axis=1)\n",
    "        # Testing Set\n",
    "        predicted_classes_test_logistic_BM = classifier.best_estimator_.predict(X_test)\n",
    "        Architope_prediction_y_test_logistic_BM = np.take_along_axis(predictions_test, \n",
    "                                                                     predicted_classes_test_logistic_BM[:,None], \n",
    "                                                                     axis=1)\n",
    "    else:\n",
    "        #### Write Predicted Class(es)\n",
    "        # Training Set\n",
    "        Architope_prediction_y_train_logistic_BM = predictions_train\n",
    "        # Testing Set\n",
    "        Architope_prediction_y_test_logistic_BM = predictions_test    \n",
    "    # Extract Number of Parameters Logistic Regressor\n",
    "    if N_parts > 1:\n",
    "        N_params_best_logistic = (classifier.best_estimator_.coef_.shape[0])*(classifier.best_estimator_.coef_.shape[1]) + len(classifier.best_estimator_.intercept_)\n",
    "    else:\n",
    "        N_params_best_logistic = 1\n",
    "    N_params_best_logistic = N_params_best_logistic + N_neurons_subPatterns*N_parts    \n",
    "    # Time-Elapsed Training linear classifier\n",
    "    Architope_logistic_classifier_training = time.time() - Architope_logistic_classifier_training_begin\n",
    "    #### Compute Performance\n",
    "    # Compute Peformance\n",
    "    performance_architope_ffNN_logistic = reporter(y_train_hat_in=Architope_prediction_y_train_logistic_BM,\n",
    "                                        y_test_hat_in=Architope_prediction_y_test_logistic_BM,\n",
    "                                        y_train_in=y_train,\n",
    "                                        y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_architope_ffNN_logistic.to_latex((results_tables_path+\"Architopes_logistic_performance.tex\"))\n",
    "    \n",
    "    ##### --- #####\n",
    "    print('Training PCNN-Bagged')\n",
    "    ##### --- #####\n",
    "    # Time for Bagging\n",
    "    Bagging_ffNN_bagging_time_begin = time.time()\n",
    "    # Train Bagging Weights in-sample\n",
    "    bagging_coefficients = LinearRegression().fit(predictions_train,y_train)\n",
    "    # Predict Bagging Weights out-of-sample\n",
    "    bagged_prediction_train = bagging_coefficients.predict(predictions_train)\n",
    "    bagged_prediction_test = bagging_coefficients.predict(predictions_test)\n",
    "    # Write number of trainable bagging parameters\n",
    "    N_bagged_parameters = len(bagging_coefficients.coef_) + 1\n",
    "    # Time for Bagging\n",
    "    Bagging_ffNN_bagging_time = time.time() - Bagging_ffNN_bagging_time_begin\n",
    "    # Compute Peformance\n",
    "    performance_bagged_ffNN = reporter(y_train_hat_in=bagged_prediction_train,\n",
    "                                        y_test_hat_in=bagged_prediction_test,\n",
    "                                        y_train_in=y_train,\n",
    "                                        y_test_in=y_test)\n",
    "    # Write Performance\n",
    "    performance_bagged_ffNN.to_latex((results_tables_path+\"ffNN_Bagged.tex\"))\n",
    "    \n",
    "    for jj in range(5):\n",
    "        print('-----------------------')\n",
    "    print('...Returning Results...')\n",
    "    for jj in range(5):\n",
    "        print('-----------------------')\n",
    "    # Return Output(s)\n",
    "    return performance_PCNN, PCNN_Model_Complexity, N_parts_Generated_by_Algo_2, N_neurons, N_neurons_subPatterns,N_neurons_deep_Zero_Sets, Mean_Width_Subnetworks, performance_architope_ffNN_logistic, N_params_best_logistic, performance_bagged_ffNN, Bagging_ffNN_bagging_time, Architope_logistic_classifier_training, Deep_Zero_Sets_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Perform Ablation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Ablation Completion Percentage: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "-------------------------------------------------------\n",
      "Randomly Initialized Parts - Via Randomized Algorithm 2\n",
      "-------------------------------------------------------\n",
      "The_parts_listhe number of parts are: 2.\n",
      "-----------------------------------------------------\n",
      "Training Sub-Networks on Each Randomly Generated Part\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 0/2Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  0\n",
      "(416, 1)\n",
      "(416, 1)\n",
      "TQAL\n",
      "(416, 1)\n",
      "training_quality\n",
      "(416, 1)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 1/2Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  1\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 1)\n",
      "training_quality\n",
      "(416, 2)\n",
      "-----------------------\n",
      "Training Deep Zero-Sets\n",
      "-----------------------\n",
      "(416, 1)\n",
      "(9585, 1)\n",
      "(9585, 2)\n",
      "(416, 2)\n",
      "(416, 2)\n",
      "{'batch_size': [16], 'epochs': [200], 'learning_rate': [0.0001], 'height': [2000], 'depth': [2], 'input_dim': [1], 'output_dim': [1]}\n",
      "     0  0\n",
      "0    0  1\n",
      "1    1  0\n",
      "2    0  1\n",
      "3    0  1\n",
      "4    1  0\n",
      "..  .. ..\n",
      "411  0  1\n",
      "412  0  1\n",
      "413  0  1\n",
      "414  0  1\n",
      "415  1  0\n",
      "\n",
      "[416 rows x 2 columns]\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "[0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1\n",
      " 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0\n",
      " 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1\n",
      " 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1\n",
      " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0\n",
      " 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 1]\n",
      "Generating Random Deep Features for Deep Zero-Sets\n",
      "Get Classifier\n",
      "OK!\n",
      "-----------------------------------\n",
      "Computing Final Performance Metrics\n",
      "-----------------------------------\n",
      "             train          test\n",
      "MAE   6.912867e+02  6.060114e+02\n",
      "MSE   2.106946e+06  1.743224e+06\n",
      "MAPE  3.033840e+02  3.922169e+02\n",
      "      L-time     P-time  N_params_expt   AIC-like       Eff  N. Parts\n",
      "0  46.748044  47.010621           7000  13987.186  5365.422         2\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "# ---- Getting Benchmarks ---- #\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "Training PCNN-lgt\n",
      "N parts:  2\n",
      "Training PCNN-Bagged\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "...Returning Results...\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Ablation Completion Percentage: 0.3333333333333333\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "-------------------------------------------------------\n",
      "Randomly Initialized Parts - Via Randomized Algorithm 2\n",
      "-------------------------------------------------------\n",
      "Final Loop\n",
      "breaking early\n",
      "The_parts_listhe number of parts are: 11.\n",
      "-----------------------------------------------------\n",
      "Training Sub-Networks on Each Randomly Generated Part\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 0/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  0\n",
      "(416, 1)\n",
      "(416, 1)\n",
      "TQAL\n",
      "(416, 1)\n",
      "training_quality\n",
      "(416, 1)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 1/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  1\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 1)\n",
      "training_quality\n",
      "(416, 2)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 2/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  2\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 2)\n",
      "training_quality\n",
      "(416, 3)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 3/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  3\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 3)\n",
      "training_quality\n",
      "(416, 4)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 4/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  4\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 4)\n",
      "training_quality\n",
      "(416, 5)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 5/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  5\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 5)\n",
      "training_quality\n",
      "(416, 6)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 6/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  6\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 6)\n",
      "training_quality\n",
      "(416, 7)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 7/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  7\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 7)\n",
      "training_quality\n",
      "(416, 8)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 8/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  8\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 8)\n",
      "training_quality\n",
      "(416, 9)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 9/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  9\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 9)\n",
      "training_quality\n",
      "(416, 10)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 10/11Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_part:  10\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 10)\n",
      "training_quality\n",
      "(416, 11)\n",
      "-----------------------\n",
      "Training Deep Zero-Sets\n",
      "-----------------------\n",
      "(416, 1)\n",
      "(9585, 1)\n",
      "(9585, 11)\n",
      "(416, 11)\n",
      "(416, 11)\n",
      "{'batch_size': [16], 'epochs': [200], 'learning_rate': [0.0001], 'height': [2000], 'depth': [2], 'input_dim': [1], 'output_dim': [1]}\n",
      "     0  0  0  0  0  0  0  0  0  0  0\n",
      "0    0  0  0  0  0  0  0  1  0  0  0\n",
      "1    0  0  0  0  0  0  1  0  0  0  0\n",
      "2    0  0  0  0  0  0  0  1  0  0  0\n",
      "3    0  0  0  0  0  0  0  1  0  0  0\n",
      "4    0  0  0  0  0  0  0  0  1  0  0\n",
      "..  .. .. .. .. .. .. .. .. .. .. ..\n",
      "411  1  0  0  0  0  0  0  0  0  0  0\n",
      "412  0  0  0  0  0  0  0  0  0  1  0\n",
      "413  0  0  0  0  0  0  0  0  0  1  0\n",
      "414  0  0  0  0  0  1  0  0  0  0  0\n",
      "415  0  0  0  1  0  0  0  0  0  0  0\n",
      "\n",
      "[416 rows x 11 columns]\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0]\n",
      "Generating Random Deep Features for Deep Zero-Sets\n",
      "Get Classifier\n",
      "OK!\n",
      "-----------------------------------\n",
      "Computing Final Performance Metrics\n",
      "-----------------------------------\n",
      "          train       test\n",
      "MAE    1.120545   1.196311\n",
      "MSE    5.410050   5.876937\n",
      "MAPE  31.847460  33.616231\n",
      "      L-time     P-time  N_params_expt   AIC-like     Eff  N. Parts\n",
      "0  46.035019  46.207995          20500  40999.642  11.877        11\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "# ---- Getting Benchmarks ---- #\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "#------------------------------#\n",
      "Training PCNN-lgt\n",
      "N parts:  11\n",
      "Training PCNN-Bagged\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "...Returning Results...\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Ablation Completion Percentage: 0.6666666666666666\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "-------------------------------------------------------\n",
      "Randomly Initialized Parts - Via Randomized Algorithm 2\n",
      "-------------------------------------------------------\n",
      "The_parts_listhe number of parts are: 20.\n",
      "-----------------------------------------------------\n",
      "Training Sub-Networks on Each Randomly Generated Part\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 0/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  0\n",
      "(416, 1)\n",
      "(416, 1)\n",
      "TQAL\n",
      "(416, 1)\n",
      "training_quality\n",
      "(416, 1)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 1/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  1\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 1)\n",
      "training_quality\n",
      "(416, 2)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 2/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  2\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 2)\n",
      "training_quality\n",
      "(416, 3)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 3/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  3\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 3)\n",
      "training_quality\n",
      "(416, 4)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 4/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  4\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 4)\n",
      "training_quality\n",
      "(416, 5)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 5/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  5\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 5)\n",
      "training_quality\n",
      "(416, 6)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 6/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  6\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 6)\n",
      "training_quality\n",
      "(416, 7)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 7/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  7\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 7)\n",
      "training_quality\n",
      "(416, 8)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 8/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  8\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 8)\n",
      "training_quality\n",
      "(416, 9)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 9/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n",
      "current_part:  9\n",
      "(416, 1)\n",
      "TQAL-else\n",
      "(416, 9)\n",
      "training_quality\n",
      "(416, 10)\n",
      "-----------------------------------------------------------\n",
      "Currently Training Part: 10/20Total Parts.\n",
      "-----------------------------------------------------------\n",
      "Randomized Predictions\n"
     ]
    }
   ],
   "source": [
    "# Initialize \n",
    "# q_implicit_N_parts_possibilities = np.linspace(min_parts_threshold,max_parts_threshold,N_plot_finess)\n",
    "# Sequential # of Parts:\n",
    "N_parts_possibilities = np.unique(np.round(np.linspace(N_min_parts,N_max_plots,num=N_plot_finess))).astype(int)\n",
    "# Custom # of Parts: # N_parts_possibilities = np.array([1,20,50]); N_plot_finess = len(N_parts_possibilities)\n",
    "\n",
    "# Get Performance Metric\n",
    "for inplicit_N_parts_loop in range(len(N_parts_possibilities)):\n",
    "    ### UPDATE USER ###\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    print('Ablation Completion Percentage:',(inplicit_N_parts_loop/N_plot_finess))\n",
    "    for k in range(10):\n",
    "        print('--------------------------------------')\n",
    "    \n",
    "    # Implicitly Set: Current Number of Parts\n",
    "#     q_implicit_N_parts_loop = q_implicit_N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    N_parts_possibilities_loop = N_parts_possibilities[inplicit_N_parts_loop]\n",
    "    # Run Algos. 1+2\n",
    "    performance_Architope_loop, Architope_Model_Complexity_full_loop, N_parts_Generated_by_Algo_2_loop, N_params_architope_loop, N_neurons_subPatterns_loop, N_neurons_deep_Zero_Sets_loop, height_mean_loop, performance_PCNN_ffNN_logistic_loop, N_params_PCNN_logistic_loop,performance_bagged_ffNN_loop, baggin_time_loop, logistic_time_loop, Deep_Zero_Sets_timer_loop = get_PCNNs(N_parts_possibilities_loop,X_train,y_train,X_test,y_test)\n",
    "    # Reshape\n",
    "    performance_Architope_loop = performance_Architope_loop.to_numpy().reshape([3,2,1])\n",
    "    Architope_Model_Complexity_full_loop = Architope_Model_Complexity_full_loop.to_numpy().reshape([1,6,1])\n",
    "    performance_PCNN_ffNN_logistic_loop = performance_PCNN_ffNN_logistic_loop.to_numpy().reshape([3,2,1])\n",
    "    performance_bagged_ffNN_loop = performance_bagged_ffNN_loop.to_numpy().reshape([3,2,1])\n",
    "    # Record\n",
    "    if inplicit_N_parts_loop == 0:\n",
    "        # Don't count partitioner if only one parts is active!\n",
    "        if N_parts_possibilities_loop <= 1:\n",
    "            Architope_Model_Complexity_full_loop[:,1] = Architope_Model_Complexity_full_loop[:,0]\n",
    "            N_neurons_deep_Zero_Sets_loop = 0\n",
    "        # Record Model Complexities Otherwise    \n",
    "        performance_Architope_history = performance_Architope_loop\n",
    "        Architope_Model_Complexity_history = Architope_Model_Complexity_full_loop\n",
    "        N_parts_Generated_by_Algo_2_history = N_parts_Generated_by_Algo_2_loop\n",
    "        N_params_subPatterns_hist = N_neurons_subPatterns_loop\n",
    "        N_neurons_deep_Zero_Sets_hist = N_neurons_deep_Zero_Sets_loop\n",
    "        N_params_architope_hist = N_neurons_deep_Zero_Sets_loop + N_neurons_subPatterns_loop\n",
    "        height_mean_hist = height_mean_loop\n",
    "        N_neurons_per_input = N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))\n",
    "        ### BENCHMARKs\n",
    "        ### Logistic PCNN\n",
    "        performance_PCNN_ffNN_logistic_hist = performance_PCNN_ffNN_logistic_loop\n",
    "        N_params_PCNN_logistic_hist = N_params_PCNN_logistic_loop\n",
    "        logistic_time_hist =  logistic_time_loop\n",
    "        baggin_time_hist = baggin_time_loop\n",
    "        ### Bagged PCNNs\n",
    "        performance_bagged_ffNN_hist = performance_bagged_ffNN_loop\n",
    "        ### Misc\n",
    "        Deep_Zero_Sets_timer_hist = Deep_Zero_Sets_timer_loop\n",
    "    else:\n",
    "        performance_Architope_history = np.concatenate((performance_Architope_history,performance_Architope_loop),axis=2)\n",
    "        Architope_Model_Complexity_history = np.concatenate((Architope_Model_Complexity_history,Architope_Model_Complexity_full_loop),axis=2)\n",
    "        N_parts_Generated_by_Algo_2_history = np.append(N_parts_Generated_by_Algo_2_history,N_parts_Generated_by_Algo_2_loop)\n",
    "        N_params_architope_hist = np.append(N_params_architope_hist,N_params_architope_loop)\n",
    "        N_params_subPatterns_hist = np.append(N_params_subPatterns_hist,N_neurons_subPatterns_loop)\n",
    "        N_neurons_deep_Zero_Sets_hist = np.append(N_neurons_deep_Zero_Sets_hist,N_neurons_deep_Zero_Sets_loop)\n",
    "        height_mean_hist = np.append(height_mean_hist,height_mean_loop)\n",
    "        N_neurons_per_input = np.append(N_neurons_per_input,(N_neurons_deep_Zero_Sets_loop + int(round(N_neurons_subPatterns_loop/N_parts_possibilities_loop))))\n",
    "        ### Logistic PCNN\n",
    "        performance_PCNN_ffNN_logistic_hist = np.concatenate((performance_PCNN_ffNN_logistic_hist,\n",
    "                                                              performance_PCNN_ffNN_logistic_loop),\n",
    "                                                             axis=2)\n",
    "        N_params_PCNN_logistic_hist = np.append(N_params_PCNN_logistic_hist,N_params_PCNN_logistic_loop)\n",
    "        logistic_time_hist = np.append(logistic_time_hist,logistic_time_loop)\n",
    "        ### Bagged Performance\n",
    "        performance_bagged_ffNN_hist = np.concatenate((performance_bagged_ffNN_hist,\n",
    "                                                       performance_bagged_ffNN_loop),\n",
    "                                                      axis=2)\n",
    "        baggin_time_hist = np.append(baggin_time_hist,baggin_time_loop)\n",
    "        ### Misc\n",
    "        Deep_Zero_Sets_timer_hist = np.append(Deep_Zero_Sets_timer_hist,Deep_Zero_Sets_timer_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "## Randomization may produce duplicates; we remove these with the following snippet:\n",
    "get_unique_entries = np.unique(N_parts_Generated_by_Algo_2_history, return_index=True)[1]\n",
    "N_parts_Generated_by_Algo_2_history_report = N_parts_Generated_by_Algo_2_history[get_unique_entries]\n",
    "\n",
    "# Write\n",
    "## Prediction Qualities\n",
    "performance_Architope_history_report_MAE_train = (performance_Architope_history[0,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MAE_test = (performance_Architope_history[0,1,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_train = (performance_Architope_history[1,0,:])[get_unique_entries]\n",
    "performance_Architope_history_report_MSE_test = (performance_Architope_history[1,1,:])[get_unique_entries]\n",
    "## Model Complexities\n",
    "L_Times = (Architope_Model_Complexity_history[:,1].reshape(-1,))[get_unique_entries]\n",
    "P_Times = (Architope_Model_Complexity_history[:,0].reshape(-1,))[get_unique_entries]\n",
    "N_Params = (N_params_architope_hist.reshape(-1,))[get_unique_entries]\n",
    "mean_subpattern_widths_hist = (height_mean_hist.reshape(-1,))[get_unique_entries]\n",
    "AIC_Like = (Architope_Model_Complexity_history[:,3].reshape(-1,))[get_unique_entries]\n",
    "Eff = (Architope_Model_Complexity_history[:,4].reshape(-1,))[get_unique_entries]\n",
    "N_neurons_per_input = (N_neurons_per_input.reshape(-1,))[get_unique_entries]\n",
    "## Misc\n",
    "Deep_Zero_Sets_timer_hist = Deep_Zero_Sets_timer_hist[get_unique_entries]\n",
    "\n",
    "# Record Benchmark Complexities\n",
    "## PCNN-lgt\n",
    "performance_lgt_ffNN_report_MAE_train = (performance_PCNN_ffNN_logistic_hist[0,0,:])[get_unique_entries]\n",
    "performance_lgt_ffNN_report_MAE_test = (performance_PCNN_ffNN_logistic_hist[0,1,:])[get_unique_entries]\n",
    "performance_lgt_ffNN_report_MSE_train = (performance_PCNN_ffNN_logistic_hist[1,0,:])[get_unique_entries]\n",
    "performance_lgt_ffNN_report_MSE_test = (performance_PCNN_ffNN_logistic_hist[1,1,:])[get_unique_entries]\n",
    "N_params_PCNN_logistic_hist = (N_params_subPatterns_hist + N_parts_Generated_by_Algo_2_history_report)[get_unique_entries]\n",
    "N_params_PCNN_logistic_hist_per_input = N_parts_Generated_by_Algo_2_history_report + N_params_subPatterns_hist[get_unique_entries]\n",
    "P_time_PCNN_lgt = logistic_time_hist[get_unique_entries] + P_Times - Deep_Zero_Sets_timer_hist\n",
    "L_time_PCNN_lgt = logistic_time_hist[get_unique_entries] + L_Times - Deep_Zero_Sets_timer_hist\n",
    "## PCNN-bag\n",
    "performance_PCNN_ffNN_bag_report_MAE_train = (performance_bagged_ffNN_hist[0,0,:])[get_unique_entries]\n",
    "performance_PCNN_ffNN_bag_report_MAE_test = (performance_bagged_ffNN_hist[0,1,:])[get_unique_entries]\n",
    "performance_PCNN_ffNN_bag_report_MSE_train = (performance_bagged_ffNN_hist[1,0,:])[get_unique_entries]\n",
    "performance_PCNN_ffNN_bag_report_MSE_test = (performance_bagged_ffNN_hist[1,1,:])[get_unique_entries]\n",
    "N_params_PCNN_ffNN_bag = (N_params_subPatterns_hist*N_parts_Generated_by_Algo_2_history_report)[get_unique_entries]\n",
    "N_params_PCNN_ffNN_bag_per_input = N_params_PCNN_ffNN_bag\n",
    "P_time_PCNN_bag = baggin_time_hist[get_unique_entries] + P_Times - Deep_Zero_Sets_timer_hist\n",
    "L_time_PCNN_bag = baggin_time_hist[get_unique_entries] + L_Times - Deep_Zero_Sets_timer_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Get Best PCNN\n",
    "This is identified as the PCNN with the smallest training MAE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_N_parts = 2#np.argmin(performance_Architope_history_report_MAE_test)\n",
    "# Get PCNN Performance Metrics\n",
    "PCNN_MAE_train = performance_Architope_history_report_MAE_train[best_N_parts]\n",
    "PCNN_MAE_test = performance_Architope_history_report_MAE_test[best_N_parts]\n",
    "PCNN_MSE_train = performance_Architope_history_report_MSE_train[best_N_parts]\n",
    "PCNN_MSE_test = performance_Architope_history_report_MSE_test[best_N_parts]\n",
    "PCNN_performance_all = performance_Architope_history[:,:,best_N_parts]\n",
    "## Model Complexities\n",
    "PCNN_L_time = L_Times[best_N_parts]\n",
    "PCNN_P_time = P_Times[best_N_parts]\n",
    "PCNN_subpattern_widths_hist = mean_subpattern_widths_hist[best_N_parts]\n",
    "PCNN_AIC_Like = AIC_Like[best_N_parts]\n",
    "PCNN_Eff = Eff[best_N_parts]\n",
    "PCNN_N_neurons_per_input = N_neurons_per_input[best_N_parts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Get Other Benchmark(s)\n",
    "## Feedforward Neural Network (ffNN) Benchmark\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if randomize_subpattern_construction == True:\n",
    "    exec(open('Grid_Enhanced_Network.py').read())\n",
    "    ffNN_train_time = time.time()\n",
    "    y_hat_ffNN_train, y_hat_ffNN_test, N_neurons_ffNN = build_ffNN(n_folds = 4, \n",
    "                                                                   n_jobs = n_jobs,\n",
    "                                                                   n_iter = n_iter, \n",
    "                                                                   param_grid_in = param_grid_Vanilla_Nets, \n",
    "                                                                   X_train= X_train, \n",
    "                                                                   y_train=y_train,\n",
    "                                                                   X_test_partial=X_train,\n",
    "                                                                   X_test=X_test,\n",
    "                                                                   NOCV=True)\n",
    "    #### Compute Performance\n",
    "    # Compute Peformance\n",
    "    performance_ffNN = reporter(y_train_hat_in=y_hat_ffNN_train,\n",
    "                                y_test_hat_in=y_hat_ffNN_test,\n",
    "                                y_train_in=y_train,\n",
    "                                y_test_in=y_test)\n",
    "    P_time_ffNN = time.time() - ffNN_train_time\n",
    "    L_time_ffNN = P_time_ffNN\n",
    "    Width_ffNN = param_grid_Vanilla_Nets['height'][0]; Width_neurons_ffNN = Width_ffNN\n",
    "    MAE_ffNN = np.repeat(performance_ffNN.to_numpy()[0,1],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "    MSE_ffNN = np.repeat(performance_ffNN.to_numpy()[1,1],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "    N_neurons_per_input_ffNN = np.repeat(N_neurons_ffNN,len(N_parts_Generated_by_Algo_2_history_report)); N_neurons_ffNN = N_neurons_per_input_ffNN\n",
    "    # Misc\n",
    "    ffNN_performance_all = performance_ffNN.to_numpy()\n",
    "else:\n",
    "    # Record Model complexities for ffNNs\n",
    "    P_time_ffNN = P_Times[0]\n",
    "    L_time_ffNN = P_Times[0]\n",
    "    Width_ffNN = height_mean_hist[0]\n",
    "    # For: Plots\n",
    "    MAE_ffNN = np.repeat(performance_Architope_history_report_MAE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "    MSE_ffNN = np.repeat(performance_Architope_history_report_MSE_test[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "    N_neurons_per_input_ffNN = np.repeat(N_neurons_per_input[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "    Width_neurons_ffNN = np.repeat(mean_subpattern_widths_hist[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "    N_neurons_ffNN = np.repeat(N_Params[0],len(N_parts_Generated_by_Algo_2_history_report))\n",
    "    # Misc\n",
    "    ffNN_performance_all = performance_Architope_history[:,:,0]\n",
    "L_times_ffNN_plot = np.repeat(L_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "P_times_ffNN_plot = np.repeat(P_time_ffNN,len(N_parts_Generated_by_Algo_2_history_report))\n",
    "# Record in Table\n",
    "ffNN_Model_Complexity = pd.DataFrame({'L-time': [L_time_ffNN],\n",
    "                                               'P-time':[P_time_ffNN],\n",
    "                                               'N_params_expt': [N_neurons_ffNN],\n",
    "                                               'AIC-like': [0],\n",
    "                                               'Eff': [0],\n",
    "                                               'N. Parts':[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Random Forest Regressor (GBRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print('Training Gradient-Boosted Random Forest: In-progress...')\n",
    "# Run from External Script\n",
    "exec(open('Gradient_Boosted_Random_Forest_Regressor.py').read())\n",
    "# Update User #\n",
    "#-------------#\n",
    "print('Training of Gradient-Boosted Random Forest: Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write Required Training Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update User #\n",
    "#-------------#\n",
    "print('Completing Table: Required Training Times')\n",
    "\n",
    "# Format Required Training Time(s)\n",
    "training_times_In_Line = pd.DataFrame({'ffNN': [round(L_Times[0],3)],\n",
    "                                       'GBRF': [round(Gradient_boosted_Random_forest_time,3)],\n",
    "                                       'ffNN-Bag': [round(L_time_PCNN_bag[best_N_parts],3)],\n",
    "                                       'ffNN-log': [round(L_time_PCNN_lgt[best_N_parts],3)],\n",
    "                                       'PCNN': [round(PCNN_L_time,3)]\n",
    "                                      },index=['In-Line (L-Time)'])\n",
    "\n",
    "training_times_Parallel = pd.DataFrame({'ffNN': ['-'],\n",
    "                                       'GBRF': ['-'],\n",
    "                                       'ffNN-Bag': [round(L_time_PCNN_bag[best_N_parts],3)],\n",
    "                                       'ffNN-log': [round(L_time_PCNN_lgt[best_N_parts],3)],\n",
    "                                       'PCNN': [round(PCNN_L_time,3)]\n",
    "                                      },index=['Parallel (P-Time)'])\n",
    "\n",
    "# Combine Training Times into Single Data-Frame #\n",
    "#-----------------------------------------------#\n",
    "Model_Training_times = training_times_In_Line.append(training_times_Parallel)\n",
    "Model_Training_times = Model_Training_times.transpose()\n",
    "\n",
    "# Write Required Training Time(s)\n",
    "Model_Training_times.to_latex((results_tables_path+\"Model_Training_Times.tex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Metric(s)\n",
    "#### Write Predictive Performance Dataframe(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Training Performance\n",
    "predictive_performance_training = pd.DataFrame({'ffNN': ffNN_performance_all[:,0],\n",
    "                                                'GBRF': Gradient_boosted_tree.train,\n",
    "                                                'ffNN-bag': performance_bagged_ffNN_hist[:,:,best_N_parts][:,0],\n",
    "                                                'ffNN-lgt': performance_PCNN_ffNN_logistic_hist[:,:,best_N_parts][:,0],\n",
    "                                                'PCNN': PCNN_performance_all[:,0]})\n",
    "predictive_performance_training = predictive_performance_training.transpose()\n",
    "\n",
    "# Write Testing Performance\n",
    "predictive_performance_test = pd.DataFrame({'ffNN': ffNN_performance_all[:,1],\n",
    "                                            'GBRF': Gradient_boosted_tree.test,\n",
    "                                            'ffNN-bag': performance_bagged_ffNN_hist[:,:,best_N_parts][:,1],\n",
    "                                            'ffNN-lgt': performance_PCNN_ffNN_logistic_hist[:,:,best_N_parts][:,1],\n",
    "                                            'PCNN': PCNN_performance_all[:,1]})\n",
    "predictive_performance_test = predictive_performance_test.transpose()\n",
    "\n",
    "# Write into one Dataframe #\n",
    "#--------------------------#\n",
    "predictive_performance_training.to_latex((results_tables_path+\"Models_predictive_performance_training.tex\"))\n",
    "predictive_performance_test.to_latex((results_tables_path+\"Models_predictive_performance_testing.tex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Plots\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.savefig('./outputs/plotsANDfigures/dump.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MSE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MSE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MSE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_lgt_ffNN_report_MSE_test,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_PCNN_ffNN_bag_report_MSE_test,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "# Add Legend\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MSE_test___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"MAE\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"MAE\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_Architope_history_report_MAE_test,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         MAE_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_lgt_ffNN_report_MAE_test,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         performance_PCNN_ffNN_bag_report_MAE_test,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_MAE___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"L-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"L-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_time_PCNN_lgt,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         L_time_PCNN_bag,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_L_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"P-Time\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"P-Time\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_Times,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_times_ffNN_plot,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_time_PCNN_lgt,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         P_time_PCNN_bag,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_P_Time___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"N. Params\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"N. Params\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_Params,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_logistic_hist,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_ffNN_bag,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_N_Params___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Active Neurons Per Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Active Neurons per. Input\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Active Neurons per. Input\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_per_input,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_neurons_ffNN,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_logistic_hist_per_input,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         N_params_PCNN_ffNN_bag_per_input,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Active_Neurons_per_input___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Widths for Sub-Pattern Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize Plot #\n",
    "#-----------------#\n",
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "# Format Plot #\n",
    "#-------------#\n",
    "plt.title(\"Mean Subpattern Widths\")\n",
    "plt.xlabel(\"N. Parts\")\n",
    "plt.ylabel(\"Mean Subpattern Widths\")\n",
    "\n",
    "# Generate Plots #\n",
    "#----------------#\n",
    "# Plot Signal\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'ffNN-lgt',\n",
    "         color='red',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'ffNN-bag',\n",
    "         color='orange',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'PCNN',\n",
    "         color='seagreen',\n",
    "         linewidth=2.5)\n",
    "plt.plot(N_parts_Generated_by_Algo_2_history_report,\n",
    "         mean_subpattern_widths_hist,\n",
    "         label = 'ffNN',\n",
    "         color='darkmagenta',\n",
    "         linewidth=2.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Export #\n",
    "#--------#\n",
    "# SAVE Figure to .eps\n",
    "plt.savefig('./outputs/plotsANDfigures/Ablation_Mean_Widths___'+str(Option_Function)+'__Fix_Neurons_Q'+str(Tied_Neurons_Q)+'.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print('Prediction Metric(s)')\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print(predictive_performance_test)\n",
    "for j in range(3):\n",
    "    print(' ')\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print('Model Complexitie(s)')\n",
    "for j in range(3):\n",
    "    print('---------------------')\n",
    "print(training_times_In_Line)\n",
    "print(training_times_Parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
